# Comparing `tmp/quixstreams-2.3.3-py3-none-any.whl.zip` & `tmp/quixstreams-2.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,84 +1,85 @@
-Zip file size: 116334 bytes, number of entries: 82
--rw-r--r--  2.0 unx      163 b- defN 24-Mar-25 19:08 quixstreams/__init__.py
--rw-r--r--  2.0 unx    33054 b- defN 24-Mar-25 19:08 quixstreams/app.py
--rw-r--r--  2.0 unx     2563 b- defN 24-Mar-25 19:08 quixstreams/context.py
--rw-r--r--  2.0 unx     1561 b- defN 24-Mar-25 19:08 quixstreams/error_callbacks.py
--rw-r--r--  2.0 unx     1409 b- defN 24-Mar-25 19:08 quixstreams/logging.py
--rw-r--r--  2.0 unx     7210 b- defN 24-Mar-25 19:08 quixstreams/rowconsumer.py
--rw-r--r--  2.0 unx     3934 b- defN 24-Mar-25 19:08 quixstreams/rowproducer.py
--rw-r--r--  2.0 unx      112 b- defN 24-Mar-25 19:08 quixstreams/types.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 19:08 quixstreams/core/__init__.py
--rw-r--r--  2.0 unx       47 b- defN 24-Mar-25 19:08 quixstreams/core/stream/__init__.py
--rw-r--r--  2.0 unx     7682 b- defN 24-Mar-25 19:08 quixstreams/core/stream/functions.py
--rw-r--r--  2.0 unx     8070 b- defN 24-Mar-25 19:08 quixstreams/core/stream/stream.py
--rw-r--r--  2.0 unx       90 b- defN 24-Mar-25 19:08 quixstreams/dataframe/__init__.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-25 19:08 quixstreams/dataframe/base.py
--rw-r--r--  2.0 unx    21469 b- defN 24-Mar-25 19:08 quixstreams/dataframe/dataframe.py
--rw-r--r--  2.0 unx      137 b- defN 24-Mar-25 19:08 quixstreams/dataframe/exceptions.py
--rw-r--r--  2.0 unx    15568 b- defN 24-Mar-25 19:08 quixstreams/dataframe/series.py
--rw-r--r--  2.0 unx      754 b- defN 24-Mar-25 19:08 quixstreams/dataframe/utils.py
--rw-r--r--  2.0 unx      106 b- defN 24-Mar-25 19:08 quixstreams/dataframe/windows/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Mar-25 19:08 quixstreams/dataframe/windows/base.py
--rw-r--r--  2.0 unx    10524 b- defN 24-Mar-25 19:08 quixstreams/dataframe/windows/definitions.py
--rw-r--r--  2.0 unx     7044 b- defN 24-Mar-25 19:08 quixstreams/dataframe/windows/time_based.py
--rw-r--r--  2.0 unx       46 b- defN 24-Mar-25 19:08 quixstreams/exceptions/__init__.py
--rw-r--r--  2.0 unx      326 b- defN 24-Mar-25 19:08 quixstreams/exceptions/assignment.py
--rw-r--r--  2.0 unx       41 b- defN 24-Mar-25 19:08 quixstreams/exceptions/base.py
--rw-r--r--  2.0 unx       48 b- defN 24-Mar-25 19:08 quixstreams/kafka/__init__.py
--rw-r--r--  2.0 unx    21935 b- defN 24-Mar-25 19:08 quixstreams/kafka/consumer.py
--rw-r--r--  2.0 unx     6365 b- defN 24-Mar-25 19:08 quixstreams/kafka/producer.py
--rw-r--r--  2.0 unx      146 b- defN 24-Mar-25 19:08 quixstreams/models/__init__.py
--rw-r--r--  2.0 unx     1949 b- defN 24-Mar-25 19:08 quixstreams/models/messagecontext.py
--rw-r--r--  2.0 unx      485 b- defN 24-Mar-25 19:08 quixstreams/models/messages.py
--rw-r--r--  2.0 unx     2229 b- defN 24-Mar-25 19:08 quixstreams/models/rows.py
--rw-r--r--  2.0 unx     1813 b- defN 24-Mar-25 19:08 quixstreams/models/timestamps.py
--rw-r--r--  2.0 unx     1389 b- defN 24-Mar-25 19:08 quixstreams/models/types.py
--rw-r--r--  2.0 unx     1016 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/__init__.py
--rw-r--r--  2.0 unx     3173 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/base.py
--rw-r--r--  2.0 unx     1392 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/exceptions.py
--rw-r--r--  2.0 unx     1936 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/json.py
--rw-r--r--  2.0 unx    16878 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/quix.py
--rw-r--r--  2.0 unx     4516 b- defN 24-Mar-25 19:08 quixstreams/models/serializers/simple_types.py
--rw-r--r--  2.0 unx       65 b- defN 24-Mar-25 19:08 quixstreams/models/topics/__init__.py
--rw-r--r--  2.0 unx     6536 b- defN 24-Mar-25 19:08 quixstreams/models/topics/admin.py
--rw-r--r--  2.0 unx      317 b- defN 24-Mar-25 19:08 quixstreams/models/topics/exceptions.py
--rw-r--r--  2.0 unx    12089 b- defN 24-Mar-25 19:08 quixstreams/models/topics/manager.py
--rw-r--r--  2.0 unx    10905 b- defN 24-Mar-25 19:08 quixstreams/models/topics/topic.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 19:08 quixstreams/platforms/__init__.py
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/__init__.py
--rw-r--r--  2.0 unx     5634 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/api.py
--rw-r--r--  2.0 unx     1531 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/checks.py
--rw-r--r--  2.0 unx    20920 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/config.py
--rw-r--r--  2.0 unx     1972 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/env.py
--rw-r--r--  2.0 unx     3024 b- defN 24-Mar-25 19:08 quixstreams/platforms/quix/topic_manager.py
--rw-r--r--  2.0 unx       68 b- defN 24-Mar-25 19:08 quixstreams/state/__init__.py
--rw-r--r--  2.0 unx      350 b- defN 24-Mar-25 19:08 quixstreams/state/exceptions.py
--rw-r--r--  2.0 unx    12858 b- defN 24-Mar-25 19:08 quixstreams/state/manager.py
--rw-r--r--  2.0 unx    13403 b- defN 24-Mar-25 19:08 quixstreams/state/recovery.py
--rw-r--r--  2.0 unx     1510 b- defN 24-Mar-25 19:08 quixstreams/state/state.py
--rw-r--r--  2.0 unx     9911 b- defN 24-Mar-25 19:08 quixstreams/state/types.py
--rw-r--r--  2.0 unx      143 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/__init__.py
--rw-r--r--  2.0 unx      605 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/exceptions.py
--rw-r--r--  2.0 unx      253 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/metadata.py
--rw-r--r--  2.0 unx     2993 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/options.py
--rw-r--r--  2.0 unx    12157 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/partition.py
--rw-r--r--  2.0 unx      948 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/serialization.py
--rw-r--r--  2.0 unx     5423 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/store.py
--rw-r--r--  2.0 unx    12099 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/transaction.py
--rw-r--r--  2.0 unx      648 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/types.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/__init__.py
--rw-r--r--  2.0 unx      118 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/metadata.py
--rw-r--r--  2.0 unx     2467 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/partition.py
--rw-r--r--  2.0 unx     2014 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/serialization.py
--rw-r--r--  2.0 unx     2876 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/state.py
--rw-r--r--  2.0 unx     2107 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/store.py
--rw-r--r--  2.0 unx     7468 b- defN 24-Mar-25 19:08 quixstreams/state/rocksdb/windowed/transaction.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-25 19:08 quixstreams/utils/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 24-Mar-25 19:08 quixstreams/utils/dicts.py
--rw-r--r--  2.0 unx      636 b- defN 24-Mar-25 19:08 quixstreams/utils/json.py
--rw-r--r--  2.0 unx    11353 b- defN 24-Mar-25 19:09 quixstreams-2.3.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     9422 b- defN 24-Mar-25 19:09 quixstreams-2.3.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 19:09 quixstreams-2.3.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-Mar-25 19:09 quixstreams-2.3.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7395 b- defN 24-Mar-25 19:09 quixstreams-2.3.3.dist-info/RECORD
-82 files, 371994 bytes uncompressed, 104456 bytes compressed:  71.9%
+Zip file size: 117137 bytes, number of entries: 83
+-rw-r--r--  2.0 unx      163 b- defN 24-Apr-04 14:05 quixstreams/__init__.py
+-rw-r--r--  2.0 unx    35815 b- defN 24-Apr-04 14:05 quixstreams/app.py
+-rw-r--r--  2.0 unx     2559 b- defN 24-Apr-04 14:05 quixstreams/context.py
+-rw-r--r--  2.0 unx     1561 b- defN 24-Apr-04 14:05 quixstreams/error_callbacks.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-Apr-04 14:05 quixstreams/logging.py
+-rw-r--r--  2.0 unx     7194 b- defN 24-Apr-04 14:05 quixstreams/rowconsumer.py
+-rw-r--r--  2.0 unx     3926 b- defN 24-Apr-04 14:05 quixstreams/rowproducer.py
+-rw-r--r--  2.0 unx      112 b- defN 24-Apr-04 14:05 quixstreams/types.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-04 14:05 quixstreams/core/__init__.py
+-rw-r--r--  2.0 unx       47 b- defN 24-Apr-04 14:05 quixstreams/core/stream/__init__.py
+-rw-r--r--  2.0 unx     7678 b- defN 24-Apr-04 14:05 quixstreams/core/stream/functions.py
+-rw-r--r--  2.0 unx     8070 b- defN 24-Apr-04 14:05 quixstreams/core/stream/stream.py
+-rw-r--r--  2.0 unx       90 b- defN 24-Apr-04 14:05 quixstreams/dataframe/__init__.py
+-rw-r--r--  2.0 unx      455 b- defN 24-Apr-04 14:05 quixstreams/dataframe/base.py
+-rw-r--r--  2.0 unx    21428 b- defN 24-Apr-04 14:05 quixstreams/dataframe/dataframe.py
+-rw-r--r--  2.0 unx      133 b- defN 24-Apr-04 14:05 quixstreams/dataframe/exceptions.py
+-rw-r--r--  2.0 unx    15568 b- defN 24-Apr-04 14:05 quixstreams/dataframe/series.py
+-rw-r--r--  2.0 unx      754 b- defN 24-Apr-04 14:05 quixstreams/dataframe/utils.py
+-rw-r--r--  2.0 unx      106 b- defN 24-Apr-04 14:05 quixstreams/dataframe/windows/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-04 14:05 quixstreams/dataframe/windows/base.py
+-rw-r--r--  2.0 unx    10516 b- defN 24-Apr-04 14:05 quixstreams/dataframe/windows/definitions.py
+-rw-r--r--  2.0 unx     7044 b- defN 24-Apr-04 14:05 quixstreams/dataframe/windows/time_based.py
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-04 14:05 quixstreams/exceptions/__init__.py
+-rw-r--r--  2.0 unx      322 b- defN 24-Apr-04 14:05 quixstreams/exceptions/assignment.py
+-rw-r--r--  2.0 unx       41 b- defN 24-Apr-04 14:05 quixstreams/exceptions/base.py
+-rw-r--r--  2.0 unx       48 b- defN 24-Apr-04 14:05 quixstreams/kafka/__init__.py
+-rw-r--r--  2.0 unx    21935 b- defN 24-Apr-04 14:05 quixstreams/kafka/consumer.py
+-rw-r--r--  2.0 unx     6375 b- defN 24-Apr-04 14:05 quixstreams/kafka/producer.py
+-rw-r--r--  2.0 unx      146 b- defN 24-Apr-04 14:05 quixstreams/models/__init__.py
+-rw-r--r--  2.0 unx     1949 b- defN 24-Apr-04 14:05 quixstreams/models/messagecontext.py
+-rw-r--r--  2.0 unx      485 b- defN 24-Apr-04 14:05 quixstreams/models/messages.py
+-rw-r--r--  2.0 unx     2229 b- defN 24-Apr-04 14:05 quixstreams/models/rows.py
+-rw-r--r--  2.0 unx     1813 b- defN 24-Apr-04 14:05 quixstreams/models/timestamps.py
+-rw-r--r--  2.0 unx     1309 b- defN 24-Apr-04 14:05 quixstreams/models/types.py
+-rw-r--r--  2.0 unx     1016 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/__init__.py
+-rw-r--r--  2.0 unx     3157 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/base.py
+-rw-r--r--  2.0 unx     1370 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/exceptions.py
+-rw-r--r--  2.0 unx     1936 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/json.py
+-rw-r--r--  2.0 unx    16878 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/quix.py
+-rw-r--r--  2.0 unx     4516 b- defN 24-Apr-04 14:05 quixstreams/models/serializers/simple_types.py
+-rw-r--r--  2.0 unx       65 b- defN 24-Apr-04 14:05 quixstreams/models/topics/__init__.py
+-rw-r--r--  2.0 unx     6536 b- defN 24-Apr-04 14:05 quixstreams/models/topics/admin.py
+-rw-r--r--  2.0 unx      297 b- defN 24-Apr-04 14:05 quixstreams/models/topics/exceptions.py
+-rw-r--r--  2.0 unx    12089 b- defN 24-Apr-04 14:05 quixstreams/models/topics/manager.py
+-rw-r--r--  2.0 unx     9598 b- defN 24-Apr-04 14:05 quixstreams/models/topics/topic.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-04 14:05 quixstreams/platforms/__init__.py
+-rw-r--r--  2.0 unx      118 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/__init__.py
+-rw-r--r--  2.0 unx     6358 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/api.py
+-rw-r--r--  2.0 unx     1531 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/checks.py
+-rw-r--r--  2.0 unx    21205 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/config.py
+-rw-r--r--  2.0 unx     1773 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/env.py
+-rw-r--r--  2.0 unx      329 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/exceptions.py
+-rw-r--r--  2.0 unx     2968 b- defN 24-Apr-04 14:05 quixstreams/platforms/quix/topic_manager.py
+-rw-r--r--  2.0 unx       68 b- defN 24-Apr-04 14:05 quixstreams/state/__init__.py
+-rw-r--r--  2.0 unx      330 b- defN 24-Apr-04 14:05 quixstreams/state/exceptions.py
+-rw-r--r--  2.0 unx    12858 b- defN 24-Apr-04 14:05 quixstreams/state/manager.py
+-rw-r--r--  2.0 unx    13403 b- defN 24-Apr-04 14:05 quixstreams/state/recovery.py
+-rw-r--r--  2.0 unx     1510 b- defN 24-Apr-04 14:05 quixstreams/state/state.py
+-rw-r--r--  2.0 unx     9847 b- defN 24-Apr-04 14:05 quixstreams/state/types.py
+-rw-r--r--  2.0 unx      143 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/__init__.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/exceptions.py
+-rw-r--r--  2.0 unx      253 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/metadata.py
+-rw-r--r--  2.0 unx     2959 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/options.py
+-rw-r--r--  2.0 unx    12157 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/partition.py
+-rw-r--r--  2.0 unx      948 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/serialization.py
+-rw-r--r--  2.0 unx     5423 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/store.py
+-rw-r--r--  2.0 unx    12099 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/transaction.py
+-rw-r--r--  2.0 unx      640 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/types.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/__init__.py
+-rw-r--r--  2.0 unx      118 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/metadata.py
+-rw-r--r--  2.0 unx     2467 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/partition.py
+-rw-r--r--  2.0 unx     2014 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/serialization.py
+-rw-r--r--  2.0 unx     2876 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/state.py
+-rw-r--r--  2.0 unx     2107 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/store.py
+-rw-r--r--  2.0 unx     7468 b- defN 24-Apr-04 14:05 quixstreams/state/rocksdb/windowed/transaction.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-04 14:05 quixstreams/utils/__init__.py
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-04 14:05 quixstreams/utils/dicts.py
+-rw-r--r--  2.0 unx      636 b- defN 24-Apr-04 14:05 quixstreams/utils/json.py
+-rw-r--r--  2.0 unx    11353 b- defN 24-Apr-04 14:05 quixstreams-2.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     9072 b- defN 24-Apr-04 14:05 quixstreams-2.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-04 14:05 quixstreams-2.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-04 14:05 quixstreams-2.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7491 b- defN 24-Apr-04 14:05 quixstreams-2.4.0.dist-info/RECORD
+83 files, 373908 bytes uncompressed, 105103 bytes compressed:  71.9%
```

## zipnote {}

```diff
@@ -147,14 +147,17 @@
 
 Filename: quixstreams/platforms/quix/config.py
 Comment: 
 
 Filename: quixstreams/platforms/quix/env.py
 Comment: 
 
+Filename: quixstreams/platforms/quix/exceptions.py
+Comment: 
+
 Filename: quixstreams/platforms/quix/topic_manager.py
 Comment: 
 
 Filename: quixstreams/state/__init__.py
 Comment: 
 
 Filename: quixstreams/state/exceptions.py
@@ -225,23 +228,23 @@
 
 Filename: quixstreams/utils/dicts.py
 Comment: 
 
 Filename: quixstreams/utils/json.py
 Comment: 
 
-Filename: quixstreams-2.3.3.dist-info/LICENSE
+Filename: quixstreams-2.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: quixstreams-2.3.3.dist-info/METADATA
+Filename: quixstreams-2.4.0.dist-info/METADATA
 Comment: 
 
-Filename: quixstreams-2.3.3.dist-info/WHEEL
+Filename: quixstreams-2.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: quixstreams-2.3.3.dist-info/top_level.txt
+Filename: quixstreams-2.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: quixstreams-2.3.3.dist-info/RECORD
+Filename: quixstreams-2.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## quixstreams/__init__.py

```diff
@@ -1,7 +1,7 @@
 from .app import Application
 from .context import message_context, message_key
 from .models import MessageContext
 from .state import State
 
 
-__version__ = "2.3.3"
+__version__ = "2.4.0"
```

## quixstreams/app.py

```diff
@@ -1,10 +1,13 @@
 import contextlib
+import functools
 import logging
+import os
 import signal
+import warnings
 from typing import Optional, List, Callable
 
 from confluent_kafka import TopicPartition
 from typing_extensions import Self
 
 from .context import set_message_context, copy_context
 from .core.stream import Filtered
@@ -57,18 +60,17 @@
 
     Most functionality is explained the various methods, except for
     "column assignment".
 
 
     What it Does:
 
-    - During user setup:
+    - On init:
         - Provides defaults or helper methods for commonly needed objects
-        - For Quix Platform Users: Configures the app for it
-            (see `Application.Quix()`)
+        - If `quix_sdk_token` is passed, configures the app to use the Quix Cloud.
     - When executed via `.run()` (after setup):
         - Initializes Topics and StreamingDataFrames
         - Facilitates processing of Kafka messages with a `StreamingDataFrame`
         - Handles all Kafka client consumer/producer responsibilities.
 
 
     Example Snippet:
@@ -86,16 +88,17 @@
 
     app.run(dataframe=df)
     ```
     """
 
     def __init__(
         self,
-        broker_address: str,
-        consumer_group: str = "quixstreams-default",
+        broker_address: Optional[str] = None,
+        quix_sdk_token: Optional[str] = None,
+        consumer_group: Optional[str] = None,
         auto_offset_reset: AutoOffsetReset = "latest",
         auto_commit_enable: bool = True,
         partitioner: Partitioner = "murmur2",
         consumer_extra_config: Optional[dict] = None,
         producer_extra_config: Optional[dict] = None,
         state_dir: str = "state",
         rocksdb_options: Optional[RocksDBOptionsType] = None,
@@ -104,66 +107,130 @@
         on_producer_error: Optional[ProducerErrorCallback] = None,
         on_message_processed: Optional[MessageProcessedCallback] = None,
         consumer_poll_timeout: float = 1.0,
         producer_poll_timeout: float = 0.0,
         loglevel: Optional[LogLevel] = "INFO",
         auto_create_topics: bool = True,
         use_changelog_topics: bool = True,
+        quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,
         topic_manager: Optional[TopicManager] = None,
     ):
         """
-
         :param broker_address: Kafka broker host and port in format `<host>:<port>`.
             Passed as `bootstrap.servers` to `confluent_kafka.Consumer`.
+            Either this OR `quix_sdk_token` must be set to use `Application` (not both).
+            Linked Environment Variable: `Quix__Broker__Address`.
+            Default: `None`
+        :param quix_sdk_token: If using the Quix Cloud, the SDK token to connect with.
+            Either this OR `broker_address` must be set to use Application (not both).
+            Linked Environment Variable: `Quix__Sdk__Token`.
+            Default: None (if not run on Quix Cloud)
+              >***NOTE:*** the environment variable is set for you in the Quix Cloud
         :param consumer_group: Kafka consumer group.
-            Passed as `group.id` to `confluent_kafka.Consumer`
-            Default - "quixstreams-default".
+            Passed as `group.id` to `confluent_kafka.Consumer`.
+            Linked Environment Variable: `Quix__Consumer__Group`.
+            Default - "quixstreams-default" (set during init)
+              >***NOTE:*** Quix Applications will prefix it with the Quix workspace id.
         :param auto_offset_reset: Consumer `auto.offset.reset` setting
         :param auto_commit_enable: If true, periodically commit offset of
             the last message handed to the application. Default - `True`.
         :param partitioner: A function to be used to determine the outgoing message
             partition.
         :param consumer_extra_config: A dictionary with additional options that
             will be passed to `confluent_kafka.Consumer` as is.
         :param producer_extra_config: A dictionary with additional options that
             will be passed to `confluent_kafka.Producer` as is.
         :param state_dir: path to the application state directory.
-            Default - ".state".
+            Default - `".state"`.
         :param rocksdb_options: RocksDB options.
             If `None`, the default options will be used.
-        :param consumer_poll_timeout: timeout for `RowConsumer.poll()`. Default - 1.0s
-        :param producer_poll_timeout: timeout for `RowProducer.poll()`. Default - 0s.
+        :param consumer_poll_timeout: timeout for `RowConsumer.poll()`. Default - `1.0`s
+        :param producer_poll_timeout: timeout for `RowProducer.poll()`. Default - `0`s.
         :param on_message_processed: a callback triggered when message is successfully
             processed.
         :param loglevel: a log level for "quixstreams" logger.
             Should be a string or None.
             If `None` is passed, no logging will be configured.
             You may pass `None` and configure "quixstreams" logger
             externally using `logging` library.
-            Default - "INFO".
+            Default - `"INFO"`.
         :param auto_create_topics: Create all `Topic`s made via Application.topic()
             Default - `True`
         :param use_changelog_topics: Use changelog topics to back stateful operations
             Default - `True`
-        :param topic_manager: A TopicManager instance
-
-        ***Error Handlers***
+        :param topic_manager: A `TopicManager` instance
 
+        <br><br>***Error Handlers***<br>
         To handle errors, `Application` accepts callbacks triggered when
             exceptions occur on different stages of stream processing. If the callback
             returns `True`, the exception will be ignored. Otherwise, the exception
             will be propagated and the processing will eventually stop.
         :param on_consumer_error: triggered when internal `RowConsumer` fails
-        to poll Kafka or cannot deserialize a message.
+            to poll Kafka or cannot deserialize a message.
         :param on_processing_error: triggered when exception is raised within
             `StreamingDataFrame.process()`.
-        :param on_producer_error: triggered when RowProducer fails to serialize
+        :param on_producer_error: triggered when `RowProducer` fails to serialize
             or to produce a message to Kafka.
+        <br><br>***Quix Cloud Parameters***<br>
+        :param quix_config_builder: instance of `QuixKafkaConfigsBuilder` to be used
+            instead of the default one.
+            > NOTE: It is recommended to just use `quix_sdk_token` instead.
         """
         configure_logging(loglevel=loglevel)
+
+        # We can't use os.getenv as defaults (and have testing work nicely)
+        # since it evaluates getenv when the function is defined.
+        # In general this is just a most robust approach.
+        broker_address = broker_address or os.getenv("Quix__Broker__Address")
+        quix_sdk_token = quix_sdk_token or os.getenv("Quix__Sdk__Token")
+        consumer_group = consumer_group or os.getenv(
+            "Quix__Consumer_Group", "quixstreams-default"
+        )
+
+        if quix_config_builder:
+            quix_app_source = "Quix Config Builder"
+        if quix_config_builder and quix_sdk_token:
+            raise warnings.warn(
+                "'quix_config_builder' is not necessary when an SDK token is defined; "
+                "we recommend letting the Application generate it automatically"
+            )
+
+        if quix_sdk_token and not quix_config_builder:
+            quix_app_source = "Quix SDK Token"
+            quix_config_builder = QuixKafkaConfigsBuilder(quix_sdk_token=quix_sdk_token)
+
+        if broker_address and quix_config_builder:
+            raise ValueError("Cannot provide both broker address and Quix SDK Token")
+        elif not (broker_address or quix_config_builder):
+            raise ValueError("Either broker address or Quix SDK Token must be provided")
+        elif quix_config_builder:
+            # SDK Token or QuixKafkaConfigsBuilder were provided
+            logger.info(
+                f"{quix_app_source} detected; "
+                f"the application will connect to Quix Cloud brokers"
+            )
+            topic_manager_factory = functools.partial(
+                QuixTopicManager, quix_config_builder=quix_config_builder
+            )
+            quix_configs = quix_config_builder.get_confluent_broker_config()
+            # Check if the state dir points to the mounted PVC while running on Quix
+            # TODO: Do we still need this?
+            check_state_dir(state_dir=state_dir)
+
+            broker_address = quix_configs.pop("bootstrap.servers")
+            # Quix Cloud prefixes consumer group with workspace id
+            consumer_group = quix_config_builder.prepend_workspace_id(consumer_group)
+            consumer_extra_config = {**quix_configs, **(consumer_extra_config or {})}
+            producer_extra_config = {**quix_configs, **(producer_extra_config or {})}
+        else:
+            # Only broker address is provided
+            topic_manager_factory = TopicManager
+
+        self._is_quix_app = bool(quix_config_builder)
+
         self._broker_address = broker_address
         self._consumer_group = consumer_group
         self._auto_offset_reset = auto_offset_reset
         self._auto_commit_enable = auto_commit_enable
         self._partitioner = partitioner
         self._producer_extra_config = producer_extra_config
         self._consumer_extra_config = consumer_extra_config
@@ -185,20 +252,19 @@
         )
 
         self._consumer_poll_timeout = consumer_poll_timeout
         self._producer_poll_timeout = producer_poll_timeout
         self._running = False
         self._on_processing_error = on_processing_error or default_on_processing_error
         self._on_message_processed = on_message_processed
-        self._quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None
         self._auto_create_topics = auto_create_topics
         self._do_recovery_check = False
 
         if not topic_manager:
-            topic_manager = TopicManager(
+            topic_manager = topic_manager_factory(
                 topic_admin=TopicAdmin(
                     broker_address=broker_address,
                     extra_config=producer_extra_config,
                 )
             )
         self._topic_manager = topic_manager
 
@@ -222,21 +288,18 @@
                     topic_manager=self._topic_manager,
                 )
                 if use_changelog_topics
                 else None
             ),
         )
 
-    def _set_quix_config_builder(self, config_builder: QuixKafkaConfigsBuilder):
-        self._quix_config_builder = config_builder
-
     @classmethod
     def Quix(
         cls,
-        consumer_group: str = "quixstreams-default",
+        consumer_group: Optional[str] = None,
         auto_offset_reset: AutoOffsetReset = "latest",
         auto_commit_enable: bool = True,
         partitioner: Partitioner = "murmur2",
         consumer_extra_config: Optional[dict] = None,
         producer_extra_config: Optional[dict] = None,
         state_dir: str = "state",
         rocksdb_options: Optional[RocksDBOptionsType] = None,
@@ -249,21 +312,23 @@
         loglevel: Optional[LogLevel] = "INFO",
         quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,
         auto_create_topics: bool = True,
         use_changelog_topics: bool = True,
         topic_manager: Optional[QuixTopicManager] = None,
     ) -> Self:
         """
-        Initialize an Application to work with Quix platform,
-        assuming environment is properly configured (by default in the platform).
+        >***NOTE:*** DEPRECATED: use Application with `quix_sdk_token` argument instead.
+
+        Initialize an Application to work with Quix Cloud,
+        assuming environment is properly configured (by default in Quix Cloud).
 
         It takes the credentials from the environment and configures consumer and
-        producer to properly connect to the Quix platform.
+        producer to properly connect to the Quix Cloud.
 
-        >***NOTE:*** Quix platform requires `consumer_group` and topic names to be
+        >***NOTE:*** Quix Cloud requires `consumer_group` and topic names to be
             prefixed with workspace id.
             If the application is created via `Application.Quix()`, the real consumer
             group will be `<workspace_id>-<consumer_group>`,
             and the real topic names will be `<workspace_id>-<topic_name>`.
 
 
 
@@ -283,116 +348,95 @@
         df = df.to_topic(output_topic)
 
         app.run(dataframe=df)
         ```
 
         :param consumer_group: Kafka consumer group.
             Passed as `group.id` to `confluent_kafka.Consumer`.
-            Default - "quixstreams-default".
-              >***NOTE:*** The consumer group will be prefixed by Quix workspace id.
+            Linked Environment Variable: `Quix__Consumer__Group`.
+            Default - "quixstreams-default" (set during init).
+              >***NOTE:*** Quix Applications will prefix it with the Quix workspace id.
         :param auto_offset_reset: Consumer `auto.offset.reset` setting
         :param auto_commit_enable: If true, periodically commit offset of
             the last message handed to the application. Default - `True`.
         :param partitioner: A function to be used to determine the outgoing message
             partition.
         :param consumer_extra_config: A dictionary with additional options that
             will be passed to `confluent_kafka.Consumer` as is.
         :param producer_extra_config: A dictionary with additional options that
             will be passed to `confluent_kafka.Producer` as is.
         :param state_dir: path to the application state directory.
-            Default - ".state".
+            Default - `".state"`.
         :param rocksdb_options: RocksDB options.
             If `None`, the default options will be used.
-        :param consumer_poll_timeout: timeout for `RowConsumer.poll()`. Default - 1.0s
-        :param producer_poll_timeout: timeout for `RowProducer.poll()`. Default - 0s.
+        :param consumer_poll_timeout: timeout for `RowConsumer.poll()`. Default - `1.0`s
+        :param producer_poll_timeout: timeout for `RowProducer.poll()`. Default - `0`s.
         :param on_message_processed: a callback triggered when message is successfully
             processed.
         :param loglevel: a log level for "quixstreams" logger.
-            Should be a string or None.
+            Should be a string or `None`.
             If `None` is passed, no logging will be configured.
             You may pass `None` and configure "quixstreams" logger
             externally using `logging` library.
-            Default - "INFO".
-        :param auto_create_topics: Create all `Topic`s made via Application.topic()
+            Default - `"INFO"`.
+        :param auto_create_topics: Create all `Topic`s made via `Application.topic()`
             Default - `True`
         :param use_changelog_topics: Use changelog topics to back stateful operations
             Default - `True`
-        :param topic_manager: A QuixTopicManager instance
-
-        ***Error Handlers***
+        :param topic_manager: A `QuixTopicManager` instance
 
+        <br><br>***Error Handlers***<br>
         To handle errors, `Application` accepts callbacks triggered when
             exceptions occur on different stages of stream processing. If the callback
             returns `True`, the exception will be ignored. Otherwise, the exception
             will be propagated and the processing will eventually stop.
         :param on_consumer_error: triggered when internal `RowConsumer` fails to poll
             Kafka or cannot deserialize a message.
         :param on_processing_error: triggered when exception is raised within
             `StreamingDataFrame.process()`.
         :param on_producer_error: triggered when RowProducer fails to serialize
             or to produce a message to Kafka.
-
-
-        ***Quix-specific Parameters***
-
+        <br><br>***Quix Cloud Parameters***<br>
         :param quix_config_builder: instance of `QuixKafkaConfigsBuilder` to be used
             instead of the default one.
 
         :return: `Application` object
         """
-        configure_logging(loglevel=loglevel)
-        quix_config_builder = quix_config_builder or QuixKafkaConfigsBuilder()
-        quix_configs = quix_config_builder.get_confluent_broker_config()
-
-        # Check if the state dir points to the mounted PVC while running on Quix
-        # Otherwise, the state won't be shared and replicas won't be able to
-        # recover the same state.
-        check_state_dir(state_dir=state_dir)
-
-        broker_address = quix_configs.pop("bootstrap.servers")
-        # Quix platform prefixes consumer group with workspace id
-        consumer_group = quix_config_builder.prepend_workspace_id(consumer_group)
-        consumer_extra_config = {**quix_configs, **(consumer_extra_config or {})}
-        producer_extra_config = {**quix_configs, **(producer_extra_config or {})}
-
-        if topic_manager is None:
-            topic_admin = TopicAdmin(
-                broker_address=broker_address,
-                extra_config=producer_extra_config,
-            )
-            topic_manager = QuixTopicManager(
-                topic_admin=topic_admin, quix_config_builder=quix_config_builder
-            )
+        warnings.warn(
+            "Application.Quix() is being deprecated; "
+            "To connect to Quix Cloud, "
+            'use Application() with "quix_sdk_token" parameter or set the '
+            '"Quix__Sdk__Token" environment variable (like with Application.Quix).',
+            DeprecationWarning,
+        )
         app = cls(
-            broker_address=broker_address,
+            broker_address=None,
+            quix_sdk_token=os.getenv("Quix__Sdk__Token"),
             consumer_group=consumer_group,
             consumer_extra_config=consumer_extra_config,
             producer_extra_config=producer_extra_config,
             auto_offset_reset=auto_offset_reset,
             auto_commit_enable=auto_commit_enable,
             partitioner=partitioner,
             on_consumer_error=on_consumer_error,
             on_processing_error=on_processing_error,
             on_producer_error=on_producer_error,
             on_message_processed=on_message_processed,
             consumer_poll_timeout=consumer_poll_timeout,
             producer_poll_timeout=producer_poll_timeout,
+            loglevel=loglevel,
             state_dir=state_dir,
             rocksdb_options=rocksdb_options,
             auto_create_topics=auto_create_topics,
             use_changelog_topics=use_changelog_topics,
             topic_manager=topic_manager,
+            quix_config_builder=quix_config_builder,
         )
-        app._set_quix_config_builder(quix_config_builder)
         return app
 
-    @property
-    def is_quix_app(self) -> bool:
-        return self._quix_config_builder is not None
-
     def topic(
         self,
         name: str,
         value_deserializer: DeserializerType = "json",
         key_deserializer: DeserializerType = "bytes",
         value_serializer: SerializerType = "json",
         key_serializer: SerializerType = "bytes",
@@ -600,18 +644,17 @@
         """
         self._state_manager.clear_stores()
 
     def _quix_runtime_init(self):
         """
         Do a runtime setup only applicable to an Application.Quix instance
         - Ensure that "State management" flag is enabled for deployment if the app
-          is stateful and is running on Quix platform
+          is stateful and is running in Quix Cloud
         """
         # Ensure that state management is enabled if application is stateful
-        # and is running on Quix platform
         if self._state_manager.stores:
             check_state_management_enabled()
 
     def _setup_topics(self):
         topics_list = ", ".join(
             f'"{topic.name}"' for topic in self._topic_manager.all_topics
         )
@@ -706,15 +749,15 @@
 
         logger.info(
             f"Starting the Application with the config: "
             f'broker_address="{self._broker_address}" '
             f'consumer_group="{self._consumer_group}" '
             f'auto_offset_reset="{self._auto_offset_reset}"'
         )
-        if self.is_quix_app:
+        if self._is_quix_app:
             self._quix_runtime_init()
 
         self._setup_topics()
 
         exit_stack = contextlib.ExitStack()
         exit_stack.enter_context(self._producer)
         exit_stack.enter_context(self._consumer)
```

## quixstreams/context.py

```diff
@@ -11,16 +11,15 @@
     "message_context",
     "copy_context",
 )
 
 _current_message_context = ContextVar("current_message_context")
 
 
-class MessageContextNotSetError(QuixException):
-    ...
+class MessageContextNotSetError(QuixException): ...
 
 
 def set_message_context(context: Optional[MessageContext]):
     """
     Set a MessageContext for the current message in the given `contextvars.Context`
 
     >***NOTE:*** This is for advanced usage only. If you need to change the message key,
```

## quixstreams/rowconsumer.py

```diff
@@ -39,25 +39,23 @@
 
 class RowConsumerProto(Protocol):
     def commit(
         self,
         message=None,
         offsets: List[TopicPartition] = None,
         asynchronous: bool = True,
-    ) -> Optional[List[TopicPartition]]:
-        ...
+    ) -> Optional[List[TopicPartition]]: ...
 
     def subscribe(
         self,
         topics: List[Topic],
         on_assign: Optional[RebalancingCallback] = None,
         on_revoke: Optional[RebalancingCallback] = None,
         on_lost: Optional[RebalancingCallback] = None,
-    ):
-        ...
+    ): ...
 
 
 class RowConsumer(Consumer, RowConsumerProto):
     def __init__(
         self,
         broker_address: str,
         consumer_group: str,
```

## quixstreams/rowproducer.py

```diff
@@ -14,16 +14,15 @@
     def produce_row(
         self,
         row: Row,
         topic: Topic,
         key: Optional[Any] = None,
         partition: Optional[int] = None,
         timestamp: Optional[int] = None,
-    ):
-        ...
+    ): ...
 
 
 class RowProducer(Producer, RowProducerProto):
     """
     A producer class that is capable of serializing Rows to bytes and send them to Kafka.
     The serialization is performed according to the Topic serialization settings.
```

## quixstreams/core/stream/functions.py

```diff
@@ -16,16 +16,15 @@
 
 R = TypeVar("R")
 T = TypeVar("T")
 
 StreamCallable = Callable[[T], R]
 
 
-class Filtered(Exception):
-    ...
+class Filtered(Exception): ...
 
 
 class StreamFunction(abc.ABC):
     """
     A base class for all the streaming operations in Quix Streams.
 
     It provides two methods that return closures to be called on the input values:
```

## quixstreams/dataframe/base.py

```diff
@@ -4,17 +4,14 @@
 from quixstreams.core.stream import Stream, StreamCallable
 from quixstreams.models.messagecontext import MessageContext
 
 
 class BaseStreaming:
     @property
     @abc.abstractmethod
-    def stream(self) -> Stream:
-        ...
+    def stream(self) -> Stream: ...
 
     @abc.abstractmethod
-    def compose(self, *args, **kwargs) -> StreamCallable:
-        ...
+    def compose(self, *args, **kwargs) -> StreamCallable: ...
 
     @abc.abstractmethod
-    def test(self, value: Any, ctx: Optional[MessageContext] = None) -> Any:
-        ...
+    def test(self, value: Any, ctx: Optional[MessageContext] = None) -> Any: ...
```

## quixstreams/dataframe/dataframe.py

```diff
@@ -443,19 +443,18 @@
 
         Notes:
 
         - Every window is grouped by the current Kafka message key.
         - Messages with `None` key will be ignored.
         - The time windows always use the current event time.
 
-        Example Snippet:
 
         Example Snippet:
-        ```python
 
+        ```python
         app = Application()
         sdf = app.dataframe(...)
 
         sdf = (
             # Define a hopping window of 60s with step 30s and grace period of 10s
             sdf.hopping_window(
                 duration_ms=timedelta(seconds=60),
@@ -553,20 +552,18 @@
                 lambda v: operator.setitem(v, key, value_composed(v))
             )
         else:
             stream = self.stream.add_update(lambda v: operator.setitem(v, key, value))
         self._stream = stream
 
     @overload
-    def __getitem__(self, item: str) -> StreamingSeries:
-        ...
+    def __getitem__(self, item: str) -> StreamingSeries: ...
 
     @overload
-    def __getitem__(self, item: Union[StreamingSeries, List[str], Self]) -> Self:
-        ...
+    def __getitem__(self, item: Union[StreamingSeries, List[str], Self]) -> Self: ...
 
     def __getitem__(
         self, item: Union[str, List[str], StreamingSeries, Self]
     ) -> Union[Self, StreamingSeries]:
         if isinstance(item, StreamingSeries):
             # Filter SDF based on StreamingSeries
             item_composed = item.compose(allow_filters=False, allow_updates=False)
```

## quixstreams/dataframe/exceptions.py

```diff
@@ -1,8 +1,7 @@
 from quixstreams.exceptions.base import QuixException
 
 
 __all__ = ("InvalidOperation",)
 
 
-class InvalidOperation(QuixException):
-    ...
+class InvalidOperation(QuixException): ...
```

## quixstreams/dataframe/windows/definitions.py

```diff
@@ -46,16 +46,15 @@
 
     @abstractmethod
     def _create_window(
         self,
         func_name: str,
         aggregate_func: WindowAggregateFunc,
         merge_func: Optional[WindowMergeFunc] = None,
-    ) -> "FixedTimeWindow":
-        ...
+    ) -> "FixedTimeWindow": ...
 
     @property
     def duration_ms(self) -> int:
         return self._duration_ms
 
     @property
     def grace_ms(self) -> int:
```

## quixstreams/exceptions/assignment.py

```diff
@@ -6,9 +6,8 @@
 class PartitionAssignmentError(QuixException):
     """
     Error happened during partition rebalancing.
     Raised from `on_assign`, `on_revoke` and `on_lost` callbacks
     """
 
 
-class KafkaPartitionError(QuixException):
-    ...
+class KafkaPartitionError(QuixException): ...
```

## quixstreams/kafka/producer.py

```diff
@@ -159,15 +159,16 @@
 
     def flush(self, timeout: Optional[float] = None) -> int:
         """
         Wait for all messages in the Producer queue to be delivered.
 
         :param float timeout: time to attempt flushing (seconds).
             None or -1 is infinite. Default: None
-        :return: number of messages delivered
+
+        :return: number of messages remaining to flush
         """
         return self._producer.flush(timeout=timeout if timeout is not None else -1)
 
     @property
     def _producer(self) -> ConfluentProducer:
         if not self._inner_producer:
             self._inner_producer = ConfluentProducer(self._producer_config)
```

## quixstreams/models/types.py

```diff
@@ -16,36 +16,26 @@
     Use it to not depend on exact implementation and simplify testing.
 
     Instances of `confluent_kafka.Message` cannot be directly created from Python,
     see https://github.com/confluentinc/confluent-kafka-python/issues/1535.
 
     """
 
-    def headers(self, *args, **kwargs) -> Optional[MessageHeadersTuples]:
-        ...
+    def headers(self, *args, **kwargs) -> Optional[MessageHeadersTuples]: ...
 
-    def key(self, *args, **kwargs) -> Optional[MessageKey]:
-        ...
+    def key(self, *args, **kwargs) -> Optional[MessageKey]: ...
 
-    def offset(self, *args, **kwargs) -> int:
-        ...
+    def offset(self, *args, **kwargs) -> int: ...
 
-    def partition(self, *args, **kwargs) -> int:
-        ...
+    def partition(self, *args, **kwargs) -> int: ...
 
-    def timestamp(self, *args, **kwargs) -> Tuple[int, int]:
-        ...
+    def timestamp(self, *args, **kwargs) -> Tuple[int, int]: ...
 
-    def topic(self, *args, **kwargs) -> str:
-        ...
+    def topic(self, *args, **kwargs) -> str: ...
 
-    def value(self, *args, **kwargs) -> Optional[MessageValue]:
-        ...
+    def value(self, *args, **kwargs) -> Optional[MessageValue]: ...
 
-    def latency(self, *args, **kwargs) -> Optional[float]:
-        ...
+    def latency(self, *args, **kwargs) -> Optional[float]: ...
 
-    def leader_epoch(self, *args, **kwargs) -> Optional[int]:
-        ...
+    def leader_epoch(self, *args, **kwargs) -> Optional[int]: ...
 
-    def __len__(self) -> int:
-        ...
+    def __len__(self) -> int: ...
```

## quixstreams/models/serializers/base.py

```diff
@@ -64,16 +64,15 @@
 
     def _to_dict(self, value: Any) -> Union[Any, dict]:
         if self.column_name:
             return {self.column_name: value}
         return value
 
     @abc.abstractmethod
-    def __call__(self, *args, **kwargs) -> Any:
-        ...
+    def __call__(self, *args, **kwargs) -> Any: ...
 
 
 class Serializer(abc.ABC):
     """
     A base class for all Serializers
     """
 
@@ -87,16 +86,15 @@
         Keys must be strings, and values must be strings, bytes or None.
 
         :return: dict with headers
         """
         return {}
 
     @abc.abstractmethod
-    def __call__(self, *args, **kwargs) -> Union[str, bytes]:
-        ...
+    def __call__(self, *args, **kwargs) -> Union[str, bytes]: ...
 
 
 SerializerStr: TypeAlias = Literal[
     "str",
     "string",
     "bytes",
     "double",
```

## quixstreams/models/serializers/exceptions.py

```diff
@@ -16,40 +16,35 @@
     "ValueDeserializationError",
     "SerializerIsNotProvidedError",
     "DeserializerIsNotProvidedError",
     "IgnoreMessage",
 )
 
 
-class SerializationError(exceptions.QuixException, _SerializationError):
-    ...
+class SerializationError(exceptions.QuixException, _SerializationError): ...
 
 
-class KeyDeserializationError(exceptions.QuixException, _KeyDeserializationError):
-    ...
+class KeyDeserializationError(exceptions.QuixException, _KeyDeserializationError): ...
 
 
-class KeySerializationError(exceptions.QuixException, _KeySerializationError):
-    ...
+class KeySerializationError(exceptions.QuixException, _KeySerializationError): ...
 
 
-class ValueSerializationError(exceptions.QuixException, _ValueSerializationError):
-    ...
+class ValueSerializationError(exceptions.QuixException, _ValueSerializationError): ...
 
 
-class ValueDeserializationError(exceptions.QuixException, _ValueDeserializationError):
-    ...
+class ValueDeserializationError(
+    exceptions.QuixException, _ValueDeserializationError
+): ...
 
 
-class SerializerIsNotProvidedError(exceptions.QuixException):
-    ...
+class SerializerIsNotProvidedError(exceptions.QuixException): ...
 
 
-class DeserializerIsNotProvidedError(exceptions.QuixException):
-    ...
+class DeserializerIsNotProvidedError(exceptions.QuixException): ...
 
 
 class IgnoreMessage(exceptions.QuixException):
     """
     Raise this exception from Deserializer.__call__ in order to ignore the processing
     of the particular message.
     """
```

## quixstreams/models/topics/exceptions.py

```diff
@@ -1,21 +1,16 @@
 from quixstreams.exceptions import QuixException
 
 
-class TopicNotFoundError(QuixException):
-    ...
+class TopicNotFoundError(QuixException): ...
 
 
-class TopicConfigurationMismatch(QuixException):
-    ...
+class TopicConfigurationMismatch(QuixException): ...
 
 
-class CreateTopicTimeout(QuixException):
-    ...
+class CreateTopicTimeout(QuixException): ...
 
 
-class CreateTopicFailure(QuixException):
-    ...
+class CreateTopicFailure(QuixException): ...
 
 
-class TopicNameLengthExceeded(QuixException):
-    ...
+class TopicNameLengthExceeded(QuixException): ...
```

## quixstreams/models/topics/topic.py

```diff
@@ -79,16 +79,15 @@
                 f"valid options are {list(DESERIALIZERS.keys())}"
             )
     return deserializer
 
 
 class Topic:
     """
-    A representation of a Kafka topic and its expected data format via
-    designated key and value serializers/deserializers.
+    A definition of a Kafka topic.
 
     Typically created with an `app = quixstreams.app.Application()` instance via
     `app.topic()`, and used by `quixstreams.dataframe.StreamingDataFrame`
     instance.
     """
 
     def __init__(
@@ -98,57 +97,22 @@
         value_deserializer: Optional[DeserializerType] = None,
         key_deserializer: Optional[DeserializerType] = BytesDeserializer(),
         value_serializer: Optional[SerializerType] = None,
         key_serializer: Optional[SerializerType] = BytesSerializer(),
         timestamp_extractor: Optional[TimestampExtractor] = None,
     ):
         """
-        Can specify serialization that should be used when consuming/producing
-        to the topic in the form of a string name (i.e. "json" for JSON) or a
-        serialization class instance directly, like JSONSerializer().
-
-
-        Example Snippet:
-
-        ```python
-        from quixstreams.dataframe import StreamingDataFrame
-        from quixstreams.models import Topic, JSONSerializer
-
-        # Specify an input and output topic for a `StreamingDataFrame` instance,
-        # where the output topic requires adjusting the key serializer.
-        input_topic = Topic("input-topic", value_deserializer="json")
-        output_topic = Topic(
-            "output-topic", key_serializer="str", value_serializer=JSONSerializer()
-        )
-        sdf = StreamingDataFrame(input_topic)
-        sdf.to_topic(output_topic)
-        ```
-
-
         :param name: topic name
         :param value_deserializer: a deserializer type for values
         :param key_deserializer: a deserializer type for keys
         :param value_serializer: a serializer type for values
         :param key_serializer: a serializer type for keys
         :param config: optional topic configs via `TopicConfig` (creation/validation)
         :param timestamp_extractor: a callable that returns a timestamp in
             milliseconds from a deserialized message.
-
-        Example Snippet:
-
-        ```python
-        def custom_ts_extractor(
-            value: Any,
-            headers: Optional[List[Tuple[str, bytes]]],
-            timestamp: float,
-            timestamp_type: TimestampType,
-        ) -> int:
-            return value["timestamp"]
-        topic = Topic("input-topic", timestamp_extractor=custom_ts_extractor)
-        ```
         """
         self._name = name
         self._config = config
         self._key_serializer = _get_serializer(key_serializer)
         self._key_deserializer = _get_deserializer(key_deserializer)
         self._value_serializer = _get_serializer(value_serializer)
         self._value_deserializer = _get_deserializer(value_deserializer)
```

## quixstreams/platforms/quix/__init__.py

```diff
@@ -1,4 +1,5 @@
 from .api import *
 from .checks import *
 from .config import *
+from .exceptions import *
 from .topic_manager import *
```

## quixstreams/platforms/quix/api.py

```diff
@@ -1,16 +1,20 @@
 from io import BytesIO
 from typing import Optional, List, Literal
 from urllib.parse import urljoin
 from zipfile import ZipFile
 
 import requests
 
-from quixstreams.exceptions import QuixException
 from .env import QUIX_ENVIRONMENT
+from .exceptions import (
+    UndefinedQuixWorkspaceId,
+    MissingConnectionRequirements,
+    QuixApiRequestFailure,
+)
 
 __all__ = ("QuixPortalApiService",)
 DEFAULT_PORTAL_API_URL = "https://portal-api.platform.quix.io/"
 
 
 class QuixPortalApiService:
     """
@@ -24,67 +28,85 @@
     else is required. Non-200's will raise exceptions.
 
     See the swagger documentation for more info about the endpoints.
     """
 
     def __init__(
         self,
-        portal_api: Optional[str] = None,
         auth_token: Optional[str] = None,
+        portal_api: Optional[str] = None,
         api_version: Optional[str] = None,
         default_workspace_id: Optional[str] = None,
     ):
         self._portal_api = (
             portal_api or QUIX_ENVIRONMENT.portal_api or DEFAULT_PORTAL_API_URL
         )
         self._auth_token = auth_token or QUIX_ENVIRONMENT.sdk_token
+        if not self._auth_token:
+            raise MissingConnectionRequirements(
+                f"A Quix Cloud auth token (SDK or PAT) is required; "
+                f"set with environment variable {QUIX_ENVIRONMENT.SDK_TOKEN}"
+            )
         self._default_workspace_id = (
             default_workspace_id or QUIX_ENVIRONMENT.workspace_id
         )
         self.api_version = api_version or "2.0"
         self.session = self._init_session()
 
-    class MissingConnectionRequirements(QuixException):
-        ...
-
-    class UndefinedQuixWorkspaceId(QuixException):
-        ...
-
     class SessionWithUrlBase(requests.Session):
         def __init__(self, url_base: str):
             self.url_base = url_base
             super().__init__()
 
         def request(self, method, url, **kwargs):
             timeout = kwargs.pop("timeout", 10)
             return super().request(
                 method, urljoin(base=self.url_base, url=url), timeout=timeout, **kwargs
             )
 
     @property
     def default_workspace_id(self) -> str:
         if not self._default_workspace_id:
-            raise self.UndefinedQuixWorkspaceId("You must provide a Quix Workspace ID")
+            raise UndefinedQuixWorkspaceId(
+                f"A Quix Cloud Workspace ID is required; "
+                f"set with environment variable {QUIX_ENVIRONMENT.WORKSPACE_ID}"
+            )
         return self._default_workspace_id
 
     @default_workspace_id.setter
     def default_workspace_id(self, value):
         self._default_workspace_id = value
 
+    def _response_handler(self, r: requests.Response, *args, **kwargs):
+        """
+        Custom callback/hook that is called after receiving a request.Response
+
+        Catches non-200's and passes both the original exception and the Response body.
+
+        Note: *args and **kwargs expected for hook
+        """
+        try:
+            r.raise_for_status()
+        except requests.exceptions.HTTPError as e:
+            try:
+                reason_text = e.response.json()
+            except requests.exceptions.JSONDecodeError:
+                reason_text = e.response.text
+
+            if reason_text:
+                e = (
+                    f'Error {e.response.status_code} for url "{e.response.url}": '
+                    f"{reason_text}"
+                )
+
+            raise QuixApiRequestFailure(e)
+
     def _init_session(self) -> SessionWithUrlBase:
-        if not (self._portal_api and self._auth_token):
-            missing = filter(
-                None,
-                [self._portal_api or "portal_api", self._auth_token or "auth_token"],
-            )
-            raise self.MissingConnectionRequirements(
-                f"Using the API requires a portal and token. Missing: {missing}"
-            )
         s = self.SessionWithUrlBase(self._portal_api)
-        s.hooks = {"response": lambda r, *args, **kwargs: r.raise_for_status()}
+        s.hooks = {"response": self._response_handler}
         s.headers.update(
             {
                 "X-Version": self.api_version,
                 "Authorization": f"Bearer {self._auth_token}",
             }
         )
         return s
```

## quixstreams/platforms/quix/config.py

```diff
@@ -95,26 +95,35 @@
 
     It also currently handles the app_auto_create_topics setting for Application.Quix.
     """
 
     # TODO: Consider a workspace class?
     def __init__(
         self,
-        quix_portal_api_service: Optional[QuixPortalApiService] = None,
+        quix_sdk_token: Optional[str] = None,
         workspace_id: Optional[str] = None,
         workspace_cert_path: Optional[str] = None,
+        quix_portal_api_service: Optional[QuixPortalApiService] = None,
     ):
         """
         :param quix_portal_api_service: A QuixPortalApiService instance (else generated)
         :param workspace_id: A valid Quix Workspace ID (else searched for)
         :param workspace_cert_path: path to an existing workspace cert (else retrieved)
         """
-        self.api = quix_portal_api_service or QuixPortalApiService(
-            default_workspace_id=workspace_id
-        )
+        if quix_sdk_token:
+            self.api = QuixPortalApiService(
+                default_workspace_id=workspace_id, auth_token=quix_sdk_token
+            )
+        elif quix_portal_api_service:
+            self.api = quix_portal_api_service
+        else:
+            raise ValueError(
+                'Either "quix_sdk_token" or "quix_portal_api_service" must be provided'
+            )
+
         try:
             self._workspace_id = workspace_id or self.api.default_workspace_id
         except self.api.UndefinedQuixWorkspaceId:
             self._workspace_id = None
             logger.warning(
                 "No workspace ID was provided directly or found via environment; "
                 "if there happens to be only one valid workspace for your provided "
@@ -124,25 +133,21 @@
             )
         self._workspace_cert_path = workspace_cert_path
         self._confluent_broker_config = None
         self._quix_broker_config = None
         self._quix_broker_settings = None
         self._workspace_meta = None
 
-    class NoWorkspaceFound(QuixException):
-        ...
+    class NoWorkspaceFound(QuixException): ...
 
-    class MultipleWorkspaces(QuixException):
-        ...
+    class MultipleWorkspaces(QuixException): ...
 
-    class MissingQuixTopics(QuixException):
-        ...
+    class MissingQuixTopics(QuixException): ...
 
-    class CreateTopicTimeout(QuixException):
-        ...
+    class CreateTopicTimeout(QuixException): ...
 
     @property
     def workspace_id(self) -> str:
         if not self._workspace_id:
             self.get_workspace_info()
         return self._workspace_id
```

## quixstreams/platforms/quix/env.py

```diff
@@ -49,22 +49,14 @@
         Return Quix Portal API url if set
 
         :return: portal API URL or None
         """
         return os.environ.get(self.PORTAL_API)
 
     @property
-    def sdk_token(self) -> Optional[str]:
-        """
-        Return Quix SDK token if set
-        :return: sdk token or None
-        """
-        return os.environ.get(self.SDK_TOKEN)
-
-    @property
     def state_dir(self) -> str:
         """
         Return application state directory on Quix.
         :return: path to state dir
         """
         # TODO: Use env variables instead when they're available
         return "/app/state"
```

## quixstreams/platforms/quix/topic_manager.py

```diff
@@ -1,8 +1,8 @@
-from typing import Optional, List
+from typing import List
 
 from quixstreams.models.topics import TopicManager, TopicAdmin, Topic
 from .config import QuixKafkaConfigsBuilder
 
 __all__ = ("QuixTopicManager",)
 
 
@@ -26,25 +26,25 @@
 
     _changelog_extra_config_defaults = {"cleanup.policy": "compact"}
     _changelog_extra_config_imports_defaults = {"retention.bytes", "retention.ms"}
 
     def __init__(
         self,
         topic_admin: TopicAdmin,
+        quix_config_builder: QuixKafkaConfigsBuilder,
         create_timeout: int = 60,
-        quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,
     ):
         """
         :param topic_admin: an `Admin` instance
         :param create_timeout: timeout for topic creation
         :param quix_config_builder: A QuixKafkaConfigsBuilder instance, else one is
             generated for you.
         """
         super().__init__(create_timeout=create_timeout, topic_admin=topic_admin)
-        self._quix_config_builder = quix_config_builder or QuixKafkaConfigsBuilder()
+        self._quix_config_builder = quix_config_builder
 
     def _create_topics(self, topics: List[Topic]):
         """
         Method that actually creates the topics in Kafka via the
         QuixConfigBuilder instance.
 
         :param topics: list of `Topic`s
```

## quixstreams/state/exceptions.py

```diff
@@ -1,21 +1,16 @@
 from quixstreams.exceptions import QuixException
 
 
-class PartitionNotAssignedError(QuixException):
-    ...
+class PartitionNotAssignedError(QuixException): ...
 
 
-class PartitionStoreIsUsed(QuixException):
-    ...
+class PartitionStoreIsUsed(QuixException): ...
 
 
-class StoreNotRegisteredError(QuixException):
-    ...
+class StoreNotRegisteredError(QuixException): ...
 
 
-class WindowedStoreAlreadyRegisteredError(QuixException):
-    ...
+class WindowedStoreAlreadyRegisteredError(QuixException): ...
 
 
-class InvalidStoreTransactionStateError(QuixException):
-    ...
+class InvalidStoreTransactionStateError(QuixException): ...
```

## quixstreams/state/types.py

```diff
@@ -73,19 +73,17 @@
 
     def close(self):
         """
         Close store and revoke all store partitions
         """
         ...
 
-    def __enter__(self):
-        ...
+    def __enter__(self): ...
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        ...
+    def __exit__(self, exc_type, exc_val, exc_tb): ...
 
 
 class StorePartition(Protocol):
     """
     A base class to access state in the underlying storage.
     It represents a single instance of some storage (e.g. a single database for
     the persistent storage).
@@ -239,19 +237,17 @@
 
     def maybe_flush(self, offset: Optional[int] = None):
         """
         Flush the recent updates and last processed offset to the storage.
         :param offset: offset of the last processed message, optional.
         """
 
-    def __enter__(self):
-        ...
+    def __enter__(self): ...
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        ...
+    def __exit__(self, exc_type, exc_val, exc_tb): ...
 
 
 class WindowedState(Protocol):
     """
     A windowed state to be provided into `StreamingDataFrame` window functions.
     """
 
@@ -307,16 +303,15 @@
         :param grace_ms: grace period in milliseconds. Default - "0"
         """
         ...
 
 
 class WindowedPartitionTransaction(WindowedState):
     @property
-    def state(self) -> WindowedState:
-        ...
+    def state(self) -> WindowedState: ...
 
     @property
     def failed(self) -> bool:
         """
         Return `True` if transaction failed to update data at some point.
 
         Failed transactions cannot be re-used.
@@ -347,27 +342,24 @@
 
     def maybe_flush(self, offset: Optional[int] = None):
         """
         Flush the recent updates and last processed offset to the storage.
         :param offset: offset of the last processed message, optional.
         """
 
-    def __enter__(self):
-        ...
+    def __enter__(self): ...
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        ...
+    def __exit__(self, exc_type, exc_val, exc_tb): ...
 
 
 class PartitionRecoveryTransaction(Protocol):
     """
     A class for managing recovery for a StorePartition from a changelog message
     """
 
-    def write_from_changelog_message(self):
-        ...
+    def write_from_changelog_message(self): ...
 
     def flush(self):
         """
         Flush the recovery update and last processed offset to the storage.
         """
         ...
```

## quixstreams/state/rocksdb/exceptions.py

```diff
@@ -6,33 +6,26 @@
     "NestedPrefixError",
     "ColumnFamilyDoesNotExist",
     "ColumnFamilyAlreadyExists",
     "ColumnFamilyHeaderMissing",
 )
 
 
-class StateError(QuixException):
-    ...
+class StateError(QuixException): ...
 
 
-class StateSerializationError(StateError):
-    ...
+class StateSerializationError(StateError): ...
 
 
-class StateTransactionError(StateError):
-    ...
+class StateTransactionError(StateError): ...
 
 
-class NestedPrefixError(StateError):
-    ...
+class NestedPrefixError(StateError): ...
 
 
-class ColumnFamilyDoesNotExist(StateError):
-    ...
+class ColumnFamilyDoesNotExist(StateError): ...
 
 
-class ColumnFamilyAlreadyExists(StateError):
-    ...
+class ColumnFamilyAlreadyExists(StateError): ...
 
 
-class ColumnFamilyHeaderMissing(StateError):
-    ...
+class ColumnFamilyHeaderMissing(StateError): ...
```

## quixstreams/state/rocksdb/options.py

```diff
@@ -31,23 +31,23 @@
     :param open_max_retries: number of times to retry opening the database
             if it's locked by another process. To disable retrying, pass 0
     :param open_retry_backoff: number of seconds to wait between each retry.
 
     Please see `rocksdict.Options` for a complete description of other options.
     """
 
-    write_buffer_size: int = 64 * 1024 * 1024  # 64MB
-    target_file_size_base: int = 64 * 1024 * 1024  # 64MB
+    write_buffer_size: int = 64 * 1024 * 1024
+    target_file_size_base: int = 64 * 1024 * 1024
     max_write_buffer_number: int = 3
-    block_cache_size: int = 128 * 1024 * 1024  # 128MB
+    block_cache_size: int = 128 * 1024 * 1024
     bloom_filter_bits_per_key: int = 10
     enable_pipelined_write: bool = False
     compression_type: CompressionType = "lz4"
     wal_dir: Optional[str] = None
-    max_total_wal_size: int = 128 * 1024 * 1024  # 128MB
+    max_total_wal_size: int = 128 * 1024 * 1024
     db_log_dir: Optional[str] = None
     dumps: DumpsFunc = dumps
     loads: LoadsFunc = loads
     open_max_retries: int = 10
     open_retry_backoff: float = 3.0
 
     def to_options(self) -> rocksdict.Options:
```

## quixstreams/state/rocksdb/types.py

```diff
@@ -17,9 +17,8 @@
     wal_dir: Optional[str]
     db_log_dir: Optional[str]
     dumps: DumpsFunc
     loads: LoadsFunc
     open_max_retries: int
     open_retry_backoff: float
 
-    def to_options(self) -> rocksdict.Options:
-        ...
+    def to_options(self) -> rocksdict.Options: ...
```

## Comparing `quixstreams-2.3.3.dist-info/LICENSE` & `quixstreams-2.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `quixstreams-2.3.3.dist-info/METADATA` & `quixstreams-2.4.0.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: quixstreams
-Version: 2.3.3
+Version: 2.4.0
 Summary: Python library for building stream processing applications with Apache Kafka
 Home-page: https://github.com/quixio/quix-streams
 Author: Quix Analytics Ltd
 Author-email: devs@quix.io
 License: Apache 2.0
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
@@ -59,14 +59,16 @@
 ```
 
 #### Requirements
 Python 3.8+, Apache Kafka 0.10+
 
 See [requirements.txt](https://github.com/quixio/quix-streams/blob/main/requirements.txt) for the full list of requirements
 
+## Documentation
+[Quix Streams Docs](https://quix.io/docs/quix-streams/introduction.html)
 
 ### Example Application
 
 Here's an example of how to <b>process</b> data from a Kafka Topic with Quix Streams:
 
 ```python
 from quixstreams import Application, State
@@ -141,58 +143,49 @@
 - Process it with your `StreamingDataFrame`.
 - Produce it to the output topic.
 - Automatically commit the topic offset and state updates after the message is processed.
 - React to Kafka rebalancing updates and manage the topic partitions.
 - Manage the State store.
 - Handle OS signals and gracefully exit the application.
 
-### More Examples
-> You may find more examples in the `examples` folder **[here](https://github.com/quixio/quix-streams/tree/main/examples)**.
+### Tutorials
 
-### Advanced Usage
+To see Quix Streams in action, check out the Quickstart and Tutorials in the docs: 
 
-For more in-depth description of Quix Streams components, please
-follow these links:
-- ***[StreamingDataFrame](https://github.com/quixio/quix-streams/blob/main/docs/streamingdataframe.md)***.
-- ***[Serialization](https://github.com/quixio/quix-streams/blob/main/docs/serialization.md)***.
-- ***[Stateful Processing](https://github.com/quixio/quix-streams/blob/main/docs/stateful-processing.md)***.
-- ***[Usage with Quix SaaS Platform](https://github.com/quixio/quix-streams/blob/main/docs/quix-platform.md)***.
-- ***[Upgrading from Quix Streams <2.0](https://github.com/quixio/quix-streams/blob/main/docs/upgrading-legacy.md)***.
+- [**Quickstart**](https://quix.io/docs/quix-streams/quickstart.html)
+- [**Tutorial - Word Count**](tutorials/word-count/tutorial.md)
+- [**Tutorial - Anomaly Detection**](tutorials/anomaly-detection/tutorial.md)
+- [**Tutorial - Purchase Filtering**](tutorials/purchase-filtering/tutorial.md)
 
 
-### Using the [Quix Platform](https://quix.io/) - `Application.Quix()`
+### Using the [Quix Cloud](https://quix.io/)
 
-This library doesn't have any dependency on any commercial products, but if you use it together with Quix SaaS Platform you will get some advantages out of the box during your development process such as:
+This library doesn't have any dependency on any commercial products, but if you use it together with Quix Cloud you will get some advantages out of the box during your development process such as:
 - Auto-configuration.
 - Monitoring.
 - Data explorer.
 - Data persistence.
 - Pipeline visualization.
 - Metrics.
 
 and more.
 
-Quix Streams provides a seamless integration with Quix Platform via `Application.Quix()` class.
-This class will automatically configure the Application using Quix SDK Token.
-<br>
-If you are running this within the Quix platform it will be configured 
-automatically.
-<br>
-Otherwise, please see 
-[**Quix Platform Configuration**](https://github.com/quixio/quix-streams/blob/main/docs/quix-platform.md).
+Quix Streams provides a seamless integration with Quix Cloud, and it can automatically configure the `Application` using Quix SDK Token.
 
+Please see the [**Connecting to Quix Cloud**](https://quix.io/docs/quix-streams/quix-platform.html) page 
+to learn how to use Quix Streams and Quix Cloud together.
 
 ### What's Next
 
 This library is being actively developed. 
 
 Here are some of the planned improvements:
 
 - [x] [Windowed aggregations over Tumbling & Hopping windows](https://quix.io/docs/quix-streams/v2-0-latest/windowing.html)
-- [x] State recovery based on Kafka changelog topics
+- [x] [State recovery based on Kafka changelog topics](https://quix.io/docs/quix-streams/advanced/stateful-processing.html#fault-tolerance-recovery)
 - [ ] Windowed aggregations over Sliding windows
 - [ ] Group-bys and joins (for merging topics/keys)
 - [ ] Support for "exactly-once" Kafka processing (aka transactions)
 - [ ] Support for Avro and Protobuf formats
 - [ ] Schema Registry support
```

## Comparing `quixstreams-2.3.3.dist-info/RECORD` & `quixstreams-2.4.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,82 +1,83 @@
-quixstreams/__init__.py,sha256=1RAqD_epb06mnJGOshsY3wMPWaQqGttxxruKkKC-MeM,163
-quixstreams/app.py,sha256=nhnm-gMAdsHir3bQCpsgi-XhULDfJBOW5EUy7rOPPGc,33054
-quixstreams/context.py,sha256=fYVgDXlpODSOMxb8yB_9em5SgeYevKziUsgxP-LC3B0,2563
+quixstreams/__init__.py,sha256=UXcE4Lmc7Ac_Wy4e1hoiBppTYbO6oglgxYCcm3dri8w,163
+quixstreams/app.py,sha256=0Zk2AjFP-Q9xo5ItOPNBVEmmrSk2uNXJPaKRPUE2ie8,35815
+quixstreams/context.py,sha256=Q7SnL6bKMrcDzE-FQfpcchuheQFZ2HlBmqGCVgiWrDU,2559
 quixstreams/error_callbacks.py,sha256=RLvvlEK0hq_XtI-irJiNJf9sEFx9K1QCFZXx_3CMbWU,1561
 quixstreams/logging.py,sha256=b_M0GPJd3vjXWquqK0tsZkn69j-RcSikAzV1yTDFFIo,1409
-quixstreams/rowconsumer.py,sha256=TMuA5Q-UpNN3HSNOHV2pdjr5hggKfTXtWkPsZiJOA_E,7210
-quixstreams/rowproducer.py,sha256=XiumabkC4Bd-pTc9BEHdQEB1qCNW-pXvtZb9RxoVZJc,3934
+quixstreams/rowconsumer.py,sha256=rD0Sj4bwUjI_bPcCESl1CvWxC4UioVWoqS37V_BLbMA,7194
+quixstreams/rowproducer.py,sha256=JxAdgT-Cc08Hcy1m_ajGTPGCgZQWwkPW1shNDTboYCA,3926
 quixstreams/types.py,sha256=xX1rTm6VaaPABGo1KkDlaaw-t45ssEHX3oGkW9X8Tp4,112
 quixstreams/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 quixstreams/core/stream/__init__.py,sha256=Oz8M-gUISeCwAuoUg98KRWaeFCL7xvUjCNQJn08BaJM,47
-quixstreams/core/stream/functions.py,sha256=UXfHca5gBjTOWQD8McUu8lgrFINUtyL3L1gvDAhVU1k,7682
+quixstreams/core/stream/functions.py,sha256=yIaXb8zF7NJ_ZInoQv11c02kuWoEQxsbnvSyMxaEX_A,7678
 quixstreams/core/stream/stream.py,sha256=dYo9Snw7_Z6qL2QLDUqMN9IEVu18BW_2F-dYLRmwSf4,8070
 quixstreams/dataframe/__init__.py,sha256=nh3h1dxgNBouUPwzcfNec5FOds_EURLBSz9h2zhNwWE,90
-quixstreams/dataframe/base.py,sha256=VTNKgWKxcEO_itjCTfAKBt1ymC1gY0OtNWOsN9YxJ_8,479
-quixstreams/dataframe/dataframe.py,sha256=eKs5ToGuLuaUeicy8eAXeZCpxPXf4Sc-9BWSc3vBF7s,21469
-quixstreams/dataframe/exceptions.py,sha256=OA2bpGuRsvwyuNEu8L3zCkuw0yqveQ7rOMdmLAve9wo,137
+quixstreams/dataframe/base.py,sha256=3I2-CiQ6K8CPUAT_-n7wPwHSSBSBAhAXuGzksfulZsE,455
+quixstreams/dataframe/dataframe.py,sha256=mHqeiKARF_D5DNRrBcXohbbDMRWiSvWZx-dufjAAbwA,21428
+quixstreams/dataframe/exceptions.py,sha256=V2Y1GB0M_ph-ETdLLwUoi8FJP7kggWNjI6UvOIAzkTw,133
 quixstreams/dataframe/series.py,sha256=T2rSR3XZRe3xwa1EUL2K2qqU-LvKU3UmlXIoKfhVS4g,15568
 quixstreams/dataframe/utils.py,sha256=6clRYg-QJ6N1R2IDN1TB03ktZbIi6M7cOG38rxMfSCk,754
 quixstreams/dataframe/windows/__init__.py,sha256=-5vtNgs4D2FbaX4zZ_T1Yqxz61u6SuGUzABF5-CLQec,106
 quixstreams/dataframe/windows/base.py,sha256=XUrnyKwUEv24RJ8n-b5zqJsXaz5TngSd-dUM7SHdzmE,1314
-quixstreams/dataframe/windows/definitions.py,sha256=DTzs8ykTVDwyvIE9p_Erw_71clAcD4KSk0Ao2hOrhDI,10524
+quixstreams/dataframe/windows/definitions.py,sha256=f0OvmKZUOa97KPV-XWheDHDiu_W0mvUQY3PZvO94xhc,10516
 quixstreams/dataframe/windows/time_based.py,sha256=CYkaodn9VolPYgXtxfQL6mzeBAroB_f0CcubxFKp9Cc,7044
 quixstreams/exceptions/__init__.py,sha256=4LwGqKPaAD_pRdTdGEQqF4jEjz01sRqet87v3nyFJmg,46
-quixstreams/exceptions/assignment.py,sha256=tG03a0_lOaB9eKUJQeGrdvzNKzxGVrICO0xEiqqQCTI,326
+quixstreams/exceptions/assignment.py,sha256=35VZbpdAZiaKGBgRS9PYMCqdEbEI0B7MhO5xHJJ54LU,322
 quixstreams/exceptions/base.py,sha256=2sxMDcShtqCaehXyw6Hq7KqyTzBnCu_iiezfNpjJ77Y,41
 quixstreams/kafka/__init__.py,sha256=BlSg_2Qs105RFbYs4MOXrR0RiuUAbP0xeN9aYpzcSUI,48
 quixstreams/kafka/consumer.py,sha256=rEdI0uqeOQtNJQWek99vpRxwW-YP6FPGTOv-w0RLNgA,21935
-quixstreams/kafka/producer.py,sha256=B8TcuJrgAUUSbKZhv2nq2qjgovIKimYBoO8Am7JbRMc,6365
+quixstreams/kafka/producer.py,sha256=xwCLnH_piKY9kAbambln9UMs3ZcU9nG38wZQhwHnOl8,6375
 quixstreams/models/__init__.py,sha256=9vCE_QLiZ2hiX-B4D0f22pExs5PeNVSzM-9Eqdg38r4,146
 quixstreams/models/messagecontext.py,sha256=u7zCB2BOnlxRKLmDL4bpUlzE19Zr1-g4jjWpixZ0Y7E,1949
 quixstreams/models/messages.py,sha256=L5m4Ng52Z9wzRnnAQ9T_xYyTIUmMkRQk0rReCC1xeU8,485
 quixstreams/models/rows.py,sha256=4U4pTx8IKCkq6ZJhBfJpOw4BYDd4NB6b3Glw8CZmFV0,2229
 quixstreams/models/timestamps.py,sha256=HLpLWOlzp23pEeEBOLO4HXI19_qJ9H2e8ddb0Fki_TM,1813
-quixstreams/models/types.py,sha256=m5kTAL0z3WIPt5zu-GPlQMKGuViSegprZnwhPZAJOUU,1389
+quixstreams/models/types.py,sha256=ehZmNyNlJxLZZ0Jh2E2QWuf12YEpYQJTQUvl5ZQjMUI,1309
 quixstreams/models/serializers/__init__.py,sha256=CUQhMRYQQZj1vCPJPnbO98vdxJbTLTc7YTXuNGaoCp4,1016
-quixstreams/models/serializers/base.py,sha256=38RBzETBn-51nZomYINnqWtLtZPAsD60tsnUq6Y_NYQ,3173
-quixstreams/models/serializers/exceptions.py,sha256=UQ9CIjyZ83L_uDjO2jRREBKOBIGhbYKR2xF-afrHXlQ,1392
+quixstreams/models/serializers/base.py,sha256=_6J2Cgj9TF_TMTmNua4H2H5PFREd6RYlCt9ZrL_yXKI,3157
+quixstreams/models/serializers/exceptions.py,sha256=Zd3LNu0bMJNBKSrGX0bniy-Sh5Mist6n5jijalP2joU,1370
 quixstreams/models/serializers/json.py,sha256=z3LkGVVNJ7QdTwWCRFwYdXiDbggwMBhLDRQHG85-YbE,1936
 quixstreams/models/serializers/quix.py,sha256=pmuHIU_TzEOX2wEcGypwNJeMY7c0IWpauw84Ua_2wFo,16878
 quixstreams/models/serializers/simple_types.py,sha256=6mlxn4F-7S8KcDiiECyICykm-Zi2I5m7CqzH8JPAAa8,4516
 quixstreams/models/topics/__init__.py,sha256=6bi-O9GMdc08ouk5RCOPClwg8WCIeXwPo0tTmoDC00c,65
 quixstreams/models/topics/admin.py,sha256=brypreI1CciJkuqoh-rfPo4sTWloQ76g_G-PBCrE5K4,6536
-quixstreams/models/topics/exceptions.py,sha256=5UoGEMBVug1cZa_n9LEg1R6CRuy73CBqcdKFUCLLAg0,317
+quixstreams/models/topics/exceptions.py,sha256=BmJQiWvxIdhr15IKov_oDB1uKtxGP3Rarrqc2Lgnjwk,297
 quixstreams/models/topics/manager.py,sha256=Rkr-xWWXgqCt2PHPAxcGLi5T5turFUnwX88qE_MTjGw,12089
-quixstreams/models/topics/topic.py,sha256=va1z_KM9tF01fynqLinkd88-5Y1EZkr9TxRp7DcjrLg,10905
+quixstreams/models/topics/topic.py,sha256=byAAXo7duJ4bYOueYDOjvQWBCmyhTXNOj4XmIFTDGys,9598
 quixstreams/platforms/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-quixstreams/platforms/quix/__init__.py,sha256=XwSMUGv6ZAanf2WpfWGYwuoqcwit-mWe3QLhzK7cncQ,92
-quixstreams/platforms/quix/api.py,sha256=G1YRAwRDbRVV_0metzcsnWA4WybkBlr1g4D8Oahp2Sw,5634
+quixstreams/platforms/quix/__init__.py,sha256=pbNOUeJoyKUhv59fXCsQanwoEXTHcVtfduvVOLT3W3A,118
+quixstreams/platforms/quix/api.py,sha256=iHSV_T_f3eJmko9BWczs1rVNL68FdFXMuN4YkuYH4LM,6358
 quixstreams/platforms/quix/checks.py,sha256=2aYCmnDaGr57jjBarrDVRIfNbZ0ufbjxXdQw5qwqZgE,1531
-quixstreams/platforms/quix/config.py,sha256=j0NTNCr4Uht-qk1PaKAIsoo5otKSmvfkbQ0F7WEKsGw,20920
-quixstreams/platforms/quix/env.py,sha256=IQSIreUvSDeszKfKi_za2FIR409XEw9mN1Hd3qv4d2w,1972
-quixstreams/platforms/quix/topic_manager.py,sha256=8jPEv-AJLT4wvF2impHFVZzbxlEXBNEa5YAbqe91JBo,3024
+quixstreams/platforms/quix/config.py,sha256=HcL_ZM84egckQ5F6rHeHyxg4d5FBMAUIfzGarf6rTUA,21205
+quixstreams/platforms/quix/env.py,sha256=A8vdwz1E5obuUp4c9qCCuLRDixI9LnXj_SsdmFSUbjQ,1773
+quixstreams/platforms/quix/exceptions.py,sha256=ZWO8VnsaXVDXszoXXghoM_43f3u899d7IrqbKPj-yCs,329
+quixstreams/platforms/quix/topic_manager.py,sha256=xTNwOQH6EQUc5scOJ1Ke-9aAGzJ4XV-OhsckUxo02Yg,2968
 quixstreams/state/__init__.py,sha256=sXQzET5v9-J9nI-1WLwtX2_N52nE9YANsYnSRMbHgLM,68
-quixstreams/state/exceptions.py,sha256=Cea6TkM5TSZ6_cIbQlFMWYRoUE0eaMFpzs_YZb_qu0A,350
+quixstreams/state/exceptions.py,sha256=_sf4W7GxQOLBXISfm9_cg1z3_lJ2tCrOfCUXS_cqJkY,330
 quixstreams/state/manager.py,sha256=9zywugTrsH4FFzdjmzYQFo5M4DcjfXKDyyN-koa8tfo,12858
 quixstreams/state/recovery.py,sha256=vDKuF9Fz04yKahFkEPkqgZTQQwB9xhzwM4SWE2om6ZA,13403
 quixstreams/state/state.py,sha256=UMJEHVG0RaniEHO1ybJaggNlAb0onSTHm8bcVfPF-Ls,1510
-quixstreams/state/types.py,sha256=nmvLWlu_hyYzZtgXgsmCPvaWqO9LPahEO1D0QsQO-Mg,9911
+quixstreams/state/types.py,sha256=pvF8gee9dsJ9ozSzpo77MI9okNzjEpTqpY4FAXyeo6Q,9847
 quixstreams/state/rocksdb/__init__.py,sha256=QtR97lmKpIo0PlgrnOit48tACX3rl5Aw68KfwpP9axc,143
-quixstreams/state/rocksdb/exceptions.py,sha256=-PE3ayR4bpHhVyh6GUGyTcAav1vGmRLf0QwZMI6A_PQ,605
+quixstreams/state/rocksdb/exceptions.py,sha256=_dj57UDfxZkEepnemm4QW-of8TdS0U7egpGHrNU4emY,577
 quixstreams/state/rocksdb/metadata.py,sha256=fWrvOQlgnra6pSzQDpfKHFq9QmVIEevmG-CbCKbFVNY,253
-quixstreams/state/rocksdb/options.py,sha256=8M9p8Mxe4rzw0aBobzzngXhm7JmE5Ew8JEMO2fyL7Rk,2993
+quixstreams/state/rocksdb/options.py,sha256=1uIwRV6bukhvGsFnPbxvjwC-W0BWBGy6gZ5XjuJPDPU,2959
 quixstreams/state/rocksdb/partition.py,sha256=StA3qcL_T9Ck9-MHCNvDScFzkmhnXrd2id_E9GFcrXQ,12157
 quixstreams/state/rocksdb/serialization.py,sha256=S-aJezDDYl4aQwymCMNFYjhdK-8Y5GztM7bgrwJ2jzw,948
 quixstreams/state/rocksdb/store.py,sha256=vi25P8-L3PuXDObhVZTqpKZfL2K8WROH15kVn6TevNI,5423
 quixstreams/state/rocksdb/transaction.py,sha256=CusxOudV9qjlfn4F0a54at8QLi2eySNXkIybOMoIJkU,12099
-quixstreams/state/rocksdb/types.py,sha256=tA2AfW1-xB28eCbNcNy2h_MDMQjqb4cQBCSHUUyh9Jk,648
+quixstreams/state/rocksdb/types.py,sha256=oqcYMP7_7S25dvusZuKbBnSdTKel6uzRcZq-XLhSVOU,640
 quixstreams/state/rocksdb/windowed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 quixstreams/state/rocksdb/windowed/metadata.py,sha256=pfQ3eyxKMo3n8HltNM98MR6Xv5RdNlngHJLz0Sz7Kz0,118
 quixstreams/state/rocksdb/windowed/partition.py,sha256=jbyEP99HnjG9mbCZ7P8WqxIO4otUU1JMn7tRbXjcU4w,2467
 quixstreams/state/rocksdb/windowed/serialization.py,sha256=SznE2hw0DF3VXroXkM5m6xaFHsJKWvh-otbFhO22ILw,2014
 quixstreams/state/rocksdb/windowed/state.py,sha256=Y0BSaAbWQsr3LrLOAcd6pzKw69lZlee9uE1-rMHhC-w,2876
 quixstreams/state/rocksdb/windowed/store.py,sha256=zOabG0PwfkRhWWNjasrAI_tD4k_yLPrqD1qVbTPuD1M,2107
 quixstreams/state/rocksdb/windowed/transaction.py,sha256=9D_Nc7I5z1bBX4VcjMzTZIFg2q2EaR5t0DkDVrZe0LQ,7468
 quixstreams/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 quixstreams/utils/dicts.py,sha256=Yc8l-johZzEuWIm7vzmTwYXLj19MoK_0pqWMII_F11U,607
 quixstreams/utils/json.py,sha256=wBtRWEvo2nVfwFIsZVbNpr2hOoF4TuXnnNdcF24zow0,636
-quixstreams-2.3.3.dist-info/LICENSE,sha256=P12cSoW9dae7S1OZMTZLKVwDUvS8UYdAnVwSYrUmDRE,11353
-quixstreams-2.3.3.dist-info/METADATA,sha256=rKVwInRHlcS7bzhBMuF4N0A9M4uGtgfnesEaTQssptQ,9422
-quixstreams-2.3.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-quixstreams-2.3.3.dist-info/top_level.txt,sha256=Vk4q2NfXCIP82ZaTX5k5e6e9ao3IEvYPzquAl-1C7vo,12
-quixstreams-2.3.3.dist-info/RECORD,,
+quixstreams-2.4.0.dist-info/LICENSE,sha256=P12cSoW9dae7S1OZMTZLKVwDUvS8UYdAnVwSYrUmDRE,11353
+quixstreams-2.4.0.dist-info/METADATA,sha256=rZMIh9uOrr-DnxukTLe7EY67a964aHBuEfuQ88-tvcs,9072
+quixstreams-2.4.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+quixstreams-2.4.0.dist-info/top_level.txt,sha256=Vk4q2NfXCIP82ZaTX5k5e6e9ao3IEvYPzquAl-1C7vo,12
+quixstreams-2.4.0.dist-info/RECORD,,
```

