# Comparing `tmp/sparseml_nightly-1.8.0.20240401-py3-none-any.whl.zip` & `tmp/sparseml_nightly-1.8.0.20240404-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,569 +1,569 @@
-Zip file size: 1319161 bytes, number of entries: 567
--rw-r--r--  2.0 unx     1489 b- defN 24-Apr-01 00:45 sparseml/__init__.py
--rw-r--r--  2.0 unx      898 b- defN 24-Apr-01 00:45 sparseml/analytics.py
--rw-r--r--  2.0 unx    10284 b- defN 24-Apr-01 00:45 sparseml/base.py
--rw-r--r--  2.0 unx     7288 b- defN 24-Apr-01 00:45 sparseml/integration_helper_functions.py
--rw-r--r--  2.0 unx     2483 b- defN 24-Apr-01 00:45 sparseml/log.py
--rw-r--r--  2.0 unx     1607 b- defN 24-Apr-01 00:45 sparseml/version.py
--rw-r--r--  2.0 unx      758 b- defN 24-Apr-01 00:45 sparseml/benchmark/__init__.py
--rw-r--r--  2.0 unx    17763 b- defN 24-Apr-01 00:45 sparseml/benchmark/info.py
--rw-r--r--  2.0 unx    10778 b- defN 24-Apr-01 00:45 sparseml/benchmark/serialization.py
--rw-r--r--  2.0 unx      915 b- defN 24-Apr-01 00:45 sparseml/core/__init__.py
--rw-r--r--  2.0 unx     6842 b- defN 24-Apr-01 00:45 sparseml/core/event.py
--rw-r--r--  2.0 unx     6492 b- defN 24-Apr-01 00:45 sparseml/core/factory.py
--rw-r--r--  2.0 unx     2850 b- defN 24-Apr-01 00:45 sparseml/core/framework.py
--rw-r--r--  2.0 unx     2855 b- defN 24-Apr-01 00:45 sparseml/core/framework_object.py
--rw-r--r--  2.0 unx     3780 b- defN 24-Apr-01 00:45 sparseml/core/helpers.py
--rw-r--r--  2.0 unx    23720 b- defN 24-Apr-01 00:45 sparseml/core/session.py
--rw-r--r--  2.0 unx     9034 b- defN 24-Apr-01 00:45 sparseml/core/state.py
--rw-r--r--  2.0 unx      667 b- defN 24-Apr-01 00:45 sparseml/core/data/__init__.py
--rw-r--r--  2.0 unx     1565 b- defN 24-Apr-01 00:45 sparseml/core/data/base.py
--rw-r--r--  2.0 unx     6415 b- defN 24-Apr-01 00:45 sparseml/core/data/pytorch.py
--rw-r--r--  2.0 unx      678 b- defN 24-Apr-01 00:45 sparseml/core/lifecycle/__init__.py
--rw-r--r--  2.0 unx    11954 b- defN 24-Apr-01 00:45 sparseml/core/lifecycle/event.py
--rw-r--r--  2.0 unx     8242 b- defN 24-Apr-01 00:45 sparseml/core/lifecycle/session.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/core/logger/__init__.py
--rw-r--r--  2.0 unx    44703 b- defN 24-Apr-01 00:45 sparseml/core/logger/logger.py
--rw-r--r--  2.0 unx      667 b- defN 24-Apr-01 00:45 sparseml/core/logger/utils/__init__.py
--rw-r--r--  2.0 unx    12725 b- defN 24-Apr-01 00:45 sparseml/core/logger/utils/frequency_manager.py
--rw-r--r--  2.0 unx      693 b- defN 24-Apr-01 00:45 sparseml/core/model/__init__.py
--rw-r--r--  2.0 unx     5895 b- defN 24-Apr-01 00:45 sparseml/core/model/base.py
--rw-r--r--  2.0 unx     5641 b- defN 24-Apr-01 00:45 sparseml/core/model/pytorch.py
--rw-r--r--  2.0 unx      699 b- defN 24-Apr-01 00:45 sparseml/core/modifier/__init__.py
--rw-r--r--  2.0 unx     3326 b- defN 24-Apr-01 00:45 sparseml/core/modifier/base.py
--rw-r--r--  2.0 unx    10810 b- defN 24-Apr-01 00:45 sparseml/core/modifier/modifier.py
--rw-r--r--  2.0 unx     6093 b- defN 24-Apr-01 00:45 sparseml/core/modifier/stage.py
--rw-r--r--  2.0 unx      672 b- defN 24-Apr-01 00:45 sparseml/core/optimizer/__init__.py
--rw-r--r--  2.0 unx     3409 b- defN 24-Apr-01 00:45 sparseml/core/optimizer/base.py
--rw-r--r--  2.0 unx     4584 b- defN 24-Apr-01 00:45 sparseml/core/optimizer/pytorch.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-01 00:45 sparseml/core/recipe/__init__.py
--rw-r--r--  2.0 unx     6872 b- defN 24-Apr-01 00:45 sparseml/core/recipe/args.py
--rw-r--r--  2.0 unx     1599 b- defN 24-Apr-01 00:45 sparseml/core/recipe/base.py
--rw-r--r--  2.0 unx     6313 b- defN 24-Apr-01 00:45 sparseml/core/recipe/container.py
--rw-r--r--  2.0 unx     2932 b- defN 24-Apr-01 00:45 sparseml/core/recipe/metadata.py
--rw-r--r--  2.0 unx     4224 b- defN 24-Apr-01 00:45 sparseml/core/recipe/modifier.py
--rw-r--r--  2.0 unx    24978 b- defN 24-Apr-01 00:45 sparseml/core/recipe/recipe.py
--rw-r--r--  2.0 unx     7629 b- defN 24-Apr-01 00:45 sparseml/core/recipe/stage.py
--rw-r--r--  2.0 unx      663 b- defN 24-Apr-01 00:45 sparseml/core/utils/__init__.py
--rw-r--r--  2.0 unx     1025 b- defN 24-Apr-01 00:45 sparseml/core/utils/session_helpers.py
--rw-r--r--  2.0 unx      863 b- defN 24-Apr-01 00:45 sparseml/deepsparse/__init__.py
--rw-r--r--  2.0 unx     3516 b- defN 24-Apr-01 00:45 sparseml/deepsparse/base.py
--rw-r--r--  2.0 unx      801 b- defN 24-Apr-01 00:45 sparseml/deepsparse/framework/__init__.py
--rw-r--r--  2.0 unx     6032 b- defN 24-Apr-01 00:45 sparseml/deepsparse/framework/info.py
--rw-r--r--  2.0 unx      813 b- defN 24-Apr-01 00:45 sparseml/deepsparse/sparsification/__init__.py
--rw-r--r--  2.0 unx     1348 b- defN 24-Apr-01 00:45 sparseml/deepsparse/sparsification/info.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/evaluation/__init__.py
--rw-r--r--  2.0 unx     5689 b- defN 24-Apr-01 00:45 sparseml/evaluation/cli.py
--rw-r--r--  2.0 unx     2140 b- defN 24-Apr-01 00:45 sparseml/evaluation/evaluator.py
--rw-r--r--  2.0 unx      940 b- defN 24-Apr-01 00:45 sparseml/evaluation/integrations_config.yaml
--rw-r--r--  2.0 unx     5571 b- defN 24-Apr-01 00:45 sparseml/evaluation/registry.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/evaluation/integrations/__init__.py
--rw-r--r--  2.0 unx     6000 b- defN 24-Apr-01 00:45 sparseml/evaluation/integrations/lm_evaluation_harness.py
--rw-r--r--  2.0 unx    10987 b- defN 24-Apr-01 00:45 sparseml/evaluation/integrations/perplexity.py
--rw-r--r--  2.0 unx      661 b- defN 24-Apr-01 00:45 sparseml/export/__init__.py
--rw-r--r--  2.0 unx    21172 b- defN 24-Apr-01 00:45 sparseml/export/export.py
--rw-r--r--  2.0 unx     8049 b- defN 24-Apr-01 00:45 sparseml/export/export_data.py
--rw-r--r--  2.0 unx     2681 b- defN 24-Apr-01 00:45 sparseml/export/export_torch_model.py
--rw-r--r--  2.0 unx    13052 b- defN 24-Apr-01 00:45 sparseml/export/helpers.py
--rw-r--r--  2.0 unx     8850 b- defN 24-Apr-01 00:45 sparseml/export/validators.py
--rw-r--r--  2.0 unx      786 b- defN 24-Apr-01 00:45 sparseml/exporters/__init__.py
--rw-r--r--  2.0 unx     1477 b- defN 24-Apr-01 00:45 sparseml/exporters/base_exporter.py
--rw-r--r--  2.0 unx     6576 b- defN 24-Apr-01 00:45 sparseml/exporters/kv_cache_injector.py
--rw-r--r--  2.0 unx     5522 b- defN 24-Apr-01 00:45 sparseml/exporters/onnx_to_deepsparse.py
--rw-r--r--  2.0 unx     2350 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/__init__.py
--rw-r--r--  2.0 unx     2333 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/base_transform.py
--rw-r--r--  2.0 unx     1388 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/constants_to_initializers.py
--rw-r--r--  2.0 unx     4630 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/conv_to_convinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     5973 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/conv_to_qlinearconv.py
--rw-r--r--  2.0 unx     2440 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/delete_repeated_qdq.py
--rw-r--r--  2.0 unx     1842 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/delete_trivial_onnx_adds.py
--rw-r--r--  2.0 unx     2241 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/flatten_qparams.py
--rw-r--r--  2.0 unx     3553 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/fold_conv_div_bn.py
--rw-r--r--  2.0 unx     1669 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/fold_identity_initializers.py
--rw-r--r--  2.0 unx     2070 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/fold_relu_quants.py
--rw-r--r--  2.0 unx     4418 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/gemm_to_matmulinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     7629 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/gemm_to_qlinearmatmul.py
--rw-r--r--  2.0 unx     1645 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/initializers_to_uint8.py
--rw-r--r--  2.0 unx     6297 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/matmul_add_to_matmulinteger_add_cast_mul.py
--rw-r--r--  2.0 unx     4681 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/matmul_to_matmulinteger_cast_mul.py
--rw-r--r--  2.0 unx     4156 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/matmul_to_qlinearmatmul.py
--rw-r--r--  2.0 unx     3770 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/onnx_transform.py
--rw-r--r--  2.0 unx     3433 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/propagate_dequant_through_split.py
--rw-r--r--  2.0 unx     4801 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/propagate_embedding_quantization.py
--rw-r--r--  2.0 unx     4464 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/quantize_qat_embedding.py
--rw-r--r--  2.0 unx     3869 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/quantize_residuals.py
--rw-r--r--  2.0 unx     3331 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/remove_duplicate_qconv_weights.py
--rw-r--r--  2.0 unx     2545 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/remove_duplicate_quantize_ops.py
--rw-r--r--  2.0 unx     3210 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/skip_input_quantize.py
--rw-r--r--  2.0 unx     1373 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/unwrap_batchnorms.py
--rw-r--r--  2.0 unx      911 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/__init__.py
--rw-r--r--  2.0 unx    30558 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/cache_keys_and_values.py
--rw-r--r--  2.0 unx    10686 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/configs.py
--rw-r--r--  2.0 unx     7027 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/transforms_base.py
--rw-r--r--  2.0 unx     4974 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/transforms_codegen.py
--rw-r--r--  2.0 unx     8801 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/transforms_llama.py
--rw-r--r--  2.0 unx     4261 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/transforms_mpt.py
--rw-r--r--  2.0 unx     6066 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/kv_cache/transforms_opt.py
--rw-r--r--  2.0 unx      730 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/utils/__init__.py
--rw-r--r--  2.0 unx    11112 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/utils/add_quantized_conv_matmul_add_ops.py
--rw-r--r--  2.0 unx     6809 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/utils/helpers.py
--rw-r--r--  2.0 unx    14429 b- defN 24-Apr-01 00:45 sparseml/exporters/transforms/utils/matching.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-01 00:45 sparseml/framework/__init__.py
--rw-r--r--  2.0 unx     9479 b- defN 24-Apr-01 00:45 sparseml/framework/info.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/evaluator.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/trainer.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/data/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/metrics/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/model/__init__.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/integrations/torchvision/optim/__init__.py
--rw-r--r--  2.0 unx     1144 b- defN 24-Apr-01 00:45 sparseml/keras/__init__.py
--rw-r--r--  2.0 unx     8054 b- defN 24-Apr-01 00:45 sparseml/keras/base.py
--rw-r--r--  2.0 unx      943 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/__init__.py
--rw-r--r--  2.0 unx     3297 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/dataset.py
--rw-r--r--  2.0 unx     2423 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/helpers.py
--rw-r--r--  2.0 unx     2761 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/registry.py
--rw-r--r--  2.0 unx      786 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/classification/__init__.py
--rw-r--r--  2.0 unx     8369 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     4301 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     2727 b- defN 24-Apr-01 00:45 sparseml/keras/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx      793 b- defN 24-Apr-01 00:45 sparseml/keras/framework/__init__.py
--rw-r--r--  2.0 unx     5906 b- defN 24-Apr-01 00:45 sparseml/keras/framework/info.py
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-01 00:45 sparseml/keras/models/__init__.py
--rw-r--r--  2.0 unx    11814 b- defN 24-Apr-01 00:45 sparseml/keras/models/registry.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-01 00:45 sparseml/keras/models/classification/__init__.py
--rw-r--r--  2.0 unx    17932 b- defN 24-Apr-01 00:45 sparseml/keras/models/classification/resnet.py
--rw-r--r--  2.0 unx      768 b- defN 24-Apr-01 00:45 sparseml/keras/models/external/__init__.py
--rw-r--r--  2.0 unx     4402 b- defN 24-Apr-01 00:45 sparseml/keras/models/external/keras_applications.py
--rw-r--r--  2.0 unx     1166 b- defN 24-Apr-01 00:45 sparseml/keras/optim/__init__.py
--rw-r--r--  2.0 unx     5677 b- defN 24-Apr-01 00:45 sparseml/keras/optim/manager.py
--rw-r--r--  2.0 unx    14777 b- defN 24-Apr-01 00:45 sparseml/keras/optim/mask_pruning.py
--rw-r--r--  2.0 unx    19740 b- defN 24-Apr-01 00:45 sparseml/keras/optim/mask_pruning_creator.py
--rw-r--r--  2.0 unx     9183 b- defN 24-Apr-01 00:45 sparseml/keras/optim/modifier.py
--rw-r--r--  2.0 unx     1676 b- defN 24-Apr-01 00:45 sparseml/keras/optim/modifier_epoch.py
--rw-r--r--  2.0 unx    14736 b- defN 24-Apr-01 00:45 sparseml/keras/optim/modifier_lr.py
--rw-r--r--  2.0 unx     5477 b- defN 24-Apr-01 00:45 sparseml/keras/optim/modifier_params.py
--rw-r--r--  2.0 unx    24031 b- defN 24-Apr-01 00:45 sparseml/keras/optim/modifier_pruning.py
--rw-r--r--  2.0 unx     1133 b- defN 24-Apr-01 00:45 sparseml/keras/optim/utils.py
--rw-r--r--  2.0 unx      808 b- defN 24-Apr-01 00:45 sparseml/keras/sparsification/__init__.py
--rw-r--r--  2.0 unx     1356 b- defN 24-Apr-01 00:45 sparseml/keras/sparsification/info.py
--rw-r--r--  2.0 unx      962 b- defN 24-Apr-01 00:45 sparseml/keras/utils/__init__.py
--rw-r--r--  2.0 unx     8202 b- defN 24-Apr-01 00:45 sparseml/keras/utils/callbacks.py
--rw-r--r--  2.0 unx     1022 b- defN 24-Apr-01 00:45 sparseml/keras/utils/compat.py
--rw-r--r--  2.0 unx     5737 b- defN 24-Apr-01 00:45 sparseml/keras/utils/exporter.py
--rw-r--r--  2.0 unx     6087 b- defN 24-Apr-01 00:45 sparseml/keras/utils/logger.py
--rw-r--r--  2.0 unx     1738 b- defN 24-Apr-01 00:45 sparseml/keras/utils/model.py
--rw-r--r--  2.0 unx      800 b- defN 24-Apr-01 00:45 sparseml/modifiers/__init__.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/__init__.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/output/__init__.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/output/base.py
--rw-r--r--  2.0 unx     7612 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/output/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/utils/__init__.py
--rw-r--r--  2.0 unx      715 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx    13320 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/utils/pytorch/kd_factory.py
--rw-r--r--  2.0 unx     3921 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/utils/pytorch/kd_wrapper.py
--rw-r--r--  2.0 unx     4527 b- defN 24-Apr-01 00:45 sparseml/modifiers/distillation/utils/pytorch/model_wrapper.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/experimental/__init__.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/modifiers/logarithmic_equalization/__init__.py
--rw-r--r--  2.0 unx     2764 b- defN 24-Apr-01 00:45 sparseml/modifiers/logarithmic_equalization/base.py
--rw-r--r--  2.0 unx     1947 b- defN 24-Apr-01 00:45 sparseml/modifiers/logarithmic_equalization/pytorch.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/__init__.py
--rw-r--r--  2.0 unx     5181 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/base.py
--rw-r--r--  2.0 unx     3543 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/utils/__init__.py
--rw-r--r--  2.0 unx     6686 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/utils/helpers.py
--rw-r--r--  2.0 unx     6753 b- defN 24-Apr-01 00:45 sparseml/modifiers/obcq/utils/sgpt_wrapper.py
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/__init__.py
--rw-r--r--  2.0 unx     5725 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/helpers.py
--rw-r--r--  2.0 unx      676 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/constant/__init__.py
--rw-r--r--  2.0 unx      951 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/constant/base.py
--rw-r--r--  2.0 unx     3103 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/constant/pytorch.py
--rw-r--r--  2.0 unx      677 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/magnitude/__init__.py
--rw-r--r--  2.0 unx     1168 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/magnitude/base.py
--rw-r--r--  2.0 unx     5262 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/magnitude/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/utils/__init__.py
--rw-r--r--  2.0 unx      688 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx     5984 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/utils/pytorch/layer_mask.py
--rw-r--r--  2.0 unx     5622 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/utils/pytorch/mask_factory.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/wanda/__init__.py
--rw-r--r--  2.0 unx     3905 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/wanda/base.py
--rw-r--r--  2.0 unx    10346 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/wanda/pytorch.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/wanda/utils/__init__.py
--rw-r--r--  2.0 unx     4469 b- defN 24-Apr-01 00:45 sparseml/modifiers/pruning/wanda/utils/wanda_wrapper.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/__init__.py
--rw-r--r--  2.0 unx     5512 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/base.py
--rw-r--r--  2.0 unx     8924 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/__init__.py
--rw-r--r--  2.0 unx     2220 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/constants.py
--rw-r--r--  2.0 unx     2906 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py
--rw-r--r--  2.0 unx    32720 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/helpers.py
--rw-r--r--  2.0 unx    13545 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/quantization_scheme.py
--rw-r--r--  2.0 unx    19811 b- defN 24-Apr-01 00:45 sparseml/modifiers/quantization/utils/quantize.py
--rw-r--r--  2.0 unx      654 b- defN 24-Apr-01 00:45 sparseml/modifiers/smoothquant/__init__.py
--rw-r--r--  2.0 unx     7535 b- defN 24-Apr-01 00:45 sparseml/modifiers/smoothquant/base.py
--rw-r--r--  2.0 unx     8167 b- defN 24-Apr-01 00:45 sparseml/modifiers/smoothquant/pytorch.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/modifiers/utils/__init__.py
--rw-r--r--  2.0 unx     3944 b- defN 24-Apr-01 00:45 sparseml/modifiers/utils/compression_wrapper.py
--rw-r--r--  2.0 unx     5257 b- defN 24-Apr-01 00:45 sparseml/modifiers/utils/layer_compressor.py
--rw-r--r--  2.0 unx     3265 b- defN 24-Apr-01 00:45 sparseml/modifiers/utils/pytorch_helpers.py
--rw-r--r--  2.0 unx     1036 b- defN 24-Apr-01 00:45 sparseml/onnx/__init__.py
--rw-r--r--  2.0 unx     6202 b- defN 24-Apr-01 00:45 sparseml/onnx/base.py
--rw-r--r--  2.0 unx      743 b- defN 24-Apr-01 00:45 sparseml/onnx/benchmark/__init__.py
--rw-r--r--  2.0 unx    15366 b- defN 24-Apr-01 00:45 sparseml/onnx/benchmark/info.py
--rw-r--r--  2.0 unx      823 b- defN 24-Apr-01 00:45 sparseml/onnx/framework/__init__.py
--rw-r--r--  2.0 unx     6116 b- defN 24-Apr-01 00:45 sparseml/onnx/framework/info.py
--rw-r--r--  2.0 unx      820 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/__init__.py
--rw-r--r--  2.0 unx    13285 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/analyzer_model.py
--rw-r--r--  2.0 unx    19639 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx     6470 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/structured_pruning.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/quantization/__init__.py
--rw-r--r--  2.0 unx    14753 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/quantization/calibration.py
--rw-r--r--  2.0 unx    73551 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/quantization/quantize.py
--rw-r--r--  2.0 unx     4552 b- defN 24-Apr-01 00:45 sparseml/onnx/optim/quantization/quantize_model_post_training.py
--rw-r--r--  2.0 unx      869 b- defN 24-Apr-01 00:45 sparseml/onnx/sparsification/__init__.py
--rw-r--r--  2.0 unx    10209 b- defN 24-Apr-01 00:45 sparseml/onnx/sparsification/analyzer.py
--rw-r--r--  2.0 unx     1363 b- defN 24-Apr-01 00:45 sparseml/onnx/sparsification/info.py
--rw-r--r--  2.0 unx     8009 b- defN 24-Apr-01 00:45 sparseml/onnx/sparsification/model_info.py
--rw-r--r--  2.0 unx      867 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/__init__.py
--rw-r--r--  2.0 unx    13013 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/data.py
--rw-r--r--  2.0 unx    20691 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/graph_editor.py
--rw-r--r--  2.0 unx     8133 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/graph_optimizer.py
--rw-r--r--  2.0 unx    40230 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/helpers.py
--rw-r--r--  2.0 unx     1958 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/loss.py
--rw-r--r--  2.0 unx    31591 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/model.py
--rw-r--r--  2.0 unx     5437 b- defN 24-Apr-01 00:45 sparseml/onnx/utils/sparse_tensor.py
--rw-r--r--  2.0 unx      931 b- defN 24-Apr-01 00:45 sparseml/openpifpaf/__init__.py
--rw-r--r--  2.0 unx     3713 b- defN 24-Apr-01 00:45 sparseml/openpifpaf/export.py
--rw-r--r--  2.0 unx    10950 b- defN 24-Apr-01 00:45 sparseml/openpifpaf/train.py
--rw-r--r--  2.0 unx     4211 b- defN 24-Apr-01 00:45 sparseml/openpifpaf/trainer.py
--rw-r--r--  2.0 unx      882 b- defN 24-Apr-01 00:45 sparseml/optim/__init__.py
--rw-r--r--  2.0 unx     6302 b- defN 24-Apr-01 00:45 sparseml/optim/analyzer.py
--rw-r--r--  2.0 unx    25563 b- defN 24-Apr-01 00:45 sparseml/optim/helpers.py
--rw-r--r--  2.0 unx    25984 b- defN 24-Apr-01 00:45 sparseml/optim/manager.py
--rw-r--r--  2.0 unx    30708 b- defN 24-Apr-01 00:45 sparseml/optim/modifier.py
--rw-r--r--  2.0 unx    26315 b- defN 24-Apr-01 00:45 sparseml/optim/sensitivity.py
--rw-r--r--  2.0 unx     2190 b- defN 24-Apr-01 00:45 sparseml/pytorch/__init__.py
--rw-r--r--  2.0 unx     6154 b- defN 24-Apr-01 00:45 sparseml/pytorch/base.py
--rw-r--r--  2.0 unx      933 b- defN 24-Apr-01 00:45 sparseml/pytorch/opset.py
--rw-r--r--  2.0 unx    12086 b- defN 24-Apr-01 00:45 sparseml/pytorch/torch_to_onnx_exporter.py
--rw-r--r--  2.0 unx      998 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/__init__.py
--rw-r--r--  2.0 unx     4193 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/generic.py
--rw-r--r--  2.0 unx     3014 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/registry.py
--rw-r--r--  2.0 unx      828 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/__init__.py
--rw-r--r--  2.0 unx     4457 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/cifar.py
--rw-r--r--  2.0 unx     3669 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     4000 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     6491 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/classification/mnist.py
--rw-r--r--  2.0 unx      767 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/detection/__init__.py
--rw-r--r--  2.0 unx    16159 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/detection/coco.py
--rw-r--r--  2.0 unx     5705 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/detection/helpers.py
--rw-r--r--  2.0 unx    10759 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/detection/voc.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/image_classification/__init__.py
--rw-r--r--  2.0 unx     9512 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/image_classification/ffcv_dataset.py
--rw-r--r--  2.0 unx      684 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/recommendation/__init__.py
--rw-r--r--  2.0 unx      693 b- defN 24-Apr-01 00:45 sparseml/pytorch/datasets/video/__init__.py
--rw-r--r--  2.0 unx      814 b- defN 24-Apr-01 00:45 sparseml/pytorch/framework/__init__.py
--rw-r--r--  2.0 unx     5580 b- defN 24-Apr-01 00:45 sparseml/pytorch/framework/info.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/__init__.py
--rw-r--r--  2.0 unx    18265 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/export.py
--rw-r--r--  2.0 unx     4991 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/integration_helper_functions.py
--rw-r--r--  2.0 unx    15494 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/lr_analysis.py
--rw-r--r--  2.0 unx    14444 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/pr_sensitivity.py
--rw-r--r--  2.0 unx    29287 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/train.py
--rw-r--r--  2.0 unx      682 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/utils/__init__.py
--rw-r--r--  2.0 unx     4278 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/utils/cli_helpers.py
--rw-r--r--  2.0 unx     1257 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/utils/constants.py
--rw-r--r--  2.0 unx    23894 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/utils/helpers.py
--rw-r--r--  2.0 unx    12056 b- defN 24-Apr-01 00:45 sparseml/pytorch/image_classification/utils/trainer.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/pytorch/model_load/__init__.py
--rw-r--r--  2.0 unx    11440 b- defN 24-Apr-01 00:45 sparseml/pytorch/model_load/helpers.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/__init__.py
--rw-r--r--  2.0 unx    14753 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/registry.py
--rw-r--r--  2.0 unx      901 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/__init__.py
--rw-r--r--  2.0 unx    11658 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/darknet.py
--rw-r--r--  2.0 unx    40293 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/efficientnet.py
--rw-r--r--  2.0 unx    16512 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/inception_v3.py
--rw-r--r--  2.0 unx     4164 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/mnist.py
--rw-r--r--  2.0 unx     9546 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/mobilenet.py
--rw-r--r--  2.0 unx    13014 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/mobilenet_v2.py
--rw-r--r--  2.0 unx    40800 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/resnet.py
--rw-r--r--  2.0 unx    16649 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/classification/vgg.py
--rw-r--r--  2.0 unx      824 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/__init__.py
--rw-r--r--  2.0 unx     6820 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/ssd.py
--rw-r--r--  2.0 unx     8116 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/ssd_lite.py
--rw-r--r--  2.0 unx     4046 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/ssd_mobilenet.py
--rw-r--r--  2.0 unx     9069 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/ssd_resnet.py
--rw-r--r--  2.0 unx    10188 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/detection/yolo_v3.py
--rw-r--r--  2.0 unx      763 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/external/__init__.py
--rw-r--r--  2.0 unx     6759 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/external/torchvision.py
--rw-r--r--  2.0 unx      676 b- defN 24-Apr-01 00:45 sparseml/pytorch/models/recommendation/__init__.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-01 00:45 sparseml/pytorch/nn/__init__.py
--rw-r--r--  2.0 unx     8673 b- defN 24-Apr-01 00:45 sparseml/pytorch/nn/activations.py
--rw-r--r--  2.0 unx    11854 b- defN 24-Apr-01 00:45 sparseml/pytorch/nn/fatrelu.py
--rw-r--r--  2.0 unx     1690 b- defN 24-Apr-01 00:45 sparseml/pytorch/nn/identity.py
--rw-r--r--  2.0 unx     2828 b- defN 24-Apr-01 00:45 sparseml/pytorch/nn/se.py
--rw-r--r--  2.0 unx     1243 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/__init__.py
--rw-r--r--  2.0 unx    13638 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/analyzer_as.py
--rw-r--r--  2.0 unx    15582 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/analyzer_module.py
--rw-r--r--  2.0 unx     3955 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/analyzer_pruning.py
--rw-r--r--  2.0 unx    26838 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/manager.py
--rw-r--r--  2.0 unx    36844 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/mask_creator_pruning.py
--rw-r--r--  2.0 unx    23085 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/mask_pruning.py
--rw-r--r--  2.0 unx    10449 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/mask_pruning_scorer.py
--rw-r--r--  2.0 unx     6605 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/optimizer.py
--rw-r--r--  2.0 unx    14879 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/sensitivity_as.py
--rw-r--r--  2.0 unx     6101 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/sensitivity_lr.py
--rw-r--r--  2.0 unx     9324 b- defN 24-Apr-01 00:45 sparseml/pytorch/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-01 00:45 sparseml/pytorch/recipe_template/__init__.py
--rw-r--r--  2.0 unx     4534 b- defN 24-Apr-01 00:45 sparseml/pytorch/recipe_template/cli.py
--rw-r--r--  2.0 unx     1559 b- defN 24-Apr-01 00:45 sparseml/pytorch/recipe_template/description.py
--rw-r--r--  2.0 unx    15943 b- defN 24-Apr-01 00:45 sparseml/pytorch/recipe_template/main.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/__init__.py
--rw-r--r--  2.0 unx     1366 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/info.py
--rw-r--r--  2.0 unx    32159 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/modifier.py
--rw-r--r--  2.0 unx    18952 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/modifier_thinning.py
--rw-r--r--  2.0 unx      705 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/distillation/__init__.py
--rw-r--r--  2.0 unx     4741 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/distillation/modifier_distillation.py
--rw-r--r--  2.0 unx    14742 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/distillation/modifier_distillation_base.py
--rw-r--r--  2.0 unx    19177 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/distillation/modifier_per_layer.py
--rw-r--r--  2.0 unx     1276 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/__init__.py
--rw-r--r--  2.0 unx    29250 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/mask_creator.py
--rw-r--r--  2.0 unx    24209 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/mask_params.py
--rw-r--r--  2.0 unx    13389 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_as.py
--rw-r--r--  2.0 unx    12006 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_powerpropagation.py
--rw-r--r--  2.0 unx    10455 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_acdc.py
--rw-r--r--  2.0 unx    33219 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_base.py
--rw-r--r--  2.0 unx     5757 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_constant.py
--rw-r--r--  2.0 unx     8857 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_layer.py
--rw-r--r--  2.0 unx    15595 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_magnitude.py
--rw-r--r--  2.0 unx    63519 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_mfac.py
--rw-r--r--  2.0 unx     8774 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_movement.py
--rw-r--r--  2.0 unx    24121 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_obs.py
--rw-r--r--  2.0 unx    23735 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_rigl.py
--rw-r--r--  2.0 unx    18245 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_structured.py
--rw-r--r--  2.0 unx    17683 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/modifier_pruning_topkast.py
--rw-r--r--  2.0 unx     4644 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/pruning/scorer.py
--rw-r--r--  2.0 unx      813 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/__init__.py
--rw-r--r--  2.0 unx     2220 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/constants.py
--rw-r--r--  2.0 unx    34914 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/helpers.py
--rw-r--r--  2.0 unx    33626 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/legacy_modifier_quantization.py
--rw-r--r--  2.0 unx    26778 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/modifier_quantization.py
--rw-r--r--  2.0 unx    13482 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/quantization_scheme.py
--rw-r--r--  2.0 unx    18047 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/quantize.py
--rw-r--r--  2.0 unx    76796 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/quantization/quantize_qat_export.py
--rw-r--r--  2.0 unx      790 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/__init__.py
--rw-r--r--  2.0 unx     1778 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/modifier_epoch.py
--rw-r--r--  2.0 unx     2883 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/modifier_logging.py
--rw-r--r--  2.0 unx    24287 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/modifier_lr.py
--rw-r--r--  2.0 unx    21497 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/modifier_params.py
--rw-r--r--  2.0 unx     6690 b- defN 24-Apr-01 00:45 sparseml/pytorch/sparsification/training/modifier_regularizer.py
--rw-r--r--  2.0 unx      943 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/__init__.py
--rw-r--r--  2.0 unx     6490 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/export_onnx.py
--rw-r--r--  2.0 unx     2870 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/presets.py
--rw-r--r--  2.0 unx     2530 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/sampler.py
--rw-r--r--  2.0 unx    43917 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/train.py
--rw-r--r--  2.0 unx     7128 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/transforms.py
--rw-r--r--  2.0 unx    16675 b- defN 24-Apr-01 00:45 sparseml/pytorch/torchvision/utils.py
--rw-r--r--  2.0 unx     1160 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/__init__.py
--rw-r--r--  2.0 unx     9706 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/benchmarker.py
--rw-r--r--  2.0 unx     2846 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/callbacks.py
--rw-r--r--  2.0 unx     1061 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/distributed.py
--rw-r--r--  2.0 unx    30884 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/exporter.py
--rw-r--r--  2.0 unx    42811 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/helpers.py
--rw-r--r--  2.0 unx     1663 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/log_sparsification_info.py
--rw-r--r--  2.0 unx    31374 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/logger.py
--rw-r--r--  2.0 unx    27048 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/loss.py
--rw-r--r--  2.0 unx    11754 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/model.py
--rw-r--r--  2.0 unx    39117 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/module.py
--rw-r--r--  2.0 unx     9180 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/sparsification.py
--rw-r--r--  2.0 unx    30059 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/ssd_helpers.py
--rw-r--r--  2.0 unx    12337 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/yolo_helpers.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/sparsification_info/__init__.py
--rw-r--r--  2.0 unx    15081 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/sparsification_info/configs.py
--rw-r--r--  2.0 unx     4336 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/sparsification_info/helpers.py
--rw-r--r--  2.0 unx     2788 b- defN 24-Apr-01 00:45 sparseml/pytorch/utils/sparsification_info/module_sparsification_info.py
--rw-r--r--  2.0 unx      655 b- defN 24-Apr-01 00:45 sparseml/recipe_template/__init__.py
--rw-r--r--  2.0 unx     4788 b- defN 24-Apr-01 00:45 sparseml/recipe_template/utils.py
--rw-r--r--  2.0 unx     1058 b- defN 24-Apr-01 00:45 sparseml/sparsification/__init__.py
--rw-r--r--  2.0 unx     9387 b- defN 24-Apr-01 00:45 sparseml/sparsification/analyzer.py
--rw-r--r--  2.0 unx     9065 b- defN 24-Apr-01 00:45 sparseml/sparsification/info.py
--rw-r--r--  2.0 unx    15565 b- defN 24-Apr-01 00:45 sparseml/sparsification/model_info.py
--rw-r--r--  2.0 unx     2002 b- defN 24-Apr-01 00:45 sparseml/sparsification/modifier_epoch.py
--rw-r--r--  2.0 unx    10117 b- defN 24-Apr-01 00:45 sparseml/sparsification/modifier_lr.py
--rw-r--r--  2.0 unx     5505 b- defN 24-Apr-01 00:45 sparseml/sparsification/modifier_params.py
--rw-r--r--  2.0 unx    12845 b- defN 24-Apr-01 00:45 sparseml/sparsification/modifier_pruning.py
--rw-r--r--  2.0 unx     3700 b- defN 24-Apr-01 00:45 sparseml/sparsification/oracle.py
--rw-r--r--  2.0 unx    18570 b- defN 24-Apr-01 00:45 sparseml/sparsification/recipe_builder.py
--rw-r--r--  2.0 unx    14413 b- defN 24-Apr-01 00:45 sparseml/sparsification/recipe_editor.py
--rw-r--r--  2.0 unx     1250 b- defN 24-Apr-01 00:45 sparseml/sparsification/types.py
--rw-r--r--  2.0 unx     1169 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/__init__.py
--rw-r--r--  2.0 unx     7272 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/base.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/__init__.py
--rw-r--r--  2.0 unx     8121 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/dataset.py
--rw-r--r--  2.0 unx     5600 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/helpers.py
--rw-r--r--  2.0 unx     2768 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/registry.py
--rw-r--r--  2.0 unx      807 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/classification/__init__.py
--rw-r--r--  2.0 unx    12686 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/classification/cifar.py
--rw-r--r--  2.0 unx     8690 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/classification/imagefolder.py
--rw-r--r--  2.0 unx     2032 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/classification/imagenet.py
--rw-r--r--  2.0 unx     4695 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/datasets/classification/imagenette.py
--rw-r--r--  2.0 unx      805 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/framework/__init__.py
--rw-r--r--  2.0 unx     5859 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/framework/info.py
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/__init__.py
--rw-r--r--  2.0 unx    19752 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/estimator.py
--rw-r--r--  2.0 unx    14774 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/registry.py
--rw-r--r--  2.0 unx      822 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/__init__.py
--rw-r--r--  2.0 unx     3540 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/mnist.py
--rw-r--r--  2.0 unx    11161 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/mobilenet.py
--rw-r--r--  2.0 unx    18359 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/mobilenet_v2.py
--rw-r--r--  2.0 unx    28103 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/resnet.py
--rw-r--r--  2.0 unx    26886 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/models/classification/vgg.py
--rw-r--r--  2.0 unx      865 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/nn/__init__.py
--rw-r--r--  2.0 unx    18670 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/nn/layers.py
--rw-r--r--  2.0 unx     1238 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/__init__.py
--rw-r--r--  2.0 unx     8607 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/analyzer_module.py
--rw-r--r--  2.0 unx     9591 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/manager.py
--rw-r--r--  2.0 unx    19683 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/mask_creator_pruning.py
--rw-r--r--  2.0 unx    33919 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/mask_pruning.py
--rw-r--r--  2.0 unx    15955 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/modifier.py
--rw-r--r--  2.0 unx     1715 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/modifier_epoch.py
--rw-r--r--  2.0 unx    10685 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/modifier_lr.py
--rw-r--r--  2.0 unx     7092 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/modifier_params.py
--rw-r--r--  2.0 unx    15702 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/modifier_pruning.py
--rw-r--r--  2.0 unx     5682 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/schedule_lr.py
--rw-r--r--  2.0 unx     9232 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/optim/sensitivity_pruning.py
--rw-r--r--  2.0 unx      801 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/sparsification/__init__.py
--rw-r--r--  2.0 unx     1385 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/sparsification/info.py
--rw-r--r--  2.0 unx      967 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/__init__.py
--rw-r--r--  2.0 unx    10913 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/exporter.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/helpers.py
--rw-r--r--  2.0 unx     1974 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/loss.py
--rw-r--r--  2.0 unx     8119 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/nets_utils.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/summary.py
--rw-r--r--  2.0 unx    12536 b- defN 24-Apr-01 00:45 sparseml/tensorflow_v1/utils/variable.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/tools/__init__.py
--rw-r--r--  2.0 unx     1193 b- defN 24-Apr-01 00:45 sparseml/transformers/__init__.py
--rw-r--r--  2.0 unx      985 b- defN 24-Apr-01 00:45 sparseml/transformers/base.py
--rw-r--r--  2.0 unx    23904 b- defN 24-Apr-01 00:45 sparseml/transformers/export.py
--rw-r--r--  2.0 unx     9485 b- defN 24-Apr-01 00:45 sparseml/transformers/integration_helper_functions.py
--rw-r--r--  2.0 unx    30795 b- defN 24-Apr-01 00:45 sparseml/transformers/masked_language_modeling.py
--rw-r--r--  2.0 unx    37002 b- defN 24-Apr-01 00:45 sparseml/transformers/question_answering.py
--rw-r--r--  2.0 unx    40360 b- defN 24-Apr-01 00:45 sparseml/transformers/text_classification.py
--rw-r--r--  2.0 unx      818 b- defN 24-Apr-01 00:45 sparseml/transformers/text_generation.py
--rw-r--r--  2.0 unx    34389 b- defN 24-Apr-01 00:45 sparseml/transformers/token_classification.py
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/__init__.py
--rw-r--r--  2.0 unx      749 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/compressors/__init__.py
--rw-r--r--  2.0 unx     3136 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/compressors/base.py
--rw-r--r--  2.0 unx     1160 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/compressors/dense.py
--rw-r--r--  2.0 unx     8377 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/compressors/sparse_bitmask.py
--rw-r--r--  2.0 unx      751 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/config/__init__.py
--rw-r--r--  2.0 unx     3846 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/config/base.py
--rw-r--r--  2.0 unx     1263 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/config/dense.py
--rw-r--r--  2.0 unx     1236 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/config/sparse_bitmask.py
--rw-r--r--  2.0 unx      718 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/utils/__init__.py
--rw-r--r--  2.0 unx     6092 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/utils/compress_save.py
--rw-r--r--  2.0 unx     1778 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/utils/helpers.py
--rw-r--r--  2.0 unx     5276 b- defN 24-Apr-01 00:45 sparseml/transformers/compression/utils/safetensors_load.py
--rw-r--r--  2.0 unx      701 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/__init__.py
--rw-r--r--  2.0 unx     5251 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/callbacks.py
--rw-r--r--  2.0 unx     2519 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/model_args.py
--rw-r--r--  2.0 unx    11988 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/runner.py
--rw-r--r--  2.0 unx    22944 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/session_mixin.py
--rw-r--r--  2.0 unx    13101 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/text_generation.py
--rw-r--r--  2.0 unx     4290 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/trainer.py
--rw-r--r--  2.0 unx     3005 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/training_args.py
--rw-r--r--  2.0 unx     1021 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/__init__.py
--rw-r--r--  2.0 unx     8736 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/base.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/c4.py
--rw-r--r--  2.0 unx     2537 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/cnn_dailymail.py
--rw-r--r--  2.0 unx     4281 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/custom.py
--rw-r--r--  2.0 unx     5399 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/data_args.py
--rw-r--r--  2.0 unx     8961 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/data_helpers.py
--rw-r--r--  2.0 unx     2892 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/evolcodealpaca.py
--rw-r--r--  2.0 unx     2597 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/gsm8k.py
--rw-r--r--  2.0 unx     3435 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/open_platypus.py
--rw-r--r--  2.0 unx     1369 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/ptb.py
--rw-r--r--  2.0 unx     3521 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/ultrachat_200k.py
--rw-r--r--  2.0 unx     1237 b- defN 24-Apr-01 00:45 sparseml/transformers/finetune/data/wikitext.py
--rw-r--r--  2.0 unx      922 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/__init__.py
--rw-r--r--  2.0 unx    19529 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/question_answering.py
--rw-r--r--  2.0 unx     1874 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/sparse_config.py
--rw-r--r--  2.0 unx    20054 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/sparse_model.py
--rw-r--r--  2.0 unx     2327 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/sparse_tokenizer.py
--rw-r--r--  2.0 unx    41727 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/trainer.py
--rw-r--r--  2.0 unx     1890 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/training_args.py
--rw-r--r--  2.0 unx      866 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/__init__.py
--rw-r--r--  2.0 unx     2429 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/base.py
--rw-r--r--  2.0 unx     3922 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modification_objects.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modify_model.py
--rw-r--r--  2.0 unx     9199 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_bert.py
--rw-r--r--  2.0 unx     5492 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_distilbert.py
--rw-r--r--  2.0 unx     8703 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_llama.py
--rw-r--r--  2.0 unx     6945 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_mistral.py
--rw-r--r--  2.0 unx     2549 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_mobilebert.py
--rw-r--r--  2.0 unx     9618 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/modifying_opt.py
--rw-r--r--  2.0 unx     1486 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/modification/registry.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/obcq/__init__.py
--rw-r--r--  2.0 unx    19683 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/obcq/export.py
--rw-r--r--  2.0 unx     7695 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/obcq/obcq.py
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/obcq/utils/__init__.py
--rw-r--r--  2.0 unx     3671 b- defN 24-Apr-01 00:45 sparseml/transformers/sparsification/obcq/utils/helpers.py
--rw-r--r--  2.0 unx      805 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/__init__.py
--rw-r--r--  2.0 unx    20055 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/helpers.py
--rw-r--r--  2.0 unx     6711 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/initializers.py
--rw-r--r--  2.0 unx     5081 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/load_task_dataset.py
--rw-r--r--  2.0 unx     2764 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/load_task_model.py
--rw-r--r--  2.0 unx     2536 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/metrics.py
--rw-r--r--  2.0 unx     1972 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/optimizations.py
--rw-r--r--  2.0 unx     1037 b- defN 24-Apr-01 00:45 sparseml/transformers/utils/preprocessing_functions.py
--rw-r--r--  2.0 unx      844 b- defN 24-Apr-01 00:45 sparseml/utils/__init__.py
--rw-r--r--  2.0 unx      886 b- defN 24-Apr-01 00:45 sparseml/utils/frameworks.py
--rw-r--r--  2.0 unx    31604 b- defN 24-Apr-01 00:45 sparseml/utils/helpers.py
--rw-r--r--  2.0 unx     3983 b- defN 24-Apr-01 00:45 sparseml/utils/restricted_eval.py
--rw-r--r--  2.0 unx     1083 b- defN 24-Apr-01 00:45 sparseml/utils/singleton.py
--rw-r--r--  2.0 unx     6312 b- defN 24-Apr-01 00:45 sparseml/utils/worker.py
--rw-r--r--  2.0 unx     2952 b- defN 24-Apr-01 00:45 sparseml/utils/wrapper.py
--rw-r--r--  2.0 unx      819 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/__init__.py
--rw-r--r--  2.0 unx      833 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/cifar.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/coco.py
--rw-r--r--  2.0 unx     1217 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/helpers.py
--rw-r--r--  2.0 unx    23366 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/imagenet.py
--rw-r--r--  2.0 unx     8967 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/imagenette.py
--rw-r--r--  2.0 unx     1009 b- defN 24-Apr-01 00:45 sparseml/utils/datasets/voc.py
--rw-r--r--  2.0 unx      633 b- defN 24-Apr-01 00:45 sparseml/utils/fsdp/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 24-Apr-01 00:45 sparseml/utils/fsdp/context.py
--rw-r--r--  2.0 unx     6631 b- defN 24-Apr-01 00:45 sparseml/utils/fsdp/helpers.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-01 00:45 sparseml/utils/pytorch/__init__.py
--rw-r--r--  2.0 unx    10897 b- defN 24-Apr-01 00:45 sparseml/utils/pytorch/module.py
--rw-r--r--  2.0 unx     1696 b- defN 24-Apr-01 00:45 sparseml/utils/pytorch/utils.py
--rw-r--r--  2.0 unx      680 b- defN 24-Apr-01 00:45 sparseml/utils/pytorch/pruning/__init__.py
--rw-r--r--  2.0 unx     1875 b- defN 24-Apr-01 00:45 sparseml/yolact/COCO.sh
--rw-r--r--  2.0 unx     1418 b- defN 24-Apr-01 00:45 sparseml/yolact/COCO_test.sh
--rw-r--r--  2.0 unx     4020 b- defN 24-Apr-01 00:45 sparseml/yolact/__init__.py
--rw-r--r--  2.0 unx     1784 b- defN 24-Apr-01 00:45 sparseml/yolact/scripts.py
--rw-r--r--  2.0 unx     1440 b- defN 24-Apr-01 00:45 sparseml/yolov5/__init__.py
--rw-r--r--  2.0 unx     4505 b- defN 24-Apr-01 00:45 sparseml/yolov5/helpers.py
--rw-r--r--  2.0 unx     1609 b- defN 24-Apr-01 00:45 sparseml/yolov5/scripts.py
--rw-r--r--  2.0 unx     1220 b- defN 24-Apr-01 00:45 sparseml/yolov5/yolov5.status.yaml
--rw-r--r--  2.0 unx     1117 b- defN 24-Apr-01 00:45 sparseml/yolov8/__init__.py
--rw-r--r--  2.0 unx     6061 b- defN 24-Apr-01 00:45 sparseml/yolov8/default.yaml
--rw-r--r--  2.0 unx     2815 b- defN 24-Apr-01 00:45 sparseml/yolov8/export.py
--rw-r--r--  2.0 unx     2259 b- defN 24-Apr-01 00:45 sparseml/yolov8/modules.py
--rw-r--r--  2.0 unx     7394 b- defN 24-Apr-01 00:45 sparseml/yolov8/train.py
--rw-r--r--  2.0 unx    37860 b- defN 24-Apr-01 00:45 sparseml/yolov8/trainers.py
--rw-r--r--  2.0 unx     2748 b- defN 24-Apr-01 00:45 sparseml/yolov8/val.py
--rw-r--r--  2.0 unx     8459 b- defN 24-Apr-01 00:45 sparseml/yolov8/validators.py
--rw-r--r--  2.0 unx      685 b- defN 24-Apr-01 00:45 sparseml/yolov8/utils/__init__.py
--rw-r--r--  2.0 unx     6683 b- defN 24-Apr-01 00:45 sparseml/yolov8/utils/export_samples.py
--rw-r--r--  2.0 unx     4041 b- defN 24-Apr-01 00:45 sparseml/yolov8/utils/helpers.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/LICENSE
--rw-r--r--  2.0 unx    13747 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/LICENSE-ULTRALYTICS
--rw-r--r--  2.0 unx    23637 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/METADATA
--rw-r--r--  2.0 unx     2085 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/NOTICE
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/WHEEL
--rw-r--r--  2.0 unx     3122 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    56768 b- defN 24-Apr-01 00:46 sparseml_nightly-1.8.0.20240401.dist-info/RECORD
-567 files, 4551480 bytes uncompressed, 1227073 bytes compressed:  73.0%
+Zip file size: 1319581 bytes, number of entries: 567
+-rw-r--r--  2.0 unx     1489 b- defN 24-Apr-04 11:59 sparseml/__init__.py
+-rw-r--r--  2.0 unx      898 b- defN 24-Apr-04 11:59 sparseml/analytics.py
+-rw-r--r--  2.0 unx    10284 b- defN 24-Apr-04 11:59 sparseml/base.py
+-rw-r--r--  2.0 unx     7288 b- defN 24-Apr-04 11:59 sparseml/integration_helper_functions.py
+-rw-r--r--  2.0 unx     2483 b- defN 24-Apr-04 11:59 sparseml/log.py
+-rw-r--r--  2.0 unx     1607 b- defN 24-Apr-04 11:59 sparseml/version.py
+-rw-r--r--  2.0 unx      758 b- defN 24-Apr-04 11:59 sparseml/benchmark/__init__.py
+-rw-r--r--  2.0 unx    17763 b- defN 24-Apr-04 11:59 sparseml/benchmark/info.py
+-rw-r--r--  2.0 unx    10778 b- defN 24-Apr-04 11:59 sparseml/benchmark/serialization.py
+-rw-r--r--  2.0 unx      915 b- defN 24-Apr-04 11:59 sparseml/core/__init__.py
+-rw-r--r--  2.0 unx     6842 b- defN 24-Apr-04 11:59 sparseml/core/event.py
+-rw-r--r--  2.0 unx     6492 b- defN 24-Apr-04 11:59 sparseml/core/factory.py
+-rw-r--r--  2.0 unx     2850 b- defN 24-Apr-04 11:59 sparseml/core/framework.py
+-rw-r--r--  2.0 unx     2855 b- defN 24-Apr-04 11:59 sparseml/core/framework_object.py
+-rw-r--r--  2.0 unx     3780 b- defN 24-Apr-04 11:59 sparseml/core/helpers.py
+-rw-r--r--  2.0 unx    23720 b- defN 24-Apr-04 11:59 sparseml/core/session.py
+-rw-r--r--  2.0 unx     9034 b- defN 24-Apr-04 11:59 sparseml/core/state.py
+-rw-r--r--  2.0 unx      667 b- defN 24-Apr-04 11:59 sparseml/core/data/__init__.py
+-rw-r--r--  2.0 unx     1565 b- defN 24-Apr-04 11:59 sparseml/core/data/base.py
+-rw-r--r--  2.0 unx     6415 b- defN 24-Apr-04 11:59 sparseml/core/data/pytorch.py
+-rw-r--r--  2.0 unx      678 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/__init__.py
+-rw-r--r--  2.0 unx    11954 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/event.py
+-rw-r--r--  2.0 unx     8242 b- defN 24-Apr-04 11:59 sparseml/core/lifecycle/session.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/core/logger/__init__.py
+-rw-r--r--  2.0 unx    44703 b- defN 24-Apr-04 11:59 sparseml/core/logger/logger.py
+-rw-r--r--  2.0 unx      667 b- defN 24-Apr-04 11:59 sparseml/core/logger/utils/__init__.py
+-rw-r--r--  2.0 unx    12725 b- defN 24-Apr-04 11:59 sparseml/core/logger/utils/frequency_manager.py
+-rw-r--r--  2.0 unx      693 b- defN 24-Apr-04 11:59 sparseml/core/model/__init__.py
+-rw-r--r--  2.0 unx     5895 b- defN 24-Apr-04 11:59 sparseml/core/model/base.py
+-rw-r--r--  2.0 unx     5641 b- defN 24-Apr-04 11:59 sparseml/core/model/pytorch.py
+-rw-r--r--  2.0 unx      699 b- defN 24-Apr-04 11:59 sparseml/core/modifier/__init__.py
+-rw-r--r--  2.0 unx     3326 b- defN 24-Apr-04 11:59 sparseml/core/modifier/base.py
+-rw-r--r--  2.0 unx    10810 b- defN 24-Apr-04 11:59 sparseml/core/modifier/modifier.py
+-rw-r--r--  2.0 unx     6093 b- defN 24-Apr-04 11:59 sparseml/core/modifier/stage.py
+-rw-r--r--  2.0 unx      672 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/__init__.py
+-rw-r--r--  2.0 unx     3409 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/base.py
+-rw-r--r--  2.0 unx     4584 b- defN 24-Apr-04 11:59 sparseml/core/optimizer/pytorch.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/core/recipe/__init__.py
+-rw-r--r--  2.0 unx     6872 b- defN 24-Apr-04 11:59 sparseml/core/recipe/args.py
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-04 11:59 sparseml/core/recipe/base.py
+-rw-r--r--  2.0 unx     6313 b- defN 24-Apr-04 11:59 sparseml/core/recipe/container.py
+-rw-r--r--  2.0 unx     2932 b- defN 24-Apr-04 11:59 sparseml/core/recipe/metadata.py
+-rw-r--r--  2.0 unx     4224 b- defN 24-Apr-04 11:59 sparseml/core/recipe/modifier.py
+-rw-r--r--  2.0 unx    24978 b- defN 24-Apr-04 11:59 sparseml/core/recipe/recipe.py
+-rw-r--r--  2.0 unx     7629 b- defN 24-Apr-04 11:59 sparseml/core/recipe/stage.py
+-rw-r--r--  2.0 unx      663 b- defN 24-Apr-04 11:59 sparseml/core/utils/__init__.py
+-rw-r--r--  2.0 unx     1025 b- defN 24-Apr-04 11:59 sparseml/core/utils/session_helpers.py
+-rw-r--r--  2.0 unx      863 b- defN 24-Apr-04 11:59 sparseml/deepsparse/__init__.py
+-rw-r--r--  2.0 unx     3516 b- defN 24-Apr-04 11:59 sparseml/deepsparse/base.py
+-rw-r--r--  2.0 unx      801 b- defN 24-Apr-04 11:59 sparseml/deepsparse/framework/__init__.py
+-rw-r--r--  2.0 unx     6032 b- defN 24-Apr-04 11:59 sparseml/deepsparse/framework/info.py
+-rw-r--r--  2.0 unx      813 b- defN 24-Apr-04 11:59 sparseml/deepsparse/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1348 b- defN 24-Apr-04 11:59 sparseml/deepsparse/sparsification/info.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/evaluation/__init__.py
+-rw-r--r--  2.0 unx     5689 b- defN 24-Apr-04 11:59 sparseml/evaluation/cli.py
+-rw-r--r--  2.0 unx     2140 b- defN 24-Apr-04 11:59 sparseml/evaluation/evaluator.py
+-rw-r--r--  2.0 unx      940 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations_config.yaml
+-rw-r--r--  2.0 unx     5571 b- defN 24-Apr-04 11:59 sparseml/evaluation/registry.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/lm_evaluation_harness.py
+-rw-r--r--  2.0 unx    10987 b- defN 24-Apr-04 11:59 sparseml/evaluation/integrations/perplexity.py
+-rw-r--r--  2.0 unx      661 b- defN 24-Apr-04 11:59 sparseml/export/__init__.py
+-rw-r--r--  2.0 unx    21172 b- defN 24-Apr-04 11:59 sparseml/export/export.py
+-rw-r--r--  2.0 unx     8049 b- defN 24-Apr-04 11:59 sparseml/export/export_data.py
+-rw-r--r--  2.0 unx     2681 b- defN 24-Apr-04 11:59 sparseml/export/export_torch_model.py
+-rw-r--r--  2.0 unx    13052 b- defN 24-Apr-04 11:59 sparseml/export/helpers.py
+-rw-r--r--  2.0 unx     8868 b- defN 24-Apr-04 11:59 sparseml/export/validators.py
+-rw-r--r--  2.0 unx      786 b- defN 24-Apr-04 11:59 sparseml/exporters/__init__.py
+-rw-r--r--  2.0 unx     1477 b- defN 24-Apr-04 11:59 sparseml/exporters/base_exporter.py
+-rw-r--r--  2.0 unx     6576 b- defN 24-Apr-04 11:59 sparseml/exporters/kv_cache_injector.py
+-rw-r--r--  2.0 unx     5522 b- defN 24-Apr-04 11:59 sparseml/exporters/onnx_to_deepsparse.py
+-rw-r--r--  2.0 unx     2350 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/__init__.py
+-rw-r--r--  2.0 unx     2333 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/base_transform.py
+-rw-r--r--  2.0 unx     1388 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/constants_to_initializers.py
+-rw-r--r--  2.0 unx     4630 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/conv_to_convinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     5973 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/conv_to_qlinearconv.py
+-rw-r--r--  2.0 unx     2440 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/delete_repeated_qdq.py
+-rw-r--r--  2.0 unx     1842 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/delete_trivial_onnx_adds.py
+-rw-r--r--  2.0 unx     2241 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/flatten_qparams.py
+-rw-r--r--  2.0 unx     3553 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_conv_div_bn.py
+-rw-r--r--  2.0 unx     1669 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_identity_initializers.py
+-rw-r--r--  2.0 unx     2070 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/fold_relu_quants.py
+-rw-r--r--  2.0 unx     4418 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/gemm_to_matmulinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     7629 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/gemm_to_qlinearmatmul.py
+-rw-r--r--  2.0 unx     1645 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/initializers_to_uint8.py
+-rw-r--r--  2.0 unx     6297 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_add_to_matmulinteger_add_cast_mul.py
+-rw-r--r--  2.0 unx     4681 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_to_matmulinteger_cast_mul.py
+-rw-r--r--  2.0 unx     4156 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/matmul_to_qlinearmatmul.py
+-rw-r--r--  2.0 unx     3770 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/onnx_transform.py
+-rw-r--r--  2.0 unx     3433 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/propagate_dequant_through_split.py
+-rw-r--r--  2.0 unx     4801 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/propagate_embedding_quantization.py
+-rw-r--r--  2.0 unx     4464 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/quantize_qat_embedding.py
+-rw-r--r--  2.0 unx     3869 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/quantize_residuals.py
+-rw-r--r--  2.0 unx     3331 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/remove_duplicate_qconv_weights.py
+-rw-r--r--  2.0 unx     2545 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/remove_duplicate_quantize_ops.py
+-rw-r--r--  2.0 unx     3210 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/skip_input_quantize.py
+-rw-r--r--  2.0 unx     1373 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/unwrap_batchnorms.py
+-rw-r--r--  2.0 unx      911 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/__init__.py
+-rw-r--r--  2.0 unx    30558 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/cache_keys_and_values.py
+-rw-r--r--  2.0 unx    10686 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/configs.py
+-rw-r--r--  2.0 unx     7027 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_base.py
+-rw-r--r--  2.0 unx     4974 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_codegen.py
+-rw-r--r--  2.0 unx     8801 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_llama.py
+-rw-r--r--  2.0 unx     4261 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_mpt.py
+-rw-r--r--  2.0 unx     6066 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/kv_cache/transforms_opt.py
+-rw-r--r--  2.0 unx      730 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/__init__.py
+-rw-r--r--  2.0 unx    11112 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/add_quantized_conv_matmul_add_ops.py
+-rw-r--r--  2.0 unx     6809 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/helpers.py
+-rw-r--r--  2.0 unx    14429 b- defN 24-Apr-04 11:59 sparseml/exporters/transforms/utils/matching.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/framework/__init__.py
+-rw-r--r--  2.0 unx     9479 b- defN 24-Apr-04 11:59 sparseml/framework/info.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/evaluator.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/data/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/metrics/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/model/__init__.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/integrations/torchvision/optim/__init__.py
+-rw-r--r--  2.0 unx     1144 b- defN 24-Apr-04 11:59 sparseml/keras/__init__.py
+-rw-r--r--  2.0 unx     8054 b- defN 24-Apr-04 11:59 sparseml/keras/base.py
+-rw-r--r--  2.0 unx      943 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/__init__.py
+-rw-r--r--  2.0 unx     3297 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/dataset.py
+-rw-r--r--  2.0 unx     2423 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/helpers.py
+-rw-r--r--  2.0 unx     2761 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/registry.py
+-rw-r--r--  2.0 unx      786 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx     8369 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     4301 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     2727 b- defN 24-Apr-04 11:59 sparseml/keras/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx      793 b- defN 24-Apr-04 11:59 sparseml/keras/framework/__init__.py
+-rw-r--r--  2.0 unx     5906 b- defN 24-Apr-04 11:59 sparseml/keras/framework/info.py
+-rw-r--r--  2.0 unx      921 b- defN 24-Apr-04 11:59 sparseml/keras/models/__init__.py
+-rw-r--r--  2.0 unx    11814 b- defN 24-Apr-04 11:59 sparseml/keras/models/registry.py
+-rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/keras/models/classification/__init__.py
+-rw-r--r--  2.0 unx    17932 b- defN 24-Apr-04 11:59 sparseml/keras/models/classification/resnet.py
+-rw-r--r--  2.0 unx      768 b- defN 24-Apr-04 11:59 sparseml/keras/models/external/__init__.py
+-rw-r--r--  2.0 unx     4402 b- defN 24-Apr-04 11:59 sparseml/keras/models/external/keras_applications.py
+-rw-r--r--  2.0 unx     1166 b- defN 24-Apr-04 11:59 sparseml/keras/optim/__init__.py
+-rw-r--r--  2.0 unx     5677 b- defN 24-Apr-04 11:59 sparseml/keras/optim/manager.py
+-rw-r--r--  2.0 unx    14777 b- defN 24-Apr-04 11:59 sparseml/keras/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    19740 b- defN 24-Apr-04 11:59 sparseml/keras/optim/mask_pruning_creator.py
+-rw-r--r--  2.0 unx     9183 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier.py
+-rw-r--r--  2.0 unx     1676 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_epoch.py
+-rw-r--r--  2.0 unx    14736 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_lr.py
+-rw-r--r--  2.0 unx     5477 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_params.py
+-rw-r--r--  2.0 unx    24031 b- defN 24-Apr-04 11:59 sparseml/keras/optim/modifier_pruning.py
+-rw-r--r--  2.0 unx     1133 b- defN 24-Apr-04 11:59 sparseml/keras/optim/utils.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Apr-04 11:59 sparseml/keras/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1356 b- defN 24-Apr-04 11:59 sparseml/keras/sparsification/info.py
+-rw-r--r--  2.0 unx      962 b- defN 24-Apr-04 11:59 sparseml/keras/utils/__init__.py
+-rw-r--r--  2.0 unx     8202 b- defN 24-Apr-04 11:59 sparseml/keras/utils/callbacks.py
+-rw-r--r--  2.0 unx     1022 b- defN 24-Apr-04 11:59 sparseml/keras/utils/compat.py
+-rw-r--r--  2.0 unx     5737 b- defN 24-Apr-04 11:59 sparseml/keras/utils/exporter.py
+-rw-r--r--  2.0 unx     6087 b- defN 24-Apr-04 11:59 sparseml/keras/utils/logger.py
+-rw-r--r--  2.0 unx     1738 b- defN 24-Apr-04 11:59 sparseml/keras/utils/model.py
+-rw-r--r--  2.0 unx      800 b- defN 24-Apr-04 11:59 sparseml/modifiers/__init__.py
+-rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/__init__.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/__init__.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/base.py
+-rw-r--r--  2.0 unx     7612 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/output/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/__init__.py
+-rw-r--r--  2.0 unx      715 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx    13320 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/kd_factory.py
+-rw-r--r--  2.0 unx     3921 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/kd_wrapper.py
+-rw-r--r--  2.0 unx     4527 b- defN 24-Apr-04 11:59 sparseml/modifiers/distillation/utils/pytorch/model_wrapper.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/experimental/__init__.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/__init__.py
+-rw-r--r--  2.0 unx     2764 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/base.py
+-rw-r--r--  2.0 unx     1947 b- defN 24-Apr-04 11:59 sparseml/modifiers/logarithmic_equalization/pytorch.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/__init__.py
+-rw-r--r--  2.0 unx     5181 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/base.py
+-rw-r--r--  2.0 unx     3543 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/__init__.py
+-rw-r--r--  2.0 unx     6686 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/helpers.py
+-rw-r--r--  2.0 unx     6753 b- defN 24-Apr-04 11:59 sparseml/modifiers/obcq/utils/sgpt_wrapper.py
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/__init__.py
+-rw-r--r--  2.0 unx     5725 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/helpers.py
+-rw-r--r--  2.0 unx      676 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/__init__.py
+-rw-r--r--  2.0 unx      951 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/base.py
+-rw-r--r--  2.0 unx     3103 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/constant/pytorch.py
+-rw-r--r--  2.0 unx      677 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/__init__.py
+-rw-r--r--  2.0 unx     1168 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/base.py
+-rw-r--r--  2.0 unx     5262 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/magnitude/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/__init__.py
+-rw-r--r--  2.0 unx      688 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx     5984 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/layer_mask.py
+-rw-r--r--  2.0 unx     5622 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/utils/pytorch/mask_factory.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/__init__.py
+-rw-r--r--  2.0 unx     3905 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/base.py
+-rw-r--r--  2.0 unx    10346 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/pytorch.py
+-rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/utils/__init__.py
+-rw-r--r--  2.0 unx     4469 b- defN 24-Apr-04 11:59 sparseml/modifiers/pruning/wanda/utils/wanda_wrapper.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/__init__.py
+-rw-r--r--  2.0 unx     5512 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/base.py
+-rw-r--r--  2.0 unx     8924 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/__init__.py
+-rw-r--r--  2.0 unx     2220 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/constants.py
+-rw-r--r--  2.0 unx     2906 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py
+-rw-r--r--  2.0 unx    32720 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/helpers.py
+-rw-r--r--  2.0 unx    13545 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/quantization_scheme.py
+-rw-r--r--  2.0 unx    20463 b- defN 24-Apr-04 11:59 sparseml/modifiers/quantization/utils/quantize.py
+-rw-r--r--  2.0 unx      654 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/__init__.py
+-rw-r--r--  2.0 unx     7535 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/base.py
+-rw-r--r--  2.0 unx     8167 b- defN 24-Apr-04 11:59 sparseml/modifiers/smoothquant/pytorch.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/__init__.py
+-rw-r--r--  2.0 unx     3944 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/compression_wrapper.py
+-rw-r--r--  2.0 unx     5257 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/layer_compressor.py
+-rw-r--r--  2.0 unx     3265 b- defN 24-Apr-04 11:59 sparseml/modifiers/utils/pytorch_helpers.py
+-rw-r--r--  2.0 unx     1036 b- defN 24-Apr-04 11:59 sparseml/onnx/__init__.py
+-rw-r--r--  2.0 unx     6202 b- defN 24-Apr-04 11:59 sparseml/onnx/base.py
+-rw-r--r--  2.0 unx      743 b- defN 24-Apr-04 11:59 sparseml/onnx/benchmark/__init__.py
+-rw-r--r--  2.0 unx    15366 b- defN 24-Apr-04 11:59 sparseml/onnx/benchmark/info.py
+-rw-r--r--  2.0 unx      823 b- defN 24-Apr-04 11:59 sparseml/onnx/framework/__init__.py
+-rw-r--r--  2.0 unx     6116 b- defN 24-Apr-04 11:59 sparseml/onnx/framework/info.py
+-rw-r--r--  2.0 unx      820 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/__init__.py
+-rw-r--r--  2.0 unx    13285 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/analyzer_model.py
+-rw-r--r--  2.0 unx    19639 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx     6470 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/structured_pruning.py
+-rw-r--r--  2.0 unx      815 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/__init__.py
+-rw-r--r--  2.0 unx    14753 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/calibration.py
+-rw-r--r--  2.0 unx    73551 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/quantize.py
+-rw-r--r--  2.0 unx     4552 b- defN 24-Apr-04 11:59 sparseml/onnx/optim/quantization/quantize_model_post_training.py
+-rw-r--r--  2.0 unx      869 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/__init__.py
+-rw-r--r--  2.0 unx    10209 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/analyzer.py
+-rw-r--r--  2.0 unx     1363 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/info.py
+-rw-r--r--  2.0 unx     8009 b- defN 24-Apr-04 11:59 sparseml/onnx/sparsification/model_info.py
+-rw-r--r--  2.0 unx      867 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/__init__.py
+-rw-r--r--  2.0 unx    13013 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/data.py
+-rw-r--r--  2.0 unx    20691 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/graph_editor.py
+-rw-r--r--  2.0 unx     8133 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/graph_optimizer.py
+-rw-r--r--  2.0 unx    40230 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/helpers.py
+-rw-r--r--  2.0 unx     1958 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/loss.py
+-rw-r--r--  2.0 unx    31591 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/model.py
+-rw-r--r--  2.0 unx     5437 b- defN 24-Apr-04 11:59 sparseml/onnx/utils/sparse_tensor.py
+-rw-r--r--  2.0 unx      931 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/__init__.py
+-rw-r--r--  2.0 unx     3713 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/export.py
+-rw-r--r--  2.0 unx    10950 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/train.py
+-rw-r--r--  2.0 unx     4211 b- defN 24-Apr-04 11:59 sparseml/openpifpaf/trainer.py
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-04 11:59 sparseml/optim/__init__.py
+-rw-r--r--  2.0 unx     6302 b- defN 24-Apr-04 11:59 sparseml/optim/analyzer.py
+-rw-r--r--  2.0 unx    25563 b- defN 24-Apr-04 11:59 sparseml/optim/helpers.py
+-rw-r--r--  2.0 unx    25984 b- defN 24-Apr-04 11:59 sparseml/optim/manager.py
+-rw-r--r--  2.0 unx    30708 b- defN 24-Apr-04 11:59 sparseml/optim/modifier.py
+-rw-r--r--  2.0 unx    26315 b- defN 24-Apr-04 11:59 sparseml/optim/sensitivity.py
+-rw-r--r--  2.0 unx     2190 b- defN 24-Apr-04 11:59 sparseml/pytorch/__init__.py
+-rw-r--r--  2.0 unx     6154 b- defN 24-Apr-04 11:59 sparseml/pytorch/base.py
+-rw-r--r--  2.0 unx      933 b- defN 24-Apr-04 11:59 sparseml/pytorch/opset.py
+-rw-r--r--  2.0 unx    12086 b- defN 24-Apr-04 11:59 sparseml/pytorch/torch_to_onnx_exporter.py
+-rw-r--r--  2.0 unx      998 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/__init__.py
+-rw-r--r--  2.0 unx     4193 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/generic.py
+-rw-r--r--  2.0 unx     3014 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/registry.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx     4457 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/cifar.py
+-rw-r--r--  2.0 unx     3669 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     4000 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     6491 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/classification/mnist.py
+-rw-r--r--  2.0 unx      767 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/__init__.py
+-rw-r--r--  2.0 unx    16159 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/coco.py
+-rw-r--r--  2.0 unx     5705 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/helpers.py
+-rw-r--r--  2.0 unx    10759 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/detection/voc.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/image_classification/__init__.py
+-rw-r--r--  2.0 unx     9512 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/image_classification/ffcv_dataset.py
+-rw-r--r--  2.0 unx      684 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/recommendation/__init__.py
+-rw-r--r--  2.0 unx      693 b- defN 24-Apr-04 11:59 sparseml/pytorch/datasets/video/__init__.py
+-rw-r--r--  2.0 unx      814 b- defN 24-Apr-04 11:59 sparseml/pytorch/framework/__init__.py
+-rw-r--r--  2.0 unx     5580 b- defN 24-Apr-04 11:59 sparseml/pytorch/framework/info.py
+-rw-r--r--  2.0 unx      753 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/__init__.py
+-rw-r--r--  2.0 unx    18265 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/export.py
+-rw-r--r--  2.0 unx     4991 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/integration_helper_functions.py
+-rw-r--r--  2.0 unx    15494 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/lr_analysis.py
+-rw-r--r--  2.0 unx    14444 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/pr_sensitivity.py
+-rw-r--r--  2.0 unx    29287 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/train.py
+-rw-r--r--  2.0 unx      682 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/__init__.py
+-rw-r--r--  2.0 unx     4278 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/cli_helpers.py
+-rw-r--r--  2.0 unx     1257 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/constants.py
+-rw-r--r--  2.0 unx    23894 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/helpers.py
+-rw-r--r--  2.0 unx    12056 b- defN 24-Apr-04 11:59 sparseml/pytorch/image_classification/utils/trainer.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/model_load/__init__.py
+-rw-r--r--  2.0 unx    11440 b- defN 24-Apr-04 11:59 sparseml/pytorch/model_load/helpers.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/__init__.py
+-rw-r--r--  2.0 unx    14753 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/registry.py
+-rw-r--r--  2.0 unx      901 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/__init__.py
+-rw-r--r--  2.0 unx    11658 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/darknet.py
+-rw-r--r--  2.0 unx    40293 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/efficientnet.py
+-rw-r--r--  2.0 unx    16512 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/inception_v3.py
+-rw-r--r--  2.0 unx     4164 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mnist.py
+-rw-r--r--  2.0 unx     9546 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mobilenet.py
+-rw-r--r--  2.0 unx    13014 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/mobilenet_v2.py
+-rw-r--r--  2.0 unx    40800 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/resnet.py
+-rw-r--r--  2.0 unx    16649 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/classification/vgg.py
+-rw-r--r--  2.0 unx      824 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/__init__.py
+-rw-r--r--  2.0 unx     6820 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd.py
+-rw-r--r--  2.0 unx     8116 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_lite.py
+-rw-r--r--  2.0 unx     4046 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_mobilenet.py
+-rw-r--r--  2.0 unx     9069 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/ssd_resnet.py
+-rw-r--r--  2.0 unx    10188 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/detection/yolo_v3.py
+-rw-r--r--  2.0 unx      763 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/external/__init__.py
+-rw-r--r--  2.0 unx     6759 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/external/torchvision.py
+-rw-r--r--  2.0 unx      676 b- defN 24-Apr-04 11:59 sparseml/pytorch/models/recommendation/__init__.py
+-rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/__init__.py
+-rw-r--r--  2.0 unx     8673 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/activations.py
+-rw-r--r--  2.0 unx    11854 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/fatrelu.py
+-rw-r--r--  2.0 unx     1690 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/identity.py
+-rw-r--r--  2.0 unx     2828 b- defN 24-Apr-04 11:59 sparseml/pytorch/nn/se.py
+-rw-r--r--  2.0 unx     1243 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/__init__.py
+-rw-r--r--  2.0 unx    13638 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_as.py
+-rw-r--r--  2.0 unx    15582 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_module.py
+-rw-r--r--  2.0 unx     3955 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/analyzer_pruning.py
+-rw-r--r--  2.0 unx    26838 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/manager.py
+-rw-r--r--  2.0 unx    36844 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_creator_pruning.py
+-rw-r--r--  2.0 unx    23085 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    10449 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/mask_pruning_scorer.py
+-rw-r--r--  2.0 unx     6605 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/optimizer.py
+-rw-r--r--  2.0 unx    14879 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_as.py
+-rw-r--r--  2.0 unx     6101 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_lr.py
+-rw-r--r--  2.0 unx     9324 b- defN 24-Apr-04 11:59 sparseml/pytorch/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/__init__.py
+-rw-r--r--  2.0 unx     4534 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/cli.py
+-rw-r--r--  2.0 unx     1559 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/description.py
+-rw-r--r--  2.0 unx    15943 b- defN 24-Apr-04 11:59 sparseml/pytorch/recipe_template/main.py
+-rw-r--r--  2.0 unx      992 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1366 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/info.py
+-rw-r--r--  2.0 unx    32159 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/modifier.py
+-rw-r--r--  2.0 unx    18952 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/modifier_thinning.py
+-rw-r--r--  2.0 unx      705 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/__init__.py
+-rw-r--r--  2.0 unx     4741 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_distillation.py
+-rw-r--r--  2.0 unx    14742 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_distillation_base.py
+-rw-r--r--  2.0 unx    19177 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/distillation/modifier_per_layer.py
+-rw-r--r--  2.0 unx     1276 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/__init__.py
+-rw-r--r--  2.0 unx    29250 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/mask_creator.py
+-rw-r--r--  2.0 unx    24209 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/mask_params.py
+-rw-r--r--  2.0 unx    13389 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_as.py
+-rw-r--r--  2.0 unx    12006 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_powerpropagation.py
+-rw-r--r--  2.0 unx    10455 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_acdc.py
+-rw-r--r--  2.0 unx    33219 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_base.py
+-rw-r--r--  2.0 unx     5757 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_constant.py
+-rw-r--r--  2.0 unx     8857 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_layer.py
+-rw-r--r--  2.0 unx    15595 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_magnitude.py
+-rw-r--r--  2.0 unx    63519 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_mfac.py
+-rw-r--r--  2.0 unx     8774 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_movement.py
+-rw-r--r--  2.0 unx    24121 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_obs.py
+-rw-r--r--  2.0 unx    23735 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_rigl.py
+-rw-r--r--  2.0 unx    18245 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_structured.py
+-rw-r--r--  2.0 unx    17683 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/modifier_pruning_topkast.py
+-rw-r--r--  2.0 unx     4644 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/pruning/scorer.py
+-rw-r--r--  2.0 unx      813 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/__init__.py
+-rw-r--r--  2.0 unx     2220 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/constants.py
+-rw-r--r--  2.0 unx    34914 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/helpers.py
+-rw-r--r--  2.0 unx    33626 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/legacy_modifier_quantization.py
+-rw-r--r--  2.0 unx    26778 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/modifier_quantization.py
+-rw-r--r--  2.0 unx    13482 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantization_scheme.py
+-rw-r--r--  2.0 unx    18047 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantize.py
+-rw-r--r--  2.0 unx    76796 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/quantization/quantize_qat_export.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/__init__.py
+-rw-r--r--  2.0 unx     1778 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_epoch.py
+-rw-r--r--  2.0 unx     2883 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_logging.py
+-rw-r--r--  2.0 unx    24287 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_lr.py
+-rw-r--r--  2.0 unx    21497 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_params.py
+-rw-r--r--  2.0 unx     6690 b- defN 24-Apr-04 11:59 sparseml/pytorch/sparsification/training/modifier_regularizer.py
+-rw-r--r--  2.0 unx      943 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/__init__.py
+-rw-r--r--  2.0 unx     6490 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/export_onnx.py
+-rw-r--r--  2.0 unx     2870 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/presets.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/sampler.py
+-rw-r--r--  2.0 unx    43917 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/train.py
+-rw-r--r--  2.0 unx     7128 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/transforms.py
+-rw-r--r--  2.0 unx    16675 b- defN 24-Apr-04 11:59 sparseml/pytorch/torchvision/utils.py
+-rw-r--r--  2.0 unx     1160 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/__init__.py
+-rw-r--r--  2.0 unx     9706 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/benchmarker.py
+-rw-r--r--  2.0 unx     2846 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/callbacks.py
+-rw-r--r--  2.0 unx     1061 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/distributed.py
+-rw-r--r--  2.0 unx    30884 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/exporter.py
+-rw-r--r--  2.0 unx    42811 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/helpers.py
+-rw-r--r--  2.0 unx     1663 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/log_sparsification_info.py
+-rw-r--r--  2.0 unx    31374 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/logger.py
+-rw-r--r--  2.0 unx    27048 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/loss.py
+-rw-r--r--  2.0 unx    11754 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/model.py
+-rw-r--r--  2.0 unx    39117 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/module.py
+-rw-r--r--  2.0 unx     9180 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification.py
+-rw-r--r--  2.0 unx    30059 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/ssd_helpers.py
+-rw-r--r--  2.0 unx    12337 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/yolo_helpers.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/__init__.py
+-rw-r--r--  2.0 unx    15081 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/configs.py
+-rw-r--r--  2.0 unx     4336 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/helpers.py
+-rw-r--r--  2.0 unx     2788 b- defN 24-Apr-04 11:59 sparseml/pytorch/utils/sparsification_info/module_sparsification_info.py
+-rw-r--r--  2.0 unx      655 b- defN 24-Apr-04 11:59 sparseml/recipe_template/__init__.py
+-rw-r--r--  2.0 unx     4788 b- defN 24-Apr-04 11:59 sparseml/recipe_template/utils.py
+-rw-r--r--  2.0 unx     1058 b- defN 24-Apr-04 11:59 sparseml/sparsification/__init__.py
+-rw-r--r--  2.0 unx     9387 b- defN 24-Apr-04 11:59 sparseml/sparsification/analyzer.py
+-rw-r--r--  2.0 unx     9065 b- defN 24-Apr-04 11:59 sparseml/sparsification/info.py
+-rw-r--r--  2.0 unx    15565 b- defN 24-Apr-04 11:59 sparseml/sparsification/model_info.py
+-rw-r--r--  2.0 unx     2002 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_epoch.py
+-rw-r--r--  2.0 unx    10117 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_lr.py
+-rw-r--r--  2.0 unx     5505 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_params.py
+-rw-r--r--  2.0 unx    12845 b- defN 24-Apr-04 11:59 sparseml/sparsification/modifier_pruning.py
+-rw-r--r--  2.0 unx     3700 b- defN 24-Apr-04 11:59 sparseml/sparsification/oracle.py
+-rw-r--r--  2.0 unx    18570 b- defN 24-Apr-04 11:59 sparseml/sparsification/recipe_builder.py
+-rw-r--r--  2.0 unx    14413 b- defN 24-Apr-04 11:59 sparseml/sparsification/recipe_editor.py
+-rw-r--r--  2.0 unx     1250 b- defN 24-Apr-04 11:59 sparseml/sparsification/types.py
+-rw-r--r--  2.0 unx     1169 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/__init__.py
+-rw-r--r--  2.0 unx     7272 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/base.py
+-rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/__init__.py
+-rw-r--r--  2.0 unx     8121 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/dataset.py
+-rw-r--r--  2.0 unx     5600 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/helpers.py
+-rw-r--r--  2.0 unx     2768 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/registry.py
+-rw-r--r--  2.0 unx      807 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/__init__.py
+-rw-r--r--  2.0 unx    12686 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/cifar.py
+-rw-r--r--  2.0 unx     8690 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagefolder.py
+-rw-r--r--  2.0 unx     2032 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagenet.py
+-rw-r--r--  2.0 unx     4695 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/datasets/classification/imagenette.py
+-rw-r--r--  2.0 unx      805 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/framework/__init__.py
+-rw-r--r--  2.0 unx     5859 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/framework/info.py
+-rw-r--r--  2.0 unx      925 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/__init__.py
+-rw-r--r--  2.0 unx    19752 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/estimator.py
+-rw-r--r--  2.0 unx    14774 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/registry.py
+-rw-r--r--  2.0 unx      822 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/__init__.py
+-rw-r--r--  2.0 unx     3540 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mnist.py
+-rw-r--r--  2.0 unx    11161 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mobilenet.py
+-rw-r--r--  2.0 unx    18359 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/mobilenet_v2.py
+-rw-r--r--  2.0 unx    28103 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/resnet.py
+-rw-r--r--  2.0 unx    26886 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/models/classification/vgg.py
+-rw-r--r--  2.0 unx      865 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/nn/__init__.py
+-rw-r--r--  2.0 unx    18670 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/nn/layers.py
+-rw-r--r--  2.0 unx     1238 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/__init__.py
+-rw-r--r--  2.0 unx     8607 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/analyzer_module.py
+-rw-r--r--  2.0 unx     9591 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/manager.py
+-rw-r--r--  2.0 unx    19683 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/mask_creator_pruning.py
+-rw-r--r--  2.0 unx    33919 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/mask_pruning.py
+-rw-r--r--  2.0 unx    15955 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier.py
+-rw-r--r--  2.0 unx     1715 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_epoch.py
+-rw-r--r--  2.0 unx    10685 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_lr.py
+-rw-r--r--  2.0 unx     7092 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_params.py
+-rw-r--r--  2.0 unx    15702 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/modifier_pruning.py
+-rw-r--r--  2.0 unx     5682 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/schedule_lr.py
+-rw-r--r--  2.0 unx     9232 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/optim/sensitivity_pruning.py
+-rw-r--r--  2.0 unx      801 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/sparsification/__init__.py
+-rw-r--r--  2.0 unx     1385 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/sparsification/info.py
+-rw-r--r--  2.0 unx      967 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/__init__.py
+-rw-r--r--  2.0 unx    10913 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/exporter.py
+-rw-r--r--  2.0 unx      996 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/helpers.py
+-rw-r--r--  2.0 unx     1974 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/loss.py
+-rw-r--r--  2.0 unx     8119 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/nets_utils.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/summary.py
+-rw-r--r--  2.0 unx    12536 b- defN 24-Apr-04 11:59 sparseml/tensorflow_v1/utils/variable.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/tools/__init__.py
+-rw-r--r--  2.0 unx     1193 b- defN 24-Apr-04 11:59 sparseml/transformers/__init__.py
+-rw-r--r--  2.0 unx      985 b- defN 24-Apr-04 11:59 sparseml/transformers/base.py
+-rw-r--r--  2.0 unx    23904 b- defN 24-Apr-04 11:59 sparseml/transformers/export.py
+-rw-r--r--  2.0 unx     9485 b- defN 24-Apr-04 11:59 sparseml/transformers/integration_helper_functions.py
+-rw-r--r--  2.0 unx    30795 b- defN 24-Apr-04 11:59 sparseml/transformers/masked_language_modeling.py
+-rw-r--r--  2.0 unx    37002 b- defN 24-Apr-04 11:59 sparseml/transformers/question_answering.py
+-rw-r--r--  2.0 unx    40360 b- defN 24-Apr-04 11:59 sparseml/transformers/text_classification.py
+-rw-r--r--  2.0 unx      818 b- defN 24-Apr-04 11:59 sparseml/transformers/text_generation.py
+-rw-r--r--  2.0 unx    34389 b- defN 24-Apr-04 11:59 sparseml/transformers/token_classification.py
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/__init__.py
+-rw-r--r--  2.0 unx      749 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/__init__.py
+-rw-r--r--  2.0 unx     3052 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/base.py
+-rw-r--r--  2.0 unx     1160 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/dense.py
+-rw-r--r--  2.0 unx     8382 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/compressors/sparse_bitmask.py
+-rw-r--r--  2.0 unx      751 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/__init__.py
+-rw-r--r--  2.0 unx     3846 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/base.py
+-rw-r--r--  2.0 unx     1263 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/dense.py
+-rw-r--r--  2.0 unx     1236 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/config/sparse_bitmask.py
+-rw-r--r--  2.0 unx      718 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/__init__.py
+-rw-r--r--  2.0 unx     6092 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/compress_save.py
+-rw-r--r--  2.0 unx     1778 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/helpers.py
+-rw-r--r--  2.0 unx     6966 b- defN 24-Apr-04 11:59 sparseml/transformers/compression/utils/safetensors_load.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/__init__.py
+-rw-r--r--  2.0 unx     4265 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/callbacks.py
+-rw-r--r--  2.0 unx     2519 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/model_args.py
+-rw-r--r--  2.0 unx    11988 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/runner.py
+-rw-r--r--  2.0 unx    22620 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/session_mixin.py
+-rw-r--r--  2.0 unx    13101 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/text_generation.py
+-rw-r--r--  2.0 unx     4126 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/trainer.py
+-rw-r--r--  2.0 unx     3005 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/training_args.py
+-rw-r--r--  2.0 unx     1021 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/__init__.py
+-rw-r--r--  2.0 unx     8736 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/base.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/c4.py
+-rw-r--r--  2.0 unx     2537 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/cnn_dailymail.py
+-rw-r--r--  2.0 unx     4281 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/custom.py
+-rw-r--r--  2.0 unx     5399 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/data_args.py
+-rw-r--r--  2.0 unx     8961 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/data_helpers.py
+-rw-r--r--  2.0 unx     2892 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/evolcodealpaca.py
+-rw-r--r--  2.0 unx     2597 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/gsm8k.py
+-rw-r--r--  2.0 unx     3435 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/open_platypus.py
+-rw-r--r--  2.0 unx     1369 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/ptb.py
+-rw-r--r--  2.0 unx     3521 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/ultrachat_200k.py
+-rw-r--r--  2.0 unx     1237 b- defN 24-Apr-04 11:59 sparseml/transformers/finetune/data/wikitext.py
+-rw-r--r--  2.0 unx      922 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/__init__.py
+-rw-r--r--  2.0 unx    19272 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/question_answering.py
+-rw-r--r--  2.0 unx     1874 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_config.py
+-rw-r--r--  2.0 unx    20264 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_model.py
+-rw-r--r--  2.0 unx     2327 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/sparse_tokenizer.py
+-rw-r--r--  2.0 unx    40305 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/trainer.py
+-rw-r--r--  2.0 unx     1890 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/training_args.py
+-rw-r--r--  2.0 unx      866 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/__init__.py
+-rw-r--r--  2.0 unx     2429 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/base.py
+-rw-r--r--  2.0 unx     3922 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modification_objects.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modify_model.py
+-rw-r--r--  2.0 unx     9148 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_bert.py
+-rw-r--r--  2.0 unx     5883 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_distilbert.py
+-rw-r--r--  2.0 unx     8457 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_llama.py
+-rw-r--r--  2.0 unx     7689 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_mistral.py
+-rw-r--r--  2.0 unx     2549 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_mobilebert.py
+-rw-r--r--  2.0 unx     9862 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/modifying_opt.py
+-rw-r--r--  2.0 unx     1486 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/modification/registry.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/__init__.py
+-rw-r--r--  2.0 unx    19683 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/export.py
+-rw-r--r--  2.0 unx     7695 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/obcq.py
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/utils/__init__.py
+-rw-r--r--  2.0 unx     3671 b- defN 24-Apr-04 11:59 sparseml/transformers/sparsification/obcq/utils/helpers.py
+-rw-r--r--  2.0 unx      805 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/__init__.py
+-rw-r--r--  2.0 unx    20055 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/helpers.py
+-rw-r--r--  2.0 unx     6711 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/initializers.py
+-rw-r--r--  2.0 unx     5081 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/load_task_dataset.py
+-rw-r--r--  2.0 unx     2764 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/load_task_model.py
+-rw-r--r--  2.0 unx     2536 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/metrics.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/optimizations.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Apr-04 11:59 sparseml/transformers/utils/preprocessing_functions.py
+-rw-r--r--  2.0 unx      844 b- defN 24-Apr-04 11:59 sparseml/utils/__init__.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Apr-04 11:59 sparseml/utils/frameworks.py
+-rw-r--r--  2.0 unx    31604 b- defN 24-Apr-04 11:59 sparseml/utils/helpers.py
+-rw-r--r--  2.0 unx     3983 b- defN 24-Apr-04 11:59 sparseml/utils/restricted_eval.py
+-rw-r--r--  2.0 unx     1083 b- defN 24-Apr-04 11:59 sparseml/utils/singleton.py
+-rw-r--r--  2.0 unx     6312 b- defN 24-Apr-04 11:59 sparseml/utils/worker.py
+-rw-r--r--  2.0 unx     2952 b- defN 24-Apr-04 11:59 sparseml/utils/wrapper.py
+-rw-r--r--  2.0 unx      819 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/__init__.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/cifar.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/coco.py
+-rw-r--r--  2.0 unx     1217 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/helpers.py
+-rw-r--r--  2.0 unx    23366 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/imagenet.py
+-rw-r--r--  2.0 unx     8967 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/imagenette.py
+-rw-r--r--  2.0 unx     1009 b- defN 24-Apr-04 11:59 sparseml/utils/datasets/voc.py
+-rw-r--r--  2.0 unx      633 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/context.py
+-rw-r--r--  2.0 unx     6631 b- defN 24-Apr-04 11:59 sparseml/utils/fsdp/helpers.py
+-rw-r--r--  2.0 unx      656 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/__init__.py
+-rw-r--r--  2.0 unx    10897 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/module.py
+-rw-r--r--  2.0 unx     1696 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/utils.py
+-rw-r--r--  2.0 unx      680 b- defN 24-Apr-04 11:59 sparseml/utils/pytorch/pruning/__init__.py
+-rw-r--r--  2.0 unx     1875 b- defN 24-Apr-04 11:59 sparseml/yolact/COCO.sh
+-rw-r--r--  2.0 unx     1418 b- defN 24-Apr-04 11:59 sparseml/yolact/COCO_test.sh
+-rw-r--r--  2.0 unx     4020 b- defN 24-Apr-04 11:59 sparseml/yolact/__init__.py
+-rw-r--r--  2.0 unx     1784 b- defN 24-Apr-04 11:59 sparseml/yolact/scripts.py
+-rw-r--r--  2.0 unx     1440 b- defN 24-Apr-04 11:59 sparseml/yolov5/__init__.py
+-rw-r--r--  2.0 unx     4505 b- defN 24-Apr-04 11:59 sparseml/yolov5/helpers.py
+-rw-r--r--  2.0 unx     1609 b- defN 24-Apr-04 11:59 sparseml/yolov5/scripts.py
+-rw-r--r--  2.0 unx     1220 b- defN 24-Apr-04 11:59 sparseml/yolov5/yolov5.status.yaml
+-rw-r--r--  2.0 unx     1117 b- defN 24-Apr-04 11:59 sparseml/yolov8/__init__.py
+-rw-r--r--  2.0 unx     6061 b- defN 24-Apr-04 11:59 sparseml/yolov8/default.yaml
+-rw-r--r--  2.0 unx     2815 b- defN 24-Apr-04 11:59 sparseml/yolov8/export.py
+-rw-r--r--  2.0 unx     2259 b- defN 24-Apr-04 11:59 sparseml/yolov8/modules.py
+-rw-r--r--  2.0 unx     7394 b- defN 24-Apr-04 11:59 sparseml/yolov8/train.py
+-rw-r--r--  2.0 unx    37860 b- defN 24-Apr-04 11:59 sparseml/yolov8/trainers.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-04 11:59 sparseml/yolov8/val.py
+-rw-r--r--  2.0 unx     8459 b- defN 24-Apr-04 11:59 sparseml/yolov8/validators.py
+-rw-r--r--  2.0 unx      685 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/__init__.py
+-rw-r--r--  2.0 unx     6683 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/export_samples.py
+-rw-r--r--  2.0 unx     4041 b- defN 24-Apr-04 11:59 sparseml/yolov8/utils/helpers.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/LICENSE
+-rw-r--r--  2.0 unx    13747 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS
+-rw-r--r--  2.0 unx    23627 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/METADATA
+-rw-r--r--  2.0 unx     2085 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/NOTICE
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/WHEEL
+-rw-r--r--  2.0 unx     3122 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    56768 b- defN 24-Apr-04 12:00 sparseml_nightly-1.8.0.20240404.dist-info/RECORD
+567 files, 4551890 bytes uncompressed, 1227493 bytes compressed:  73.0%
```

## zipnote {}

```diff
@@ -1671,32 +1671,32 @@
 
 Filename: sparseml/yolov8/utils/export_samples.py
 Comment: 
 
 Filename: sparseml/yolov8/utils/helpers.py
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/LICENSE
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/LICENSE
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/LICENSE-ULTRALYTICS
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/METADATA
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/METADATA
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/NOTICE
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/NOTICE
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/WHEEL
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/WHEEL
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/entry_points.txt
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/top_level.txt
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt
 Comment: 
 
-Filename: sparseml_nightly-1.8.0.20240401.dist-info/RECORD
+Filename: sparseml_nightly-1.8.0.20240404.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sparseml/export/validators.py

```diff
@@ -13,16 +13,17 @@
 # limitations under the License.
 
 import glob
 import logging
 import os.path
 from collections import OrderedDict
 from pathlib import Path
-from typing import Callable, List, Optional, Union
+from typing import Callable, List, Optional
 from typing import OrderedDict as OrderedDictType
+from typing import Union
 
 import numpy
 import onnx
 
 from sparseml.export.export_data import InputsNames, LabelNames, OutputsNames
 from sparseml.export.helpers import ONNX_MODEL_NAME, onnx_data_files
 from sparsezoo.utils.numpy import load_numpy
```

## sparseml/modifiers/quantization/utils/quantize.py

```diff
@@ -42,27 +42,31 @@
     from torch.nn import intrinsic as torch_intrinsic
 except Exception:
     torch_quantization = None
     torch_intrinsic = None
 
 
 __all__ = [
+    "LAYER_NAME_ALIASES",
     "convert_module_qat_from_schemes",
     "is_qat_helper_module",
     "is_quantizable_module",
     "set_quantization_schemes",
     "set_qconfigs_from_quantization_schemes",
     "add_input_activation_quant_wrappers",
     "add_output_activation_observers",
     "raise_if_torch_quantization_not_available",
     "raise_if_already_quantized",
     "is_module_quantized",
 ]
 
 
+LAYER_NAME_ALIASES: Dict[str, List[str]] = {"SiLU": ["SiLUActivation"]}
+
+
 def is_qat_helper_module(module: Module) -> bool:
     """
     :param module: module to check
     :return: True if module is an instance of a torch QAT helper class
     """
     # prefer FakeQuantizeBase which was introduced around torch 1.9
     fake_quantize_class = getattr(
@@ -168,15 +172,18 @@
         # override default scheme if necessary
         override_key = _match_submodule_name_or_type(
             submodule, submodule_name, scheme_overrides
         )
         submodule_scheme = (
             scheme if override_key is None else scheme_overrides[override_key]
         )
-        is_module_type_override = override_key == submodule.__class__.__name__
+        is_module_type_override = (
+            override_key == submodule.__class__.__name__
+            or submodule.__class__.__name__ in LAYER_NAME_ALIASES.get(override_key, [])
+        )
 
         if getattr(submodule, "wrap_qat", False):
             # wrap_qat overrides default scheme behavior
             wrap_qat_targets[submodule_name] = submodule_scheme
         elif is_module_type_override or is_quantizable_module(submodule):
             # is base quantizable module or user specifically targeted module type
             raise_if_already_quantized(submodule_name, submodule)
@@ -400,22 +407,28 @@
 def _match_submodule_name_or_type(
     submodule: Module, submodule_name: str, names_or_types: List[str]
 ) -> Optional[str]:
     # match preferences:
     #   1. match module type name
     #   2. match the submodule prefix (longest first)
     submodule_match = ""
+    submodule_type = submodule.__class__.__name__
     for name_or_type in names_or_types:
         name_to_compare = submodule_name[:]
         name_to_compare = fix_fsdp_module_name(name_to_compare)
         if name_to_compare.startswith("module."):
             name_to_compare = name_to_compare[7:]
-        if name_or_type == submodule.__class__.__name__:
+        if name_or_type == submodule_type:
             # type match, return type name
             return name_or_type
+        if submodule_type in LAYER_NAME_ALIASES.get(name_or_type, []):
+            # submodule type is aliased to a target type in the recipe
+            # return type in recipe so it can be matched to its target
+            # scheme
+            return name_or_type
         if name_to_compare.startswith(name_or_type) and (
             len(name_or_type) > len(submodule_match)
         ):
             # match to most specific submodule name
             submodule_match = name_or_type
     return submodule_match or None  # return None if no match
 
@@ -470,16 +483,19 @@
         for type_or_name in types_or_names:
             matched = False
             for submodule_name, submodule in model.named_modules():
                 name_to_compare = submodule_name[:]
                 name_to_compare = fix_fsdp_module_name(name_to_compare)
                 if name_to_compare.startswith("module."):
                     name_to_compare = name_to_compare[7:]
-                if name_to_compare.startswith(type_or_name) or (
-                    submodule.__class__.__name__ == type_or_name
+                if (
+                    name_to_compare.startswith(type_or_name)
+                    or (submodule.__class__.__name__ == type_or_name)
+                    or type_or_name
+                    in LAYER_NAME_ALIASES.get(submodule.__class__.__name__, [])
                 ):
                     matched = True
                     break
             if not matched:
                 unmatched.append(type_or_name)
         return unmatched
```

## sparseml/transformers/compression/compressors/base.py

```diff
@@ -66,19 +66,18 @@
         :param param_name: name of parameterized layer to replace
         :param data: tensor to insert into model
         :param model: pytorch model to insert data into
         """
         model_device = operator.attrgetter(param_name)(model).device
         set_layer(param_name, Parameter(data.to(model_device)), model)
 
-    def overwrite_weights(self, pretrained_model_name_or_path: str, model: Module):
+    def overwrite_weights(self, model_path: str, model: Module):
         """
-        Overwrites the weights in model with weights decompressed from
-        pretrained_model_name_or_path
+        Overwrites the weights in model with weights decompressed from model_path
 
-        :param pretrained_model_name_or_path: path to compressed weights
+        :param model_path: path to compressed weights
         :param model: pytorch model to load decompressed weights into
         """
-        dense_gen = self.decompress(pretrained_model_name_or_path)
+        dense_gen = self.decompress(model_path)
         for name, data in tqdm(dense_gen, desc="Decompressing model"):
             ModelCompressor.replace_layer(name, data, model)
         setattr(model, SPARSITY_CONFIG_NAME, self.config)
```

## sparseml/transformers/compression/compressors/sparse_bitmask.py

```diff
@@ -66,15 +66,15 @@
             for key in bitmask_dict.keys():
                 if key in compressed_dict:
                     _LOGGER.warn(
                         f"Expected all compressed state_dict keys to be unique, but "
                         f"found an existing entry for {key}. The existing entry will "
                         "be replaced."
                     )
-            compressed_dict |= bitmask_dict
+            compressed_dict.update(bitmask_dict)
 
         return compressed_dict
 
     def decompress(self, model_path: str) -> Generator[Tuple[str, Tensor], None, None]:
         """
         Reads a bitmask compressed state dict located at model_path and returns a
         generator for sequentially decompressing back to a dense state dict
```

## sparseml/transformers/compression/utils/safetensors_load.py

```diff
@@ -12,28 +12,71 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import json
 import os
 import re
 import struct
-from typing import Dict, List
+from typing import Dict, List, Optional
 
-from transformers.utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME
+from transformers.utils import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, cached_file
 
 
 __all__ = [
+    "get_safetensors_folder",
     "get_safetensors_header",
     "match_param_name",
     "merge_names",
     "get_weight_mappings",
     "get_nested_weight_mappings",
 ]
 
 
+def get_safetensors_folder(
+    pretrained_model_name_or_path: str, cache_dir: Optional[str] = None
+) -> str:
+    """
+    Given a Hugging Face stub or a local path, return the folder containing the
+    safetensors weight files
+
+    :param pretrained_model_name_or_path: local path to model or HF stub
+    :param cache_dir: optional cache dir to search through, if none is specified the
+    model will be searched for in the default TRANSFORMERS_CACHE
+    :return: local folder containing model data
+    """
+    if os.path.exists(pretrained_model_name_or_path):
+        # argument is a path to a local folder
+        return pretrained_model_name_or_path
+
+    safetensors_path = cached_file(
+        pretrained_model_name_or_path,
+        SAFE_WEIGHTS_NAME,
+        cache_dir=cache_dir,
+        _raise_exceptions_for_missing_entries=False,
+    )
+    index_path = cached_file(
+        pretrained_model_name_or_path,
+        SAFE_WEIGHTS_INDEX_NAME,
+        cache_dir=cache_dir,
+        _raise_exceptions_for_missing_entries=False,
+    )
+    if safetensors_path is not None:
+        # found a single cached safetensors file
+        return os.path.split(safetensors_path)[0]
+    if index_path is not None:
+        # found a cached safetensors weight index file
+        return os.path.split(index_path)[0]
+
+    # model weights could not be found locally or cached from HF Hub
+    raise ValueError(
+        "Could not locate safetensors weight or index file from "
+        f"{pretrained_model_name_or_path}."
+    )
+
+
 def get_safetensors_header(safetensors_path: str) -> Dict[str, str]:
     """
     Extracts the metadata from a safetensors file as JSON
 
     :param safetensors_path: path to a safetensors file
     :return: dictionary of metadata extracted from the safetensors file
     """
@@ -101,14 +144,18 @@
             header[key] = SAFE_WEIGHTS_NAME
         header.pop("__metadata__", None)
     elif os.path.exists(index_path):
         # we have multiple safetensors file, read from index
         with open(index_path, "r", encoding="utf-8") as f:
             index = json.load(f)
         header = index["weight_map"]
+    else:
+        raise ValueError(
+            f"Could not find a safetensors weight or index file at {model_path}"
+        )
 
     # convert weight locations to full paths
     for key, value in header.items():
         header[key] = os.path.join(model_path, value)
 
     return header
```

## sparseml/transformers/finetune/callbacks.py

```diff
@@ -105,53 +105,26 @@
 
     def __init__(self, trainer, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.trainer = trainer
         self.on_begin_called = False
         self.quant_start_epoch = math.inf
 
-    def check_disable(self, epoch: float, force: bool = False):
-        """
-        If needed due to active quantization, disable FP16 training
-        """
-        if (
-            force or hasattr(self.trainer, "scaler") and self.trainer.scaler._enabled
-        ) and self.qat_active():
-            self.disable_amp(epoch)
-
     def qat_active(self) -> bool:
         """
         :return: True if a quantization modifier is active in the current session
         """
         session = session_manager.active_session()
         return session.state.model.qat_active()
 
-    def disable_amp(self, epoch: float):
-        """
-        Disable FP16 training
-
-        :param epoch: epoch to disable from
-        """
-        if not self.on_begin_called:
-            # disable if training loops haven't started so we don't load
-            # the empty scaler state dict and instead disable it from the start
-            self.trainer.use_cuda_amp = False
-
-        if hasattr(self.trainer, "scaler"):
-            self.trainer.scaler._enabled = False
-
-        self.quant_start_epoch = epoch
-        _LOGGER.info(f"entering QAT phase at epoch {epoch}, disabling FP16 training")
-
     def on_epoch_begin(
         self,
         args: TrainingArguments,
         state: TrainerState,
         control: TrainerControl,
         **kwargs,
     ):
         """
-        Event called at the beginning of an epoch. Disables FP16 training.
+        Event called at the beginning of an epoch.
         """
         super().on_epoch_begin(args, state, control, **kwargs)
         self.on_begin_called = True
-        self.check_disable(state.epoch)
```

## sparseml/transformers/finetune/session_mixin.py

```diff
@@ -359,15 +359,14 @@
         :param args: positional args to pass to super().train()
         :param stage: Optional stage of recipe to run, or None to run all stages
         :param kwargs: keyword args to pass to super().train()
         :return: the output from super.train()
         """
         checkpoint, epoch = self._calculate_checkpoint_info(kwargs)
         self.initialize_session(epoch=epoch, checkpoint=checkpoint, stage=stage)
-        self.callback_disable_fp16.check_disable(epoch, force=True)
         self.accelerator.wait_for_everyone()
         output = super().train(*args, **kwargs)
         self.accelerator.wait_for_everyone()
         self.finalize_session()
 
         self.accelerator.wait_for_everyone()
 
@@ -389,21 +388,15 @@
 
         :param args: positional args to pass to super().evaluate()
         :param kwargs: keyword args to pass to super().evaluate()
         :return: the output from super.evaluate()
         """
         self.initialize_structure()
 
-        # Always evaluate w/ fp32 to be closer to DeepSparse
-        use_cuda_amp = self.use_cuda_amp
-        if not self.args.fp16_full_eval and not self.args.bf16_full_eval:
-            self.use_cuda_amp = False
-
         output = super().evaluate(*args, **kwargs)
-        self.use_cuda_amp = use_cuda_amp
         self.finalize_session()
 
         return output
 
     def predict(self, *args, **kwargs):
         """
         Run a sparsification prediction cycle.
```

## sparseml/transformers/finetune/trainer.py

```diff
@@ -87,18 +87,14 @@
             with warnings.catch_warnings(record=True) as caught_warnings:
                 if self.lr_scheduler is not None:
                     torch.save(
                         self.lr_scheduler.state_dict(),
                         os.path.join(output_dir, "scheduler.pt"),
                     )
             reissue_pt_warnings(caught_warnings)
-            if self.use_cuda_amp:
-                torch.save(
-                    self.scaler.state_dict(), os.path.join(output_dir, "scaler.pt")
-                )
 
     def _save_checkpoint(self, model, trial, metrics=None):
         # Call into the save checkpoint by HF Transformers, which saves the
         # best metric if required
         super()._save_checkpoint(model, trial, metrics=metrics)
         if (
             self.args.metric_for_best_model is None
```

## sparseml/transformers/sparsification/question_answering.py

```diff
@@ -75,19 +75,14 @@
         ignore_keys=None,
         metric_key_prefix: str = "eval",
     ):
         eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
         eval_dataloader = self.get_eval_dataloader(eval_dataset)
         eval_examples = self.eval_examples if eval_examples is None else eval_examples
 
-        # Always evaluate w/ fp32 to be closer to DeepSparse
-        use_cuda_amp = self.use_cuda_amp
-        if not self.args.fp16_full_eval and not self.args.bf16_full_eval:
-            self.use_cuda_amp = False
-
         # Temporarily disable metric computation, we will do it in the loop here.
         compute_metrics = self.compute_metrics
         self.compute_metrics = None
         eval_loop = (
             self.prediction_loop
             if self.args.use_legacy_prediction_loop
             else self.evaluation_loop
@@ -125,16 +120,14 @@
             # (compile, execute times, ops, etc.)
             xm.master_print(met.metrics_report())
 
         self.control = self.callback_handler.on_evaluate(
             self.args, self.state, self.control, metrics
         )
 
-        self.use_cuda_amp = use_cuda_amp
-
         return metrics
 
     def predict(
         self,
         predict_dataset,
         predict_examples,
         ignore_keys=None,
```

## sparseml/transformers/sparsification/sparse_model.py

```diff
@@ -31,14 +31,15 @@
 from transformers.file_utils import WEIGHTS_NAME
 
 from sparseml.pytorch.model_load.helpers import (
     apply_recipe_structure_to_model,
     log_model_load,
 )
 from sparseml.transformers.compression.utils import (
+    get_safetensors_folder,
     infer_compressor_from_model_config,
     modify_save_pretrained,
 )
 from sparseml.transformers.sparsification.modification import modify_model
 from sparseml.transformers.utils.helpers import resolve_recipe
 from sparseml.utils import download_zoo_training_dir
 from sparseml.utils.fsdp.context import main_process_first_context
@@ -124,17 +125,22 @@
         logger.setLevel(level=restore_log_level)
         model = modify_model(model)
         # override the PreTrainedModel instance with compression save function
         modify_save_pretrained(model)
 
         # If model is compressed on disk, decompress and load the weights
         if compressor is not None:
-            compressor.overwrite_weights(
-                pretrained_model_name_or_path=pretrained_model_name_or_path, model=model
+            # if we loaded from a HF stub, find the cached model
+            model_path = get_safetensors_folder(
+                pretrained_model_name_or_path, cache_dir=kwargs.get("cache_dir", None)
             )
+
+            # decompress weights
+            compressor.overwrite_weights(model_path=model_path, model=model)
+
         recipe = resolve_recipe(recipe=recipe, model_path=pretrained_model_name_or_path)
         if recipe:
             apply_recipe_structure_to_model(
                 model=model,
                 model_path=pretrained_model_name_or_path,
                 recipe_path=recipe,
             )
```

## sparseml/transformers/sparsification/trainer.py

```diff
@@ -31,15 +31,15 @@
 from torch.nn import Module
 from transformers import Trainer as HFTransformersTrainer
 from transformers import TrainerCallback, TrainerControl, TrainingArguments
 from transformers.file_utils import PaddingStrategy
 from transformers.integrations import TensorBoardCallback
 from transformers.trainer_callback import TrainerState
 from transformers.trainer_pt_utils import reissue_pt_warnings
-from transformers.trainer_utils import ShardedDDPOption, get_last_checkpoint
+from transformers.trainer_utils import get_last_checkpoint
 
 from sparseml.pytorch.model_load.helpers import log_model_load
 from sparseml.pytorch.optim import ScheduledModifierManager, ScheduledOptimizer
 from sparseml.pytorch.sparsification.quantization.helpers import (
     initialize_channel_wise_scale_zp,
 )
 from sparseml.pytorch.utils import (
@@ -783,15 +783,14 @@
 
         :param args: positional args to pass to super().train()
         :param kwargs: keyword args to pass to super().train()
         :return: the output from super.train()
         """
         checkpoint, epoch = self._generate_apply_manager_params(kwargs)
         applied = self.apply_manager(epoch=epoch, checkpoint=checkpoint)
-        self.callback_disable_fp16.check_disable(epoch, force=True)
         output = None
         if not self.one_shot:
             output = super().train(*args, **kwargs)
             if applied:
                 self.finalize_manager()
         else:
             _LOGGER.info(f"Skipping Training due to one-shot: {self.one_shot}")
@@ -807,21 +806,15 @@
 
         :param args: positional args to pass to super().evaluate()
         :param kwargs: keyword args to pass to super().evaluate()
         :return: the output from super.evaluate()
         """
         applied = self.apply_manager(epoch=math.inf, checkpoint=None)
 
-        # Always evaluate w/ fp32 to be closer to DeepSparse
-        use_cuda_amp = self.use_cuda_amp
-        if not self.args.fp16_full_eval and not self.args.bf16_full_eval:
-            self.use_cuda_amp = False
-
         output = super().evaluate(*args, **kwargs)
-        self.use_cuda_amp = use_cuda_amp
         if applied:
             self.finalize_manager()
 
         return output
 
     def predict(self, *args, **kwargs):
         """
@@ -890,34 +883,27 @@
         Save optimizer, scheduler and scaler
 
         :param output_dir: The output model directory to save the above
         """
         if output_dir is None:
             output_dir = self.args.output_dir
 
-        if self.sharded_ddp == ShardedDDPOption.SIMPLE and self.optimizer is not None:
-            self.optimizer.consolidate_state_dict()
-
         if self.is_world_process_zero():
             if self.optimizer is not None:
                 torch.save(
                     self.optimizer.state_dict(),
                     os.path.join(output_dir, "optimizer.pt"),
                 )
             with warnings.catch_warnings(record=True) as caught_warnings:
                 if self.lr_scheduler is not None:
                     torch.save(
                         self.lr_scheduler.state_dict(),
                         os.path.join(output_dir, "scheduler.pt"),
                     )
             reissue_pt_warnings(caught_warnings)
-            if self.use_cuda_amp:
-                torch.save(
-                    self.scaler.state_dict(), os.path.join(output_dir, "scaler.pt")
-                )
 
     def _load_optimizer_and_scheduler(self, checkpoint):
         """
         Override the Transformers Trainer so that optimizer, scheduler and scaler could
         be loaded also from the input model folder, which is our use case (instead of
         only from a separate checkpoint folder).
         """
@@ -1023,55 +1009,36 @@
 
     def __init__(self, trainer: RecipeManagerTrainerInterface, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.trainer = trainer
         self.on_begin_called = False
         self.quant_start_epoch = math.inf
 
-    def check_disable(self, epoch: float, force: bool = False):
-        if (
-            force or hasattr(self.trainer, "scaler") and self.trainer.scaler._enabled
-        ) and self.qat_active(epoch):
-            self.disable_amp(epoch)
-
     def qat_active(self, epoch: float) -> bool:
         manager_q_active = arch_manager_q_active = False
         if self.trainer.manager:
             manager_q_active = bool(self.trainer.manager.qat_active(epoch))
         if self.trainer.arch_manager:
             arch_manager_q_active = bool(
                 self.trainer.arch_manager.quantization_modifiers
             )
         return manager_q_active or arch_manager_q_active
 
-    def disable_amp(self, epoch: float):
-        if not self.on_begin_called:
-            # disable if training loops haven't started so we don't load
-            # the empty scaler state dict and instead disable it from the start
-            self.trainer.use_cuda_amp = False
-
-        if hasattr(self.trainer, "scaler"):
-            self.trainer.scaler._enabled = False
-
-        self.quant_start_epoch = epoch
-        _LOGGER.info(f"entering QAT phase at epoch {epoch}, disabling FP16 training")
-
     def on_epoch_begin(
         self,
         args: TrainingArguments,
         state: TrainerState,
         control: TrainerControl,
         **kwargs,
     ):
         """
         Event called at the beginning of an epoch. Disables
         """
         super().on_epoch_begin(args, state, control, **kwargs)
         self.on_begin_called = True
-        self.check_disable(state.epoch)
 
         if state.epoch > self.quant_start_epoch:
             _LOGGER.info(self.trainer.model)
 
 
 def _get_teacher_base_column_name(column_name: str) -> Optional[str]:
     # if column was created by teacher tokenizer, return the base name
```

## sparseml/transformers/sparsification/modification/base.py

```diff
@@ -19,16 +19,16 @@
 from sparseml.base import get_version
 
 
 _LOGGER = logging.getLogger(__name__)
 
 __all__ = ["check_transformers_version"]
 
-_TRANSFORMERS_MIN_VERSION = "4.34.1"
-_TRANSFORMERS_MAX_VERSION = "4.35.0"
+_TRANSFORMERS_MIN_VERSION = "4.39.0"
+_TRANSFORMERS_MAX_VERSION = "4.39.2"
 
 
 def check_transformers_version(
     module_version_max: str = _TRANSFORMERS_MAX_VERSION,
     model_version_min: str = _TRANSFORMERS_MIN_VERSION,
 ) -> bool:
     """
```

## sparseml/transformers/sparsification/modification/modifying_bert.py

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 
 """
 Modification to the original Bert model required in the
 context of SparseML
 """
 
+
 import logging
 import math
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
 from transformers.models.bert.modeling_bert import BertAttention, BertSelfAttention
@@ -118,30 +119,24 @@
             key_layer = self.transpose_for_scores(self.key(hidden_states))
             value_layer = self.transpose_for_scores(self.value(hidden_states))
 
         query_layer = self.transpose_for_scores(mixed_query_layer)
 
         use_cache = past_key_value is not None
         if self.is_decoder:
-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor)
-            # of all cross attention key/value_states.
-            # Further calls to cross_attention
-            # layer can then reuse all cross-attention
+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states. # noqa
+            # Further calls to cross_attention layer can then reuse all cross-attention
             # key/value_states (first "if" case)
-            # if uni-directional self-attention
-            # (decoder) save Tuple(torch.Tensor, torch.Tensor) of
-            # all previous decoder key/value_states.
-            # Further calls to uni-directional self-attention
-            # can concat previous decoder key/value_states to
-            # current projected key/value_states (third "elif" case)
+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of # noqa
+            # all previous decoder key/value_states. Further calls to uni-directional self-attention # noqa
+            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case) # noqa
             # if encoder bi-directional self-attention `past_key_value` is always `None`
             past_key_value = (key_layer, value_layer)
 
-        # Take the dot product between "query" and "key"
-        # to get the raw attention scores.
+        # Take the dot product between "query" and "key" to get the raw attention scores. # noqa
         # ==== SparseML MODIFICATION ====
         attention_scores = self.attention_scores_matmul(
             query_layer, key_layer.transpose(-1, -2)
         )
         # ==============================
 
         if (
@@ -185,16 +180,15 @@
                     attention_scores
                     + relative_position_scores_query
                     + relative_position_scores_key
                 )
 
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
         if attention_mask is not None:
-            # Apply the attention mask is
-            # (precomputed for all layers in BertModel forward() function)
+            # Apply the attention mask is (precomputed for all layers in BertModel forward() function) # noqa
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
         attention_probs = nn.functional.softmax(attention_scores, dim=-1)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
```

## sparseml/transformers/sparsification/modification/modifying_distilbert.py

```diff
@@ -19,15 +19,18 @@
 
 import logging
 import math
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
-from transformers.models.distilbert.modeling_distilbert import MultiHeadSelfAttention
+from transformers.models.distilbert.modeling_distilbert import (
+    DistilBertFlashAttention2,
+    MultiHeadSelfAttention,
+)
 
 from sparseml.pytorch.utils.helpers import swap_modules
 from sparseml.transformers.sparsification.modification.modification_objects import (
     QATMatMul,
 )
 from sparseml.transformers.sparsification.modification.registry import (
     ModificationRegistry,
@@ -41,22 +44,30 @@
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the DistilBert model to be compatible with SparseML
 
     1. Replaces the MultiHeadSelfAttention modules with
         MultiHeadSelfAttentionWithQuantizableMatmuls modules
 
+    Note: This function will not alter any of the alternatives
+    to the MultiHeadSelfAttention module such as DistilBertFlashAttention2
+
     :param model: the original DistilBert model
     :return: the modified DistilBert model
     """
     for name, submodule in model.named_modules():
         if isinstance(submodule, MultiHeadSelfAttention):
             swap_modules(
                 model, name, MultiHeadSelfAttentionWithQuantizableMatmuls(submodule)
             )
+        if isinstance(submodule, DistilBertFlashAttention2):
+            _LOGGER.debug(
+                f"The model contains {submodule.__class__.__name__} "
+                "module, which will not be modified"
+            )
     return model
 
 
 class MultiHeadSelfAttentionWithQuantizableMatmuls(MultiHeadSelfAttention):
     """
     Wrapper around the original MultiHeadSelfAttention module to replace the
     matmul operations with quantizable matmul operations
@@ -88,23 +99,20 @@
         Parameters:
             query: torch.tensor(bs, seq_length, dim)
             key: torch.tensor(bs, seq_length, dim)
             value: torch.tensor(bs, seq_length, dim)
             mask: torch.tensor(bs, seq_length)
 
         Returns:
-            weights: torch.tensor(bs, n_heads, seq_length, seq_length)
-            Attention weights context: torch.tensor(bs,
-            seq_length, dim) Contextualized layer.
-            Optional: only if `output_attentions=True`
+            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs, # noqa
+            seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True` # noqa
         """
         bs, q_length, dim = query.size()
         k_length = key.size(1)
-        # assert dim == self.dim, f'Dimensions do not match:
-        # {dim} input vs {self.dim} configured'
+        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured' # noqa
         # assert key.size() == value.size()
 
         dim_per_head = self.dim // self.n_heads
 
         mask_reshp = (bs, 1, 1, k_length)
 
         def shape(x: torch.Tensor) -> torch.Tensor:
```

## sparseml/transformers/sparsification/modification/modifying_llama.py

```diff
@@ -21,16 +21,18 @@
 import math
 from typing import Optional, Tuple
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from transformers.models.llama.modeling_llama import (
+    Cache,
     LlamaAttention,
     LlamaFlashAttention2,
+    LlamaSdpaAttention,
     apply_rotary_pos_emb,
     repeat_kv,
 )
 
 from sparseml.pytorch.utils.helpers import swap_modules
 from sparseml.transformers.sparsification.modification.modification_objects import (
     QuantizableIdentity,
@@ -50,22 +52,23 @@
     Modify the LLaMa model to be compatible with SparseML
 
     1. Replaces the LlamaAttention modules with
         LlamaAttentionWithQuantizableMatmuls modules
 
     Note: This function will not alter any of the alternatives
     to the LlamaAttention module such as LlamaFlashAttention2
+    or LlamaSdpaAttention
 
     :param model: the original LLaMa model
     :return: the modified LLaMa model
     """
     for name, submodule in model.named_modules():
         if isinstance(submodule, LlamaAttention):
             swap_modules(model, name, LlamaAttentionWithQuantizableMatmuls(submodule))
-        elif isinstance(submodule, LlamaFlashAttention2):
+        elif isinstance(submodule, (LlamaSdpaAttention, LlamaFlashAttention2)):
             _LOGGER.debug(
                 f"The model contains {submodule.__class__.__name__} "
                 "module, which will not be modified"
             )
     return model
 
 
@@ -117,18 +120,19 @@
         )
 
     def forward(
         self,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        past_key_value: Optional[Cache] = None,
         output_attentions: bool = False,
         use_cache: bool = False,
-        padding_mask: Optional[torch.LongTensor] = None,
+        cache_position: Optional[torch.LongTensor] = None,
+        **kwargs,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         bsz, q_len, _ = hidden_states.size()
 
         if self.config.pretraining_tp > 1:
             key_value_slicing = (
                 self.num_key_value_heads * self.head_dim
             ) // self.config.pretraining_tp
@@ -167,65 +171,54 @@
         key_states = key_states.view(
             bsz, q_len, self.num_key_value_heads, self.head_dim
         ).transpose(1, 2)
         value_states = value_states.view(
             bsz, q_len, self.num_key_value_heads, self.head_dim
         ).transpose(1, 2)
 
-        kv_seq_len = key_states.shape[-2]
-        if past_key_value is not None:
-            kv_seq_len += past_key_value[0].shape[-2]
-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
+        past_key_value = getattr(self, "past_key_value", past_key_value)
+        cos, sin = self.rotary_emb(value_states, position_ids)
         query_states, key_states = apply_rotary_pos_emb(
-            query_states, key_states, cos, sin, position_ids
+            query_states, key_states, cos, sin
         )
 
         if past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
-
-        past_key_value = (key_states, value_states) if use_cache else None
+            # sin and cos are specific to RoPE models; cache_position needed for the static cache # noqa
+            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
+            key_states, value_states = past_key_value.update(
+                key_states, value_states, self.layer_idx, cache_kwargs
+            )
 
         key_states = repeat_kv(key_states, self.num_key_value_groups)
         value_states = repeat_kv(value_states, self.num_key_value_groups)
 
         # ==== SparseML MODIFICATION ====
         attn_weights = self.attn_weights_matmul(
             query_states, key_states.transpose(2, 3)
         ) / math.sqrt(self.head_dim)
         # ==============================
 
-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
-            raise ValueError(
-                f"Attention weights should be of size "
-                f"{(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
-                f" {attn_weights.size()}"
-            )
-
-        if attention_mask is not None:
-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
-                raise ValueError(
-                    f"Attention mask should be of size "
-                    f"{(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
-                )
-            attn_weights = attn_weights + attention_mask
+        if attention_mask is not None:  # no matter the length, we just slice it
+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
+            attn_weights = attn_weights + causal_mask
 
         # upcast attention to fp32
         attn_weights = nn.functional.softmax(
             attn_weights, dim=-1, dtype=torch.float32
         ).to(query_states.dtype)
+        attn_weights = nn.functional.dropout(
+            attn_weights, p=self.attention_dropout, training=self.training
+        )
         # ==== SparseML MODIFICATION ====
         attn_output = self.attn_output_matmul(attn_weights, value_states)
         # ===============================
 
         if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
             raise ValueError(
-                f"`attn_output` should be of size "
-                f"{(bsz, self.num_heads, q_len, self.head_dim)}, but is"
+                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"  # noqa
                 f" {attn_output.size()}"
             )
 
         attn_output = attn_output.transpose(1, 2).contiguous()
 
         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
```

## sparseml/transformers/sparsification/modification/modifying_mistral.py

```diff
@@ -14,21 +14,24 @@
 
 """
 Modification to the original Mistral model required in the
 context of SparseML
 """
 import logging
 import math
+import warnings
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
 from transformers.models.mistral.modeling_mistral import (
+    Cache,
     MistralAttention,
     MistralFlashAttention2,
+    MistralSdpaAttention,
     apply_rotary_pos_emb,
     repeat_kv,
 )
 
 from sparseml.pytorch.utils.helpers import swap_modules
 from sparseml.transformers.sparsification.modification.modification_objects import (
     QuantizableIdentity,
@@ -48,22 +51,23 @@
     Modify the Mistral model to be compatible with SparseML
 
     1. Replaces the MistralAttention modules with
         MistralAttentionWithQuantizableMatmuls modules
 
     Note: This function will not alter any of the alternatives
     to the MistralAttention module such as MistralFlashAttention2
+    or MistralSdpaAttention
 
     :param model: the original Mistral model
     :return: the modified Mistral model
     """
     for name, submodule in model.named_modules():
         if isinstance(submodule, MistralAttention):
             swap_modules(model, name, MistralAttentionWithQuantizableMatmuls(submodule))
-        if isinstance(submodule, MistralFlashAttention2):
+        if isinstance(submodule, (MistralSdpaAttention, MistralFlashAttention2)):
             _LOGGER.debug(
                 f"The model contains {submodule.__class__.__name__} "
                 "module, which will not be modified"
             )
     return model
 
 
@@ -108,19 +112,23 @@
         )
 
     def forward(
         self,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        past_key_value: Optional[Cache] = None,
         output_attentions: bool = False,
         use_cache: bool = False,
-        padding_mask: Optional[torch.Tensor] = None,
+        **kwargs,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        if "padding_mask" in kwargs:
+            warnings.warn(
+                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"  # noqa
+            )
         bsz, q_len, _ = hidden_states.size()
 
         query_states = self.q_proj(hidden_states)
         key_states = self.k_proj(hidden_states)
         value_states = self.v_proj(hidden_states)
 
         query_states = query_states.view(
@@ -131,65 +139,70 @@
         ).transpose(1, 2)
         value_states = value_states.view(
             bsz, q_len, self.num_key_value_heads, self.head_dim
         ).transpose(1, 2)
 
         kv_seq_len = key_states.shape[-2]
         if past_key_value is not None:
-            kv_seq_len += past_key_value[0].shape[-2]
+            if self.layer_idx is None:
+                raise ValueError(
+                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "  # noqa
+                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "  # noqa
+                    "with a layer index."
+                )
+            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
         cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
         query_states, key_states = apply_rotary_pos_emb(
             query_states, key_states, cos, sin, position_ids
         )
 
         if past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
-
-        past_key_value = (key_states, value_states) if use_cache else None
+            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
+            key_states, value_states = past_key_value.update(
+                key_states, value_states, self.layer_idx, cache_kwargs
+            )
 
         # repeat k/v heads if n_kv_heads < n_heads
         key_states = repeat_kv(key_states, self.num_key_value_groups)
         value_states = repeat_kv(value_states, self.num_key_value_groups)
 
         # ==== SparseML MODIFICATION ====
         attn_weights = self.attn_weights_matmul(
             query_states, key_states.transpose(2, 3)
         ) / math.sqrt(self.head_dim)
         # ===============================
 
         if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
             raise ValueError(
-                f"Attention weights should be of size "
-                f"{(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
+                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"  # noqa
                 f" {attn_weights.size()}"
             )
 
         if attention_mask is not None:
             if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                 raise ValueError(
-                    f"Attention mask should be of size "
-                    f"{(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
+                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"  # noqa
                 )
 
             attn_weights = attn_weights + attention_mask
 
         # upcast attention to fp32
         attn_weights = nn.functional.softmax(
             attn_weights, dim=-1, dtype=torch.float32
         ).to(query_states.dtype)
+        attn_weights = nn.functional.dropout(
+            attn_weights, p=self.attention_dropout, training=self.training
+        )
         # ==== SparseML MODIFICATION ====
         attn_output = self.attn_output_matmul(attn_weights, value_states)
         # ===============================
 
         if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
             raise ValueError(
-                f"`attn_output` should be of size "
-                f"{(bsz, self.num_heads, q_len, self.head_dim)}, but is"
+                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"  # noqa
                 f" {attn_output.size()}"
             )
 
         attn_output = attn_output.transpose(1, 2).contiguous()
         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
 
         attn_output = self.o_proj(attn_output)
```

## sparseml/transformers/sparsification/modification/modifying_opt.py

```diff
@@ -18,15 +18,15 @@
 """
 
 import logging
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
-from transformers.models.opt.modeling_opt import OPTAttention
+from transformers.models.opt.modeling_opt import OPTAttention, OptFlashAttention2
 
 from sparseml.pytorch.utils.helpers import swap_modules
 from sparseml.transformers.sparsification.modification.modification_objects import (
     QuantizableBatchMatmul,
     QuantizableIdentity,
 )
 from sparseml.transformers.sparsification.modification.registry import (
@@ -41,20 +41,28 @@
 def modify(model: nn.Module) -> nn.Module:
     """
     Modify the OPT model to be compatible with SparseML
 
     1. Replaces the OPTAttention modules with
         OPTAttentionWithQuantizableMatmuls modules
 
+    Note: This function will not alter any of the alternatives
+    to the OPTAttention module such as OptFlashAttention2
+
     :param model: the original LLaMa model
     :return: the modified LLaMa model
     """
     for name, submodule in model.named_modules():
         if isinstance(submodule, OPTAttention):
             swap_modules(model, name, OPTAttentionWithQuantizableMatmuls(submodule))
+        elif isinstance(submodule, OptFlashAttention2):
+            _LOGGER.debug(
+                f"The model contains {submodule.__class__.__name__} "
+                "module, which will not be modified"
+            )
     return model
 
 
 class BMMLeftInput_QK(QuantizableIdentity):
     ...
 
 
@@ -137,78 +145,68 @@
             value_states = torch.cat([past_key_value[1], value_states], dim=2)
         else:
             # self_attention
             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
 
         if self.is_decoder:
-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor)
-            # of all cross attention key/value_states.
-            # Further calls to cross_attention layer
-            # can then reuse all cross-attention
+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states. # noqa
+            # Further calls to cross_attention layer can then reuse all cross-attention
             # key/value_states (first "if" case)
-            # if uni-directional self-attention (decoder)
-            # save Tuple(torch.Tensor, torch.Tensor) of
-            # all previous decoder key/value_states.
-            # Further calls to uni-directional self-attention
-            # can concat previous decoder key/value_states
-            # to current projected key/value_states (third "elif" case)
-            # if encoder bi-directional self-attention
-            # `past_key_value` is always `None`
+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of # noqa
+            # all previous decoder key/value_states. Further calls to uni-directional self-attention # noqa
+            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case) # noqa
+            # if encoder bi-directional self-attention `past_key_value` is always `None`
             past_key_value = (key_states, value_states)
 
         proj_shape = (bsz * self.num_heads, -1, self.head_dim)
         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
         key_states = key_states.view(*proj_shape)
         value_states = value_states.view(*proj_shape)
 
         src_len = key_states.size(1)
         # ==== SparseML MODIFICATION ====
         attn_weights = self.attn_weights_bmm(query_states, key_states.transpose(1, 2))
         # ==============================
 
         if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
             raise ValueError(
-                f"Attention weights should be of size "
-                f"{(bsz * self.num_heads, tgt_len, src_len)}, but is"
+                f"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is"  # noqa
                 f" {attn_weights.size()}"
             )
 
         if attention_mask is not None:
             if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                 raise ValueError(
-                    f"Attention mask should be of size "
-                    f"{(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
+                    f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"  # noqa
                 )
             attn_weights = (
                 attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
                 + attention_mask
             )
             attn_weights = torch.max(
                 attn_weights,
                 torch.tensor(
                     torch.finfo(attn_weights.dtype).min, device=attn_weights.device
                 ),
             )
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        # upcast to fp32 if the weights are in fp16.
-        # Please see https://github.com/huggingface/transformers/pull/17437
+        # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437 # noqa
         if attn_weights.dtype == torch.float16:
             attn_weights = nn.functional.softmax(
                 attn_weights, dim=-1, dtype=torch.float32
             ).to(torch.float16)
         else:
             attn_weights = nn.functional.softmax(attn_weights, dim=-1)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
                 raise ValueError(
-                    f"Head mask for a single layer "
-                    f"should be of size {(self.num_heads,)}, but is"
+                    f"Head mask for a single layer should be of size {(self.num_heads,)}, but is"  # noqa
                     f" {layer_head_mask.size()}"
                 )
             attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(
                 bsz, self.num_heads, tgt_len, src_len
             )
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
@@ -232,23 +230,21 @@
 
         # ==== SparseML MODIFICATION ====
         attn_output = self.attn_output_bmm(attn_probs, value_states)
         # ==============================
 
         if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
             raise ValueError(
-                f"`attn_output` should be of size "
-                f"{(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"
+                f"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is"  # noqa
                 f" {attn_output.size()}"
             )
 
         attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
         attn_output = attn_output.transpose(1, 2)
 
-        # Use the `embed_dim` from the config (stored in the class)
-        # rather than `hidden_state` because `attn_output` can be
+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be # noqa
         # partitioned aross GPUs when using tensor-parallelism.
         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
 
         attn_output = self.out_proj(attn_output)
 
         return attn_output, attn_weights_reshaped, past_key_value
```

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/LICENSE` & `sparseml_nightly-1.8.0.20240404.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/LICENSE-ULTRALYTICS` & `sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/METADATA` & `sparseml_nightly-1.8.0.20240404.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: sparseml-nightly
-Version: 1.8.0.20240401
+Version: 1.8.0.20240404
 Summary: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models
 Home-page: https://github.com/neuralmagic/sparseml
 Author: Neuralmagic, Inc.
 Author-email: support@neuralmagic.com
 License: Apache
 Keywords: inference,machine learning,neural network,computer vision,nlp,cv,deep learning,torch,pytorch,tensorflow,keras,sparsity,pruning,deep learning libraries,onnx,quantization,automl
 Platform: UNKNOWN
@@ -77,16 +77,16 @@
 Requires-Dist: sphinx-multiversion ~=0.2.4 ; extra == 'docs'
 Requires-Dist: sphinx-pydantic ~=0.1.0 ; extra == 'docs'
 Requires-Dist: sphinx-rtd-theme ~=0.5.0 ; extra == 'docs'
 Requires-Dist: docutils <0.17 ; extra == 'docs'
 Provides-Extra: llm
 Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'llm'
 Requires-Dist: gputils ; extra == 'llm'
-Requires-Dist: transformers <4.35.0 ; extra == 'llm'
-Requires-Dist: datasets <=2.14.6 ; extra == 'llm'
+Requires-Dist: transformers <4.40 ; extra == 'llm'
+Requires-Dist: datasets <2.19 ; extra == 'llm'
 Requires-Dist: dvc ; extra == 'llm'
 Requires-Dist: scikit-learn ; extra == 'llm'
 Requires-Dist: seqeval ; extra == 'llm'
 Requires-Dist: einops ; extra == 'llm'
 Requires-Dist: evaluate >=0.4.1 ; extra == 'llm'
 Requires-Dist: accelerate >=0.20.3 ; extra == 'llm'
 Requires-Dist: safetensors >=0.4.1 ; extra == 'llm'
@@ -121,16 +121,16 @@
 Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'torchvision'
 Requires-Dist: gputils ; extra == 'torchvision'
 Requires-Dist: torchvision <0.17,>=0.3.0 ; extra == 'torchvision'
 Requires-Dist: opencv-python <=4.6.0.66 ; extra == 'torchvision'
 Provides-Extra: transformers
 Requires-Dist: torch <2.2,>=1.7.0 ; extra == 'transformers'
 Requires-Dist: gputils ; extra == 'transformers'
-Requires-Dist: transformers <4.35.0 ; extra == 'transformers'
-Requires-Dist: datasets <=2.14.6 ; extra == 'transformers'
+Requires-Dist: transformers <4.40 ; extra == 'transformers'
+Requires-Dist: datasets <2.19 ; extra == 'transformers'
 Requires-Dist: dvc ; extra == 'transformers'
 Requires-Dist: scikit-learn ; extra == 'transformers'
 Requires-Dist: seqeval ; extra == 'transformers'
 Requires-Dist: einops ; extra == 'transformers'
 Requires-Dist: evaluate >=0.4.1 ; extra == 'transformers'
 Requires-Dist: accelerate >=0.20.3 ; extra == 'transformers'
 Requires-Dist: safetensors >=0.4.1 ; extra == 'transformers'
```

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/NOTICE` & `sparseml_nightly-1.8.0.20240404.dist-info/NOTICE`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/entry_points.txt` & `sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt`

 * *Files identical despite different names*

## Comparing `sparseml_nightly-1.8.0.20240401.dist-info/RECORD` & `sparseml_nightly-1.8.0.20240404.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -60,15 +60,15 @@
 sparseml/evaluation/integrations/lm_evaluation_harness.py,sha256=2adnmKtGMzowJUveeWbaaZUjmf6YO2a8sHPVFiGcwNQ,6000
 sparseml/evaluation/integrations/perplexity.py,sha256=wBdnkBYv8bhFqt2PA3wVr3nN0GATRkn3qWgdgHKXH88,10987
 sparseml/export/__init__.py,sha256=5CrKZql6PYJq3BpNLBG397pVG7rVxU4-NIgpQeKY188,661
 sparseml/export/export.py,sha256=Z-v8Lyz_RsB74gmU9s79m33vAv3nPoT_uIRexvJRaIA,21172
 sparseml/export/export_data.py,sha256=5t9jEycKHOdcBVdHkA6ktRZxQ5luyqNRALPlSvmkdKE,8049
 sparseml/export/export_torch_model.py,sha256=SiA69b7p0KWhfM2XtYbKaYW6YHIvIdFHL_tWJdGBX58,2681
 sparseml/export/helpers.py,sha256=maoxhetc4Fu94XevWFyZ6-BQErjSZOHZie_rwKyWaFE,13052
-sparseml/export/validators.py,sha256=KZhGKo-py1aE5i6Z9iDNnxkKLQHd1bW74we1hMt0NYE,8850
+sparseml/export/validators.py,sha256=wBxARY9fXsszUGApMY9Fxf5ZjYUGh_Tg1I-C_G_l488,8868
 sparseml/exporters/__init__.py,sha256=VPcyVcyPnl7Kp05h3ws-UC2jgKR6W-cr11d-m665w9A,786
 sparseml/exporters/base_exporter.py,sha256=M1HEe9GNf51uYx1z-qxWjIYo5iGZ_Ish8uZ3mj6OZR8,1477
 sparseml/exporters/kv_cache_injector.py,sha256=2fKoRPBE7yrR6h3KXxeU7EbsU5ufyZUIbSnRP2FTjwc,6576
 sparseml/exporters/onnx_to_deepsparse.py,sha256=e3RNDtyKAbmYTF-AHV00FAg-ilQDidUPTnUU9oQm4Rg,5522
 sparseml/exporters/transforms/__init__.py,sha256=0SMdBqAFpcoDTWDklE2nSk9G4sZOAUyUfeWUJSlRAD0,2350
 sparseml/exporters/transforms/base_transform.py,sha256=IpvdUdEADl2SfqHawcp6-lcNgz-R3L0zpS05WaWF5h8,2333
 sparseml/exporters/transforms/constants_to_initializers.py,sha256=_PX3y16OC-q85d-6DeOOVpKAAMNIU1m5BdYTZlFV_tk,1388
@@ -194,15 +194,15 @@
 sparseml/modifiers/quantization/base.py,sha256=zsxVrP_mLh895z1g56o1xtNgUAMVgqMGKmUk0GBnmYg,5512
 sparseml/modifiers/quantization/pytorch.py,sha256=Gzy_AemrYNkbr29RLo4MuqVcLc_heF8bdQlrkvCRxDA,8924
 sparseml/modifiers/quantization/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/quantization/utils/constants.py,sha256=sla-j0QwLFq92AbDZkHkNLxugpGrNEQJIuLWoWqV3Ks,2220
 sparseml/modifiers/quantization/utils/fake_quant_wrapper.py,sha256=EZC7DFVz_Z4CVtVfeSpkaE9-g3OIN8WDZAaW6V7foVU,2906
 sparseml/modifiers/quantization/utils/helpers.py,sha256=7U5uorqKSWLExvkVOyGU8b3noKlcG6tGqhjsomYihrg,32720
 sparseml/modifiers/quantization/utils/quantization_scheme.py,sha256=VvDo8KBtc6uTWnKDM9izgEcWtTdXGxkR_E-zKQ2aTO0,13545
-sparseml/modifiers/quantization/utils/quantize.py,sha256=-t4-i1DDQDvKMLMFk6qNQzlYf0jz98sK7hti6zKz5dw,19811
+sparseml/modifiers/quantization/utils/quantize.py,sha256=AeGY6YBREVkyRT4QDJ4KQswcavPlemYNcQmC6mhQPGs,20463
 sparseml/modifiers/smoothquant/__init__.py,sha256=QQWiZMR2IxBVSYmsAkop1F_xCGhHzMlukySMVz4lv2I,654
 sparseml/modifiers/smoothquant/base.py,sha256=_KxGDJh54i_0Ltrw6NnN0FaRPWWbxjiK-3yzZHkH5-E,7535
 sparseml/modifiers/smoothquant/pytorch.py,sha256=bv9FmqGfiafK1TOh-MM66eBUXVHgdc5ECDye5PUwtQk,8167
 sparseml/modifiers/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/modifiers/utils/compression_wrapper.py,sha256=qjq6mycdiMvWUktylSr4gWplzHxQEctkwnP1XORUSvE,3944
 sparseml/modifiers/utils/layer_compressor.py,sha256=PgICAZVJ3qFixoA-TD6eykBByF_d4O3LF0WukdvHVyk,5257
 sparseml/modifiers/utils/pytorch_helpers.py,sha256=HjpIZmD8KnyLh8opb6YxpCbZ8JZTDlkONHloYxsKC_4,3265
@@ -450,32 +450,32 @@
 sparseml/transformers/masked_language_modeling.py,sha256=VxuHhEPXUu2nuv73eTUYbtVSnK101NrtYpwOQaS8QCA,30795
 sparseml/transformers/question_answering.py,sha256=b5fMA_dMKsxl-BEI2yIkn1hjJ2kz8VfYwZBHInnD8pQ,37002
 sparseml/transformers/text_classification.py,sha256=QfpUTL7nfPNcMYVxNhySFjLd4BKBhGqSR-YS83oQEOk,40360
 sparseml/transformers/text_generation.py,sha256=RDCzUC4tjfgkyzAOdkYvie563fez1dYivD_U4cmpWu8,818
 sparseml/transformers/token_classification.py,sha256=Opb0gL8Das6sDsxdDd9DirFUWVg7IYHpMGImL_yfU-Q,34389
 sparseml/transformers/compression/__init__.py,sha256=e-naxCnurIkngVTQ3eIU13CqTg0N0jQ_Xm0_3wZYtgo,683
 sparseml/transformers/compression/compressors/__init__.py,sha256=fXlgLEeFgLXdc7ofYWIJ07VOUUoD5pcPiSAKndHsAqg,749
-sparseml/transformers/compression/compressors/base.py,sha256=GNFr2mfDVrDT5kvJzhtraizzo2Ad6ReSmtWTlvmiWG0,3136
+sparseml/transformers/compression/compressors/base.py,sha256=KWuHhgpEX4FzvensMYAK5DnAf3hNaoJejdiIn4tPyik,3052
 sparseml/transformers/compression/compressors/dense.py,sha256=akD31OMTJfee2QkGS4ZlJn1LUuKn5XnJSuyyPmRbDMQ,1160
-sparseml/transformers/compression/compressors/sparse_bitmask.py,sha256=mDRmFX4lDQnazphaj68ZEU1sgIhA8ILNS90bbvn6X5Y,8377
+sparseml/transformers/compression/compressors/sparse_bitmask.py,sha256=soSzKXVc_GujyAOKWi-rS24-hPFXnMB3dwrNEEPBYUI,8382
 sparseml/transformers/compression/config/__init__.py,sha256=fV4XY44QQ5krlLZhf11OIg7Q91vDcTW5blRITa7LL4c,751
 sparseml/transformers/compression/config/base.py,sha256=Ova_943QUCtB7RcCjPRIQvcdCmSWd5TS7X9PLMRkeJQ,3846
 sparseml/transformers/compression/config/dense.py,sha256=j5Tp-xGtqMK6TtModLIA4h5nc1BkKW6VWM2TXKtkDRQ,1263
 sparseml/transformers/compression/config/sparse_bitmask.py,sha256=Q6lcE-7xlc-KhaldDWfG7foHwlCcW3gZqQ4KnK2Gx4s,1236
 sparseml/transformers/compression/utils/__init__.py,sha256=VJzYDHjYGHcqVRrO06Pgstju2Df4rhVhQt3W-LIdIbk,718
 sparseml/transformers/compression/utils/compress_save.py,sha256=J5CbPJH3LZdQAW2_1xm_tr4yQqQwIxVs78WhV10GXfY,6092
 sparseml/transformers/compression/utils/helpers.py,sha256=VngJebXKN0tyxLlzl5--DhwOLUJ8BxI55MwXHdaBPos,1778
-sparseml/transformers/compression/utils/safetensors_load.py,sha256=VYNcQ8S2BMgJFlOpWndOpO60zE8fYF6QJV4gcBUlloQ,5276
+sparseml/transformers/compression/utils/safetensors_load.py,sha256=k4BhpnRE_3ANo3GQgycLFu7ernYSZMM1sF8H3irncaI,6966
 sparseml/transformers/finetune/__init__.py,sha256=0bOoxD9cLg2VVtreGrcrakhzdMeUmFeTWFhm3oZkRp8,701
-sparseml/transformers/finetune/callbacks.py,sha256=j7k7LDuMmOZcn0PKxbNdcho2nAGq4lMMvRNo21DDS98,5251
+sparseml/transformers/finetune/callbacks.py,sha256=14k2PnjA85cRwzzuIqF1yfxcKrQ4t9RUKckFdeK3-IU,4265
 sparseml/transformers/finetune/model_args.py,sha256=RkppN_0YiHjK1PUCAuKZsGvIs52FHLAc8pJNqymbHPE,2519
 sparseml/transformers/finetune/runner.py,sha256=aQxCzxEBRAe8Z_YAIY_Tz491hvQdn2r7VN-z2F3ZVBA,11988
-sparseml/transformers/finetune/session_mixin.py,sha256=1PL-z7vx43prOiqnRKkdR6mKLrwp0mv4eLcsVIpqF-c,22944
+sparseml/transformers/finetune/session_mixin.py,sha256=aMV5pL-mnoDaaqWIQvBWT0ZqCKHgmcDWAqww48Zkmnw,22620
 sparseml/transformers/finetune/text_generation.py,sha256=lvCIcuV1N9GVfYYjXe3BSjw3JE6LAKejpKrWy0zNho0,13101
-sparseml/transformers/finetune/trainer.py,sha256=xqbz1ClwV3Fe9dw4batuLMp1-KiXBT-qqP_Y93YlgI0,4290
+sparseml/transformers/finetune/trainer.py,sha256=jRbQ-Je-V2Ym2z30_K6pgsuznZB_W79blXF7R3x5m60,4126
 sparseml/transformers/finetune/training_args.py,sha256=BLe9Wjcf-HVpds_1XS8NPHZzm10WQ3_Ii-EjS0ODc2U,3005
 sparseml/transformers/finetune/data/__init__.py,sha256=1suS2CfhJGBW08wniPgbFuOLG4g71rBu7f15OrI7ZrQ,1021
 sparseml/transformers/finetune/data/base.py,sha256=GwABRyBbLgooASRO-XpNvrj-_omS_oVAvBdI_59iSFg,8736
 sparseml/transformers/finetune/data/c4.py,sha256=tJW0Dnf5rs6uFmgoRD0TGuf9Sl0qLDkiVdMAtwhceSo,1322
 sparseml/transformers/finetune/data/cnn_dailymail.py,sha256=EpASl2Rd2L2IURBm8ALVkUbE-RAxzoW0LFmOsRxJs10,2537
 sparseml/transformers/finetune/data/custom.py,sha256=WqKecNf45cS1pGe8T9M_AccvSFAFO5YNJEUoyHA9tbY,4281
 sparseml/transformers/finetune/data/data_args.py,sha256=MEwc-aPL8TxZsEUtRuZVDN1OEg8YD4RhZ1nqtX-Th58,5399
@@ -483,30 +483,30 @@
 sparseml/transformers/finetune/data/evolcodealpaca.py,sha256=-P8hAodeEQ3QDGw17UXf7igRHlCU2e74ntjzExXtZ4I,2892
 sparseml/transformers/finetune/data/gsm8k.py,sha256=jHAwAUScU9a4jsnl0edtmvhSUVLlxFy07fczMjN90gI,2597
 sparseml/transformers/finetune/data/open_platypus.py,sha256=ZJEPEiiJtKpxKm5rOTUHdANnTk0C24mGL3lpghSpj8w,3435
 sparseml/transformers/finetune/data/ptb.py,sha256=mr572aTmdSK7AyY3btPE0eQh27Xcx-dnWY76jcdtMFg,1369
 sparseml/transformers/finetune/data/ultrachat_200k.py,sha256=lRNNO7JV0SIxp1SMp2RG25R4jBpD2_LopsjqCc1KhB8,3521
 sparseml/transformers/finetune/data/wikitext.py,sha256=jHCYaAqMx2vgInMV2-Hg7wXh-TU4TZ691rAkVkP9UnM,1237
 sparseml/transformers/sparsification/__init__.py,sha256=f1YW4ztJxM1OlRPimYAW4nIabqDUSaUArM-VuFbJtSA,922
-sparseml/transformers/sparsification/question_answering.py,sha256=FAXhtQ8MW_2ar1dix2tQ1x1RXZuSWMLHZbVHToNWwmo,19529
+sparseml/transformers/sparsification/question_answering.py,sha256=YENvQh5ZVine9K7cEts2N_N7oUYTdYeF0Gp9LFzBRII,19272
 sparseml/transformers/sparsification/sparse_config.py,sha256=1zAwQcO2yQB-b0kecaAXECJMjlwIQPEKnui90Wvj7Dw,1874
-sparseml/transformers/sparsification/sparse_model.py,sha256=4SI32FTT7S49vIMwMYWCTIFrBhg4fRX0TOIKfcbmv1s,20054
+sparseml/transformers/sparsification/sparse_model.py,sha256=z17-AfeLo1fkzCvePtYjd3LVK_EFtBIoKY5TqjlHPFk,20264
 sparseml/transformers/sparsification/sparse_tokenizer.py,sha256=kW_7pQ4awjsGMx3pzPvOS89n6T3ehBes1geop02XWKY,2327
-sparseml/transformers/sparsification/trainer.py,sha256=ssoMIFie31582qtzB-NGXS3abdF89rM7S_9xMN0amqY,41727
+sparseml/transformers/sparsification/trainer.py,sha256=5MdTsqPTE-ZmqYr9e4Z0wucQLK8u7g-ahMlilZQX-tk,40305
 sparseml/transformers/sparsification/training_args.py,sha256=gGivIDchLi__PZMNQRmzDOaQcQLkUVFystq3LaVko3Y,1890
 sparseml/transformers/sparsification/modification/__init__.py,sha256=LpHfgSjpeYu_-E54ddk0bOd3X_uuUMcQQvKBi4GfXtU,866
-sparseml/transformers/sparsification/modification/base.py,sha256=yKqZLDDoS-0z18WhNJsufxS9gzdTQnnjsPNx2K2T044,2429
+sparseml/transformers/sparsification/modification/base.py,sha256=tSMEYZzwsjLiuU5gi4egMSJVmwwtwIf8OjapiTNoejw,2429
 sparseml/transformers/sparsification/modification/modification_objects.py,sha256=hZzGEv1pg7z8CKpdQpBpnB-0cBVuwJM2c8SxYODAL5E,3922
 sparseml/transformers/sparsification/modification/modify_model.py,sha256=RI2DrXwX0MfCQmWQkUUwJV0beemUa7hZGq7wNp5Q6MQ,2434
-sparseml/transformers/sparsification/modification/modifying_bert.py,sha256=kzZpKS_PHW_TPcIwl95Od0N45qVeL-RqynbT7h9YgJQ,9199
-sparseml/transformers/sparsification/modification/modifying_distilbert.py,sha256=3tu06QFnJLJaAHb3zFW2LjVWxzqfpZ6Zek8hFDyCqM4,5492
-sparseml/transformers/sparsification/modification/modifying_llama.py,sha256=ID2z-ztbQnhsEZ8G6McYYQ3eLq-TiK5ltoMZj6aurWI,8703
-sparseml/transformers/sparsification/modification/modifying_mistral.py,sha256=5df_N0uBvFwC7y7VB7R3na8Dv--kIg2dYUDdhcInDPw,6945
+sparseml/transformers/sparsification/modification/modifying_bert.py,sha256=EpgzqGReQFmV1K02yPRLw8vqIll41QbqROJkTjffZ5E,9148
+sparseml/transformers/sparsification/modification/modifying_distilbert.py,sha256=L6XEQu_d4NFw5RD4s05GrBJc3tcjSnVuUur8Z4kgWP0,5883
+sparseml/transformers/sparsification/modification/modifying_llama.py,sha256=Qu-GDKn5TgGAJPcgVUwVQw1kS3kImn6pBEfR19fU2R4,8457
+sparseml/transformers/sparsification/modification/modifying_mistral.py,sha256=lfNzP0SA6a2MvoHIenjUb9RMckkkyIMm6bMsLAk0dlA,7689
 sparseml/transformers/sparsification/modification/modifying_mobilebert.py,sha256=ZCUhkLeX36sQ75yRuxXETUlwG4VoChnoEljxiznCQHs,2549
-sparseml/transformers/sparsification/modification/modifying_opt.py,sha256=sBeWxuxXVgcHoVk1hU4K9VObNqdAKX-z15JyzRwV2QI,9618
+sparseml/transformers/sparsification/modification/modifying_opt.py,sha256=VPe9yCPySaVZZHuMjGyHxzCqasB6GeAZg391NIkYtrk,9862
 sparseml/transformers/sparsification/modification/registry.py,sha256=U4AeTD4xJHt3pnn6iXskq3zP1GY1d26h9owV4r3OdzE,1486
 sparseml/transformers/sparsification/obcq/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/transformers/sparsification/obcq/export.py,sha256=6gJHqz0J4tNanmEC0fi1xn6jQ4Y0WEqzJg1nHfH4PXM,19683
 sparseml/transformers/sparsification/obcq/obcq.py,sha256=QLN1ywZCCMF5GH-Z0-ETf4EpsdfR589h8ws-_-EJL2A,7695
 sparseml/transformers/sparsification/obcq/utils/__init__.py,sha256=fH6rjBYAxuwrTzBTlTjTgCYNyh6TCvCqajCz4Im4YrA,617
 sparseml/transformers/sparsification/obcq/utils/helpers.py,sha256=btmRVzMnGC4rxqeFRaZxVfewVhoerib6xA62cBuyFfA,3671
 sparseml/transformers/utils/__init__.py,sha256=NVWYxKuoVwMglNalk_zs0PiDi6ec4JHG-DsuCNEDkBY,805
@@ -553,15 +553,15 @@
 sparseml/yolov8/train.py,sha256=eetdfCbRQJdBWc7XHb9GYPqMQxZzbKG7FG-4xQeTutk,7394
 sparseml/yolov8/trainers.py,sha256=uQsIX5Zyv6mkK8NLJqmM5kukHojZTZmVEKTKm1pkgxU,37860
 sparseml/yolov8/val.py,sha256=hlFImvknSpV1nONOxA3ivYgvzm64EmK0_Lh-JHLbOsw,2748
 sparseml/yolov8/validators.py,sha256=fkEnjRp1day-KY0n7DZ_VI-zmNqKbalh-g6U-lwHmfQ,8459
 sparseml/yolov8/utils/__init__.py,sha256=6JekgnibQP-8p8Dm1dGiIEGCGdBOAkSOnEds0BMSYhQ,685
 sparseml/yolov8/utils/export_samples.py,sha256=HFmQsXpmXZlxLTNBvHwp-lcM5mVcZN1ouBfAXTdYFeM,6683
 sparseml/yolov8/utils/helpers.py,sha256=8JZNaTT1zPKiZaOccmMtQu5mXCSMGkq8BDEgUqaPVIs,4041
-sparseml_nightly-1.8.0.20240401.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-sparseml_nightly-1.8.0.20240401.dist-info/LICENSE-ULTRALYTICS,sha256=JtH3RHvJDRVTEhj6JJZJH5MQFk5IkSg_TxYAqtPEOOs,13747
-sparseml_nightly-1.8.0.20240401.dist-info/METADATA,sha256=T6Y6oxAQD72KB5GzHFP2Pz8XVMeq_HsPSDMzTZX187Y,23637
-sparseml_nightly-1.8.0.20240401.dist-info/NOTICE,sha256=jkdOJGVbNDogFmOaXgc_Qr6U35t67RqzUJgf_Yc8C5w,2085
-sparseml_nightly-1.8.0.20240401.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-sparseml_nightly-1.8.0.20240401.dist-info/entry_points.txt,sha256=AOCpCae2aLRUTce3NNDHYPZevTpJ-GTOQuzy8YPafmc,3122
-sparseml_nightly-1.8.0.20240401.dist-info/top_level.txt,sha256=JOOlWKgkyuJBScnty7pC1SQ58fOo1ONbslvMdxB6L2M,9
-sparseml_nightly-1.8.0.20240401.dist-info/RECORD,,
+sparseml_nightly-1.8.0.20240404.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+sparseml_nightly-1.8.0.20240404.dist-info/LICENSE-ULTRALYTICS,sha256=JtH3RHvJDRVTEhj6JJZJH5MQFk5IkSg_TxYAqtPEOOs,13747
+sparseml_nightly-1.8.0.20240404.dist-info/METADATA,sha256=Cz0r_itIm479UISFLO3GHvjvZDd2Si3p0iwaoB_joDM,23627
+sparseml_nightly-1.8.0.20240404.dist-info/NOTICE,sha256=jkdOJGVbNDogFmOaXgc_Qr6U35t67RqzUJgf_Yc8C5w,2085
+sparseml_nightly-1.8.0.20240404.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
+sparseml_nightly-1.8.0.20240404.dist-info/entry_points.txt,sha256=AOCpCae2aLRUTce3NNDHYPZevTpJ-GTOQuzy8YPafmc,3122
+sparseml_nightly-1.8.0.20240404.dist-info/top_level.txt,sha256=JOOlWKgkyuJBScnty7pC1SQ58fOo1ONbslvMdxB6L2M,9
+sparseml_nightly-1.8.0.20240404.dist-info/RECORD,,
```

