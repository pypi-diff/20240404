# Comparing `tmp/datarobot_model_metrics-0.5.0-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.5.4-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 55792 bytes, number of entries: 32
--rw-r--r--  2.0 unx      637 b- defN 24-Mar-19 11:09 dmm/__init__.py
--rw-r--r--  2.0 unx    10067 b- defN 24-Mar-19 11:09 dmm/batch_metric_evaluator.py
--rw-r--r--  2.0 unx     2083 b- defN 24-Mar-19 11:09 dmm/constants.py
--rw-r--r--  2.0 unx     7089 b- defN 24-Mar-19 11:09 dmm/custom_metric.py
--rw-r--r--  2.0 unx    16633 b- defN 24-Mar-19 11:09 dmm/datarobot_api_client.py
--rw-r--r--  2.0 unx     7207 b- defN 24-Mar-19 11:09 dmm/example_data_helper.py
--rw-r--r--  2.0 unx      545 b- defN 24-Mar-19 11:09 dmm/exceptions.py
--rw-r--r--  2.0 unx    18029 b- defN 24-Mar-19 11:09 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3680 b- defN 24-Mar-19 11:09 dmm/time_bucket.py
--rw-r--r--  2.0 unx     4016 b- defN 24-Mar-19 11:09 dmm/utils.py
--rw-r--r--  2.0 unx      980 b- defN 24-Mar-19 11:09 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2348 b- defN 24-Mar-19 11:09 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     3163 b- defN 24-Mar-19 11:09 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    59608 b- defN 24-Mar-19 11:09 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3255 b- defN 24-Mar-19 11:09 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx     1510 b- defN 24-Mar-19 11:09 dmm/data_source/runtime_parameters_source.py
--rw-r--r--  2.0 unx      252 b- defN 24-Mar-19 11:09 dmm/data_source/datarobot/__init__.py
--rw-r--r--  2.0 unx     5434 b- defN 24-Mar-19 11:09 dmm/data_source/datarobot/deployment.py
--rw-r--r--  2.0 unx     5495 b- defN 24-Mar-19 11:09 dmm/data_source/datarobot/export_provider.py
--rw-r--r--  2.0 unx      428 b- defN 24-Mar-19 11:09 dmm/dr_custom_metrics/__init__.py
--rw-r--r--  2.0 unx     1667 b- defN 24-Mar-19 11:09 dmm/dr_custom_metrics/deployment_event_reporter.py
--rw-r--r--  2.0 unx     8590 b- defN 24-Mar-19 11:09 dmm/dr_custom_metrics/dr_custom_metric.py
--rw-r--r--  2.0 unx      711 b- defN 24-Mar-19 11:09 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1872 b- defN 24-Mar-19 11:09 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      572 b- defN 24-Mar-19 11:09 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     4424 b- defN 24-Mar-19 11:09 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx     1209 b- defN 24-Mar-19 11:09 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2302 b- defN 24-Mar-19 11:09 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx    40827 b- defN 24-Mar-19 11:11 datarobot_model_metrics-0.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 24-Mar-19 11:11 datarobot_model_metrics-0.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 24-Mar-19 11:11 datarobot_model_metrics-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2782 b- defN 24-Mar-19 11:11 datarobot_model_metrics-0.5.0.dist-info/RECORD
-32 files, 217529 bytes uncompressed, 51302 bytes compressed:  76.4%
+Zip file size: 56045 bytes, number of entries: 32
+-rw-r--r--  2.0 unx      637 b- defN 24-Apr-04 06:39 dmm/__init__.py
+-rw-r--r--  2.0 unx    10115 b- defN 24-Apr-04 06:39 dmm/batch_metric_evaluator.py
+-rw-r--r--  2.0 unx     2123 b- defN 24-Apr-04 06:39 dmm/constants.py
+-rw-r--r--  2.0 unx     8062 b- defN 24-Apr-04 06:39 dmm/custom_metric.py
+-rw-r--r--  2.0 unx    16633 b- defN 24-Apr-04 06:39 dmm/datarobot_api_client.py
+-rw-r--r--  2.0 unx     7206 b- defN 24-Apr-04 06:52 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx      545 b- defN 24-Apr-04 06:39 dmm/exceptions.py
+-rw-r--r--  2.0 unx    18191 b- defN 24-Apr-04 06:39 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3680 b- defN 24-Apr-04 06:39 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     4016 b- defN 24-Apr-04 06:39 dmm/utils.py
+-rw-r--r--  2.0 unx      980 b- defN 24-Apr-04 06:39 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2348 b- defN 24-Apr-04 06:39 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     3161 b- defN 24-Apr-04 06:52 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    59608 b- defN 24-Apr-04 06:39 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3255 b- defN 24-Apr-04 06:39 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx     1515 b- defN 24-Apr-04 06:39 dmm/data_source/runtime_parameters_source.py
+-rw-r--r--  2.0 unx      252 b- defN 24-Apr-04 06:39 dmm/data_source/datarobot/__init__.py
+-rw-r--r--  2.0 unx     5434 b- defN 24-Apr-04 06:39 dmm/data_source/datarobot/deployment.py
+-rw-r--r--  2.0 unx     5495 b- defN 24-Apr-04 06:39 dmm/data_source/datarobot/export_provider.py
+-rw-r--r--  2.0 unx      428 b- defN 24-Apr-04 06:39 dmm/dr_custom_metrics/__init__.py
+-rw-r--r--  2.0 unx     1667 b- defN 24-Apr-04 06:39 dmm/dr_custom_metrics/deployment_event_reporter.py
+-rw-r--r--  2.0 unx     8764 b- defN 24-Apr-04 06:39 dmm/dr_custom_metrics/dr_custom_metric.py
+-rw-r--r--  2.0 unx      711 b- defN 24-Apr-04 06:39 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1872 b- defN 24-Apr-04 06:39 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      572 b- defN 24-Apr-04 06:39 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     4424 b- defN 24-Apr-04 06:39 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx     1209 b- defN 24-Apr-04 06:39 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2302 b- defN 24-Apr-04 06:39 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx    40827 b- defN 24-Apr-04 06:54 datarobot_model_metrics-0.5.4.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 24-Apr-04 06:54 datarobot_model_metrics-0.5.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 24-Apr-04 06:54 datarobot_model_metrics-0.5.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2782 b- defN 24-Apr-04 06:54 datarobot_model_metrics-0.5.4.dist-info/RECORD
+32 files, 218928 bytes uncompressed, 51555 bytes compressed:  76.5%
```

## zipnote {}

```diff
@@ -78,20 +78,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.5.0.dist-info/METADATA
+Filename: datarobot_model_metrics-0.5.4.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.5.0.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.5.4.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.5.0.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.5.4.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.5.0.dist-info/RECORD
+Filename: datarobot_model_metrics-0.5.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/batch_metric_evaluator.py

```diff
@@ -81,17 +81,19 @@
 
         for me_data in self._metrics:
             metric_name = me_data.metric_name
             metric = me_data.metric
             scored_values[metric_name] = metric.score(
                 predictions=predictions if metric.need_predictions() else None,
                 actuals=None,
-                scoring_data=self._get_scoring_data(chunk_of_data)
-                if metric.need_scoring_data()
-                else None,
+                scoring_data=(
+                    self._get_scoring_data(chunk_of_data)
+                    if metric.need_scoring_data()
+                    else None
+                ),
             )
             self._stats.nr_calls_to_score += 1
         return scored_values
 
     def _get_data_chunk(
         self,
     ) -> Tuple[Union[pd.DataFrame, None], int, bool]:
```

## dmm/constants.py

```diff
@@ -17,14 +17,15 @@
     ACTUALS = "actuals"
     TIMESTAMP = "timestamp"
     NR_SAMPLES = "samples"
     METRIC_VALUE = "value"
     DR_TIMESTAMP_COLUMN = "DR_RESERVED_PREDICTION_TIMESTAMP"
     DR_PREDICTION_COLUMN = "DR_RESERVED_PREDICTION_VALUE"
     DR_BATCH_ID_COLUMN = "DR_RESERVED_BATCH_ID"
+    DMM_RESERVED_TS = "DMM_RESERVED_TS"
     ASSOCIATION_ID_COLUMN = "association_id"
     INTERNAL_ASSOCIATION_ID_COLUMN_NAME = "__DataRobot_Internal_Association_ID__"
     BATCH_ID_COLUMN = "batch_id"
     LABEL = "label"
     PREDICTED_CLASS = "predicted_class"
     PROMPT = "prompt"
     PROMPT_TEXT = "promptText"
```

## dmm/custom_metric.py

```diff
@@ -154,14 +154,44 @@
             custom_metric_id=self.custom_metric_id,
             model_id=self.model_id,
             buckets=buckets,
             dry_run=dry_run,
         )
         return response
 
+    def report_single_value(
+        self,
+        value: float | int,
+        timestamp: datetime | str = None,
+        association_id: str = None,
+        dry_run: bool = False,
+    ) -> requests.Response:
+        """Report a single custom metric value. If no timestamp is specified, use the current UTC timestamp"""
+        if not timestamp:
+            ts = datetime.utcnow()
+        else:
+            ts = timestamp if isinstance(timestamp, datetime) else parse(timestamp)
+
+        bucket = {
+            "timestamp": ts.isoformat(),
+            "value": value,
+            "sampleSize": 1,
+        }
+        if association_id:
+            bucket["associationId"] = association_id
+
+        response = self._api.submit_custom_metric_values(
+            deployment_id=self.deployment_id,
+            custom_metric_id=self.custom_metric_id,
+            model_id=self.model_id,
+            buckets=[bucket],
+            dry_run=dry_run,
+        )
+        return response
+
 
 class CustomLLMMetric:
     """Used for custom metrics that are generated for a single prompt-response pair from
     the LLM Playground.  Custom LLM metrics that are associated with a deployed LLM will
     use CustomMetric to report results."""
 
     def __init__(
```

## dmm/example_data_helper.py

```diff
@@ -41,15 +41,14 @@
 def gen_timestamps_list(
     nr_rows: int, time_bucket: TimeBucket, rows_per_time_bucket: int
 ) -> List[Tuple[str]]:
     timedelta_obj = timedelta_for_bucket(time_bucket)
     datetime_object = datetime.strptime("Jun 1 2005  1:00:00PM", "%b %d %Y %I:%M:%S%p")
     timestamps = []
     for row in range(nr_rows):
-
         if row % rows_per_time_bucket == 0 and row != 0:
             datetime_object += timedelta_obj
         timestamp_str = (datetime_object.strftime("%d/%m/%Y %H:%M:%S.%f"),)
         timestamps.append(timestamp_str)
     return timestamps
```

## dmm/metric_evaluator.py

```diff
@@ -226,23 +226,27 @@
             else:
                 metric_data = chunk_of_data
 
             if metric_data.empty:
                 continue
 
             scored_values[metric_name] = metric.score(
-                predictions=self._get_predictions(metric_data)
-                if metric.need_predictions()
-                else None,
-                actuals=self._get_actuals(metric_data)
-                if metric.need_actuals()
-                else None,
-                scoring_data=self._get_scoring_data(metric_data)
-                if metric.need_scoring_data()
-                else None,
+                predictions=(
+                    self._get_predictions(metric_data)
+                    if metric.need_predictions()
+                    else None
+                ),
+                actuals=(
+                    self._get_actuals(metric_data) if metric.need_actuals() else None
+                ),
+                scoring_data=(
+                    self._get_scoring_data(metric_data)
+                    if metric.need_scoring_data()
+                    else None
+                ),
             )
             self._stats.nr_calls_to_score += 1
         return scored_values
 
     def _get_data_chunk(self) -> Tuple[Union[pd.DataFrame, None], int, bool]:
         done = False
         chunk_of_data, time_bucket_id = self._data_source.get_data()
@@ -337,19 +341,20 @@
         Extract only data used in scoring
         """
         _chunk_of_data = chunk_of_data.copy()
         _chunk_of_data = _chunk_of_data.drop(
             columns=[
                 self._actuals_col,
                 self._prediction_col,
-                self._timestamp_col,
-                ColumnName.ASSOCIATION_ID_COLUMN,
             ],
             errors="ignore",
         )
+        _chunk_of_data = _chunk_of_data.rename(
+            {self._timestamp_col: ColumnName.DMM_RESERVED_TS}, axis=1, errors="ignore"
+        )
         return _chunk_of_data
 
     def _get_predictions(self, chunk_of_data: pd.DataFrame) -> np.array:
         predictions = (
             chunk_of_data[self._prediction_col].to_numpy()
             if self._prediction_col in chunk_of_data
             else None
```

## dmm/data_source/dataframe_source.py

```diff
@@ -27,28 +27,26 @@
 
     def __init__(
         self,
         df: pd.DataFrame,
         max_rows: int = 10000,
         timestamp_col: ColumnName = ColumnName.TIMESTAMP,
     ) -> None:
-
         super().__init__(max_rows)
         if max_rows <= 0:
             raise ValueError(f"max_rows must be > 0, got {max_rows}")
 
         self._df = self._preprocess_df(df.copy(), timestamp_col)
         self._max_rows = max_rows
         self._timestamp_col = timestamp_col
         self._prev_chunk_datetime = None
         self.reset()
 
     @staticmethod
     def _preprocess_df(df: pd.DataFrame, timestamp_col: str) -> pd.DataFrame:
-
         if not isinstance(df[timestamp_col].iloc[0], pd.Timestamp):
             df[timestamp_col] = pd.to_datetime(df[timestamp_col])
 
         if not df[timestamp_col].is_monotonic_increasing:
             df.sort_values(by=timestamp_col, inplace=True, ignore_index=True)
 
         return df
```

## dmm/data_source/runtime_parameters_source.py

```diff
@@ -15,15 +15,16 @@
 from dmm.data_source.data_source_base import DataSourceBase
 from dmm.utils import RunTimeParameterHandler
 
 
 class RuntimeParametersDataSource(DataSourceBase):
     """This is a special case datasource that constructs a dataframe from
     the data passed in the runtime parameters. This is specifically for the custom llm
-    assessment case where only a single prompt response is being scored outside a deployment"""
+    assessment case where only a single prompt response is being scored outside a deployment
+    """
 
     def __init__(self, parameters: RunTimeParameterHandler):
         super().__init__(max_rows=1)
         self._parameters = parameters
         self.reset()
 
     def init(self, time_bucket):
```

## dmm/dr_custom_metrics/dr_custom_metric.py

```diff
@@ -135,19 +135,25 @@
             raise ConflictError(
                 "Too many custom metrics in this deployment - not supported"
             )
 
         cm_list = cm_dict["data"]
         return cm_list
 
-    def report_value(self, custom_metric_name: str, value: Union[int, float]) -> None:
+    def report_value(
+        self,
+        custom_metric_name: str,
+        value: Union[int, float],
+        association_id: str = None,
+    ) -> None:
         """
         Report a value for a custom metric given the custom metric name. Avoid using the ID
         :param custom_metric_name:
         :param value:
+        :param association_id:
         :return:
         """
         self._has_config()
 
         metric = self._find_metric_by_name(custom_metric_name)
         if metric is None:
             raise CustomMetricNotFound(
@@ -157,27 +163,30 @@
         if "id" not in metric:
             raise DRCustomMetricConfigError(
                 f"Custom Metric {custom_metric_name} ID not found, "
                 f"make sure the 'sync()' method has been called."
             )
 
         ts = datetime.utcnow()
-        buckets = [
-            {
-                "timestamp": ts.isoformat(),
-                "value": value,
-                "sampleSize": 1,
-            }
-        ]
+        bucket = {
+            "timestamp": ts.isoformat(),
+            "value": value,
+            "sampleSize": 1,
+        }
+
+        if association_id:
+            bucket["associationId"] = association_id
+        buckets = [bucket]
+
         response = self._api.submit_custom_metric_values(
             deployment_id=self._deployment_id,
             custom_metric_id=metric["id"],
-            model_package_id=self._model_package_id
-            if metric["isModelSpecific"]
-            else None,
+            model_package_id=(
+                self._model_package_id if metric["isModelSpecific"] else None
+            ),
             buckets=buckets,
         )
         response.raise_for_status()
         self._logger.info(
             f"Submitted custom metrics value: {buckets} for {custom_metric_name}"
         )
```

## Comparing `datarobot_model_metrics-0.5.0.dist-info/METADATA` & `datarobot_model_metrics-0.5.4.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.5.0
+Version: 0.5.4
 Summary: datarobot-model-metrics provides a framework to compute model ML metrics over time and produce aggregated metrics.
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

