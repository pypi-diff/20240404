# Comparing `tmp/datasurfer-1.0.7-py2.py3-none-any.whl.zip` & `tmp/datasurfer-1.0.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,8 +1,8 @@
-Zip file size: 131795 bytes, number of entries: 43
+Zip file size: 151600 bytes, number of entries: 50
 -rw-rw-rw-  2.0 fat    40406 b- defN 24-Jan-15 14:20 datastructure/Plot_Collections.py
 -rw-rw-rw-  2.0 fat    27117 b- defN 24-Feb-27 17:24 datastructure/Plot_IDIADA.py
 -rw-rw-rw-  2.0 fat      259 b- defN 24-Mar-06 09:10 datastructure/__init__.py
 -rw-rw-rw-  2.0 fat    29177 b- defN 24-Mar-06 09:10 datastructure/datainterface.py
 -rw-rw-rw-  2.0 fat    10271 b- defN 24-Mar-06 09:10 datastructure/datalake.py
 -rw-rw-rw-  2.0 fat    56915 b- defN 24-Feb-08 14:05 datastructure/dataobjects.py
 -rw-rw-rw-  2.0 fat    48434 b- defN 24-Mar-06 09:10 datastructure/datapool.py
@@ -14,32 +14,39 @@
 -rw-rw-rw-  2.0 fat     3021 b- defN 24-Feb-27 17:24 datastructure/lib_objects/data_object.py
 -rw-rw-rw-  2.0 fat     2211 b- defN 24-Feb-27 17:24 datastructure/lib_objects/finance_object.py
 -rw-rw-rw-  2.0 fat     2862 b- defN 24-Feb-27 17:24 datastructure/lib_objects/matlab_object.py
 -rw-rw-rw-  2.0 fat     6114 b- defN 24-Mar-06 09:10 datastructure/lib_objects/mdf_object.py
 -rw-rw-rw-  2.0 fat     1624 b- defN 24-Feb-27 17:24 datastructure/lib_objects/pandas_object.py
 -rw-rw-rw-  2.0 fat    40406 b- defN 24-Mar-06 13:39 datasurfer/Plot_Collections.py
 -rw-rw-rw-  2.0 fat    27117 b- defN 24-Mar-06 13:39 datasurfer/Plot_IDIADA.py
--rw-rw-rw-  2.0 fat      324 b- defN 24-Mar-19 09:27 datasurfer/__init__.py
+-rw-rw-rw-  2.0 fat      306 b- defN 24-Apr-03 09:04 datasurfer/__init__.py
 -rw-rw-rw-  2.0 fat    33171 b- defN 24-Mar-21 20:31 datasurfer/datainterface.py
--rw-rw-rw-  2.0 fat    14647 b- defN 24-Mar-21 20:31 datasurfer/datalake.py
--rw-rw-rw-  2.0 fat    55620 b- defN 24-Mar-21 20:31 datasurfer/datapool.py
--rw-rw-rw-  2.0 fat     1000 b- defN 24-Mar-19 09:27 datasurfer/lib_objects/__init__.py
--rw-rw-rw-  2.0 fat     4019 b- defN 24-Mar-06 13:39 datasurfer/lib_objects/amedata_object.py
--rw-rw-rw-  2.0 fat     4030 b- defN 24-Mar-06 13:39 datasurfer/lib_objects/amegp_object.py
--rw-rw-rw-  2.0 fat     7764 b- defN 24-Mar-18 16:20 datasurfer/lib_objects/ameres_object.py
--rw-rw-rw-  2.0 fat     8558 b- defN 24-Mar-18 16:19 datasurfer/lib_objects/asammdf_object.py
--rw-rw-rw-  2.0 fat     3021 b- defN 24-Mar-06 13:39 datasurfer/lib_objects/data_object.py
--rw-rw-rw-  2.0 fat     2211 b- defN 24-Mar-06 13:39 datasurfer/lib_objects/finance_object.py
--rw-rw-rw-  2.0 fat      532 b- defN 24-Mar-12 07:40 datasurfer/lib_objects/json_object.py
--rw-rw-rw-  2.0 fat     3192 b- defN 24-Mar-19 09:27 datasurfer/lib_objects/matlab_object.py
--rw-rw-rw-  2.0 fat     6114 b- defN 24-Mar-06 13:39 datasurfer/lib_objects/mdf_object.py
--rw-rw-rw-  2.0 fat     1812 b- defN 24-Mar-19 09:27 datasurfer/lib_objects/pandas_object.py
--rw-rw-rw-  2.0 fat     9500 b- defN 24-Mar-21 20:31 datasurfer/lib_plots/__init__.py
--rw-rw-rw-  2.0 fat    30030 b- defN 24-Mar-21 20:31 datasurfer/lib_plots/plot_collection.py
--rw-rw-rw-  2.0 fat    40406 b- defN 24-Mar-12 07:40 datasurfer/lib_plots/plot_utils.py
--rw-rw-rw-  2.0 fat      164 b- defN 24-Mar-25 08:31 datasurfer-1.0.7.dist-info/AUTHORS.rst
--rw-rw-rw-  2.0 fat     1530 b- defN 24-Mar-25 08:31 datasurfer-1.0.7.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2636 b- defN 24-Mar-25 08:31 datasurfer-1.0.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat      110 b- defN 24-Mar-25 08:31 datasurfer-1.0.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Mar-25 08:30 datasurfer-1.0.7.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3892 b- defN 24-Mar-25 08:31 datasurfer-1.0.7.dist-info/RECORD
-43 files, 555305 bytes uncompressed, 125525 bytes compressed:  77.4%
+-rw-rw-rw-  2.0 fat    16163 b- defN 24-Apr-03 09:04 datasurfer/datalake.py
+-rw-rw-rw-  2.0 fat    51607 b- defN 24-Apr-03 12:56 datasurfer/datapool.py
+-rw-rw-rw-  2.0 fat    15464 b- defN 24-Apr-03 15:20 datasurfer/datautils.py
+-rw-rw-rw-  2.0 fat     2453 b- defN 24-Apr-03 14:00 datasurfer/lib_mlearn/__init__.py
+-rw-rw-rw-  2.0 fat      227 b- defN 24-Apr-03 12:52 datasurfer/lib_mlearn/preproc.py
+-rw-rw-rw-  2.0 fat    29556 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/__init__.py
+-rw-rw-rw-  2.0 fat     5503 b- defN 24-Apr-03 20:43 datasurfer/lib_objects/amedata_object.py
+-rw-rw-rw-  2.0 fat     4037 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/amegp_object.py
+-rw-rw-rw-  2.0 fat     7723 b- defN 24-Apr-03 18:53 datasurfer/lib_objects/ameres_object.py
+-rw-rw-rw-  2.0 fat     8510 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/asammdf_object.py
+-rw-rw-rw-  2.0 fat     3028 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/data_object.py
+-rw-rw-rw-  2.0 fat     2229 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/finance_object.py
+-rw-rw-rw-  2.0 fat     1292 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/json_object.py
+-rw-rw-rw-  2.0 fat     3188 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/matlab_object.py
+-rw-rw-rw-  2.0 fat     6040 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/mdf_object.py
+-rw-rw-rw-  2.0 fat     1852 b- defN 24-Apr-03 09:04 datasurfer/lib_objects/pandas_object.py
+-rw-rw-rw-  2.0 fat     2426 b- defN 24-Apr-03 21:02 datasurfer/lib_objects/xkf_object.py
+-rw-rw-rw-  2.0 fat    11502 b- defN 24-Apr-03 09:04 datasurfer/lib_plots/__init__.py
+-rw-rw-rw-  2.0 fat    40380 b- defN 24-Apr-03 12:24 datasurfer/lib_plots/plot_collection.py
+-rw-rw-rw-  2.0 fat    41548 b- defN 24-Apr-03 09:04 datasurfer/lib_plots/plot_utils.py
+-rw-rw-rw-  2.0 fat     4982 b- defN 24-Apr-03 15:59 datasurfer/lib_stats/__init__.py
+-rw-rw-rw-  2.0 fat      667 b- defN 24-Apr-03 09:04 datasurfer/lib_stats/distrib_methods.py
+-rw-rw-rw-  2.0 fat     1955 b- defN 24-Apr-03 09:04 datasurfer/lib_stats/interp_methods.py
+-rw-rw-rw-  2.0 fat      164 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/AUTHORS.rst
+-rw-rw-rw-  2.0 fat     1530 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     2636 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4526 b- defN 24-Apr-04 07:09 datasurfer-1.0.9.dist-info/RECORD
+50 files, 625797 bytes uncompressed, 144332 bytes compressed:  76.9%
```

## zipnote {}

```diff
@@ -63,14 +63,23 @@
 
 Filename: datasurfer/datalake.py
 Comment: 
 
 Filename: datasurfer/datapool.py
 Comment: 
 
+Filename: datasurfer/datautils.py
+Comment: 
+
+Filename: datasurfer/lib_mlearn/__init__.py
+Comment: 
+
+Filename: datasurfer/lib_mlearn/preproc.py
+Comment: 
+
 Filename: datasurfer/lib_objects/__init__.py
 Comment: 
 
 Filename: datasurfer/lib_objects/amedata_object.py
 Comment: 
 
 Filename: datasurfer/lib_objects/amegp_object.py
@@ -96,35 +105,47 @@
 
 Filename: datasurfer/lib_objects/mdf_object.py
 Comment: 
 
 Filename: datasurfer/lib_objects/pandas_object.py
 Comment: 
 
+Filename: datasurfer/lib_objects/xkf_object.py
+Comment: 
+
 Filename: datasurfer/lib_plots/__init__.py
 Comment: 
 
 Filename: datasurfer/lib_plots/plot_collection.py
 Comment: 
 
 Filename: datasurfer/lib_plots/plot_utils.py
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/AUTHORS.rst
+Filename: datasurfer/lib_stats/__init__.py
+Comment: 
+
+Filename: datasurfer/lib_stats/distrib_methods.py
+Comment: 
+
+Filename: datasurfer/lib_stats/interp_methods.py
+Comment: 
+
+Filename: datasurfer-1.0.9.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/LICENSE
+Filename: datasurfer-1.0.9.dist-info/LICENSE
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/METADATA
+Filename: datasurfer-1.0.9.dist-info/METADATA
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/WHEEL
+Filename: datasurfer-1.0.9.dist-info/WHEEL
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/top_level.txt
+Filename: datasurfer-1.0.9.dist-info/top_level.txt
 Comment: 
 
-Filename: datasurfer-1.0.7.dist-info/RECORD
+Filename: datasurfer-1.0.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## datasurfer/__init__.py

```diff
@@ -1,11 +1,11 @@
 """Top-level package for Data Structure."""
 
 __author__ = """Wei Yu"""
 __email__ = 'yuwei2005@gmail.com'
 
 from datasurfer.datapool import Data_Pool
 from datasurfer.datalake import Data_Lake
-from datasurfer.lib_objects import *
-from datasurfer.lib_plots import Plots
+from datasurfer.lib_objects import DataInterface
 
-__all__ = ['Data_Pool', 'Data_Lake', 'Plots']
+
+__all__ = ['Data_Pool', 'Data_Lake', 'DataInterface']
```

## datasurfer/datalake.py

```diff
@@ -1,59 +1,17 @@
-import os
 import re
 import pandas as pd
 
 from collections import abc
 from itertools import chain
 from pathlib import Path
-
 from datasurfer.datapool import Data_Pool
 from difflib import SequenceMatcher
 
-
-#%%
-def collect_dirs(root, *patts, patt_filter=None):
-    """
-    Collects directories and their corresponding files under the given root directory.
-
-    Args:
-        root (str or list or tuple or set): The root directory or a collection of root directories.
-        *patts (str): Patterns to match against directory names.
-        patt_filter (list or None): Patterns to filter out directory names. Defaults to None.
-
-    Yields:
-        tuple: A tuple containing the path of the directory and a list of files in that directory.
-
-    Examples:
-        >>> for path, files in collect_dirs('/path/to/root', 'dir*', patt_filter=['dir2']):
-        ...     print(f"Directory: {path}")
-        ...     print(f"Files: {files}")
-        ...
-        Directory: /path/to/root/dir1
-        Files: ['file1.txt', 'file2.txt']
-        Directory: /path/to/root/dir3
-        Files: ['file3.txt', 'file4.txt']
-    """
-
-    patt_filter = patt_filter or [r'^\..*']
-
-    if isinstance(root, (list, tuple, set)):
-        root = chain(*root)
-
-    for r, _, fs in os.walk(root):
-        d = Path(r).stem
-        ismatch = (any(re.match(patt, d) for patt in patts) 
-                    and (not any(re.match(patt, d) for patt in patt_filter)))
-
-        if ismatch:
-            path = Path(r)
-            if fs:
-                yield path, fs
-                
-
+from datasurfer.datautils import collect_dirs, show_pool_progress
 
 #%%
 class Data_Lake(object):
     """
     Represents a data lake that contains multiple data pools.
 
     Parameters:
@@ -94,14 +52,15 @@
             objs = root
         elif root is None:
             objs = []
         else:
             raise ValueError('root must be a string or a sequence of DataPool objects.')
 
         self.objs = [obj for obj in objs if len(obj)]
+        assert all(self.keys()), 'Data pool names must be unique.'
         
     def __repr__(self):
         
         if self.name:
             return f'<DataLake"{self.name}"@{tuple(self.objs)}>'
         else:
             return f'<DataLake@{tuple(self.objs)}>'
@@ -132,21 +91,25 @@
         Raises:
             KeyError: If the specified key(s) do not exist in the datalake.
         """
         if isinstance(inval, str):
             if '*' in inval:
                 if inval.strip()[0] in '#@%':
                     patt = inval.strip()[1:]
-                    out = self.search(patt)
+                    out =  sorted(set(chain(*[obj.search_signal(patt) for obj in self.objs])))
                 else:
-                    out = [self.get_pool(name) for name in self.search(inval)]
+                    out = self.__class__([self.get_pool(name) for name in self.search(inval)])
             else:
                 out = self.get_pool(inval)
+                
         elif isinstance(inval, (list, tuple, set)):
-            out = [self.__getitem__(n) for n in inval]
+            out = self.__class__([self.get_pool(n) for n in inval])
+            
+        elif isinstance(inval, slice):
+            out = self.__class__(self.objs[inval])
         else:
             out = self.objs[inval]
         return out
     
     def __iter__(self):
         """
         Returns an iterator object that iterates over the elements in the `objs` list.
@@ -187,15 +150,63 @@
         Returns a list of paths to the data pools in the data lake.
 
         Returns:
         - A list of strings representing the paths to the data pools.
         """
         return [obj.path for obj in self.objs]
     
+    def signals(self):
+
+        return sorted(set(chain(*[obj.signals() for obj in self.objs])))
+
+
+
+
+    def get_pool(self, name):
+        """
+        Retrieves the data pool with the specified name from the data lake.
+
+        Parameters:
+        - name: The name of the data pool to retrieve.
+
+        Returns:
+        - The DataPool object with the specified name.
 
+        Raises:
+        - NameError: If no data pool with the specified name is found.
+        """
+        for obj in self.objs:
+            if obj.name == name:
+                return obj
+        else:
+            raise NameError(f'Can not find any "{name}"')
+        
+    def get_signals(self, *signals):
+        
+        out = dict((obj.name, obj.get_signals(*signals)) for obj in self.objs)
+        
+        return out
+    
+    def get_signal1Ds(self, *signals, as_df=True):
+        
+        out = dict((obj.name, obj.get_signal1Ds(*signals)) for obj in self.objs)
+        
+        if as_df:
+            reform = {(outerKey, innerKey): values for outerKey, innerDict 
+                      in out.items() for innerKey, values in innerDict.items()}
+            out = pd.DataFrame(reform)
+        
+        return out
+        
+    def iterobjs(self):
+        """
+        Returns an iterator that yields all objects in the datalake.
+        """
+        return chain(*self.items())
+    
     def search(self, patt, ignore_case=True, raise_error=False):
         """
         Searches for data pools in the data lake that match the specified pattern.
 
         Parameters:
         - patt: The pattern to search for.
         - ignore_case: Whether to ignore case when matching the pattern. Defaults to True.
@@ -223,54 +234,42 @@
 
         try:
             ratios = [SequenceMatcher(a=patt, b=f).ratio() for f in found]
             _, found = zip(*sorted(zip(ratios, found))[::-1])
         except ValueError:
             pass
 
-        return list(found)
-
-
-    def get_pool(self, name):
-        """
-        Retrieves the data pool with the specified name from the data lake.
-
-        Parameters:
-        - name: The name of the data pool to retrieve.
-
-        Returns:
-        - The DataPool object with the specified name.
-
-        Raises:
-        - NameError: If no data pool with the specified name is found.
-        """
-        for obj in self.objs:
-            if obj.name == name:
-                return obj
-        else:
-            raise NameError(f'Can not find any "{name}"')
-        
-    def iterobjs(self):
-        """
-        Returns an iterator that yields all objects in the datalake.
-        """
-        return chain(*self.items())
-    
+        return list(found)  
+     
     def search_object(self, pattern):
         """
         Search for objects in the datalake that match the given pattern.
 
         Args:
             pattern (str): The regular expression pattern to match against object names.
 
         Returns:
             list: A list of objects that match the given pattern.
         """
         return [obj for obj in self.iterobjs() if re.match(pattern, obj.name)]
     
+    def search_signal(self, pattern, **kwargs):
+        """
+        Searches for a signal pattern in the datalake.
+
+        Args:
+            pattern (str): The signal pattern to search for.
+            **kwargs: Additional keyword arguments to pass to the search method.
+
+        Returns:
+            list: A sorted list of unique signals matching the pattern.
+
+        """
+        return sorted(set(chain(*[obj.search_signal(pattern, **kwargs) for obj in self.objs])))
+        
     
     def find_duplicated(self, as_df=False):
         """
         Finds duplicated items in the data structure.
 
         Returns a dictionary where the keys are the duplicated items and the values are lists of items that contain the duplicated item.
 
@@ -393,15 +392,15 @@
             elif name.lower().endswith('.yaml') or name.lower().endswith('.yml'):
                 import yaml
                 with open(name, 'w') as file:
                     yaml.safe_dump(out, file)
         return out
     
     @staticmethod
-    def load_def(path, **kwargs):
+    def from_def(path, **kwargs):
         """
         Load data definition from a file or a dictionary and create a Data_Lake object.
 
         Args:
             path (str or dict): The path to the file containing the data definition in JSON or YAML format,
                                 or a dictionary representing the data definition.
             **kwargs: Additional keyword arguments to be passed to the Data_Pool.load_def method.
@@ -428,20 +427,62 @@
         elif isinstance(path, dict):
             data = path
         else:
             raise ValueError('Input value must be a string or a dictionary.')
         
         dlk = Data_Lake()    
         for name, values in data.items():
-            dp = Data_Pool.load_def(values, name=name, **kwargs)
+            dp = Data_Pool.from_def(values, name=name, **kwargs)
             dlk.objs.append(dp)
             
         return dlk
 
+    @staticmethod
+    def pipeline(*funs, pbar=True, ignore_error=True, asiterator=True):
+        """
+        Applies a series of functions to each object in the data pool and yields the result.
+
+        Args:
+            *funs: Variable number of functions to be applied to each object.
+
+        Yields:
+            The modified object after applying all the functions.
+        """
+        if not all(hasattr(fun, '__call__') for fun in funs):
+            raise ValueError('Input values must be callable.')    
+        
+        def wrapper(dlk):
+            @show_pool_progress('Processing', show=pbar)
+            def fun(dlk):
+                for dp in dlk.objs:   
+
+                    yield Data_Pool.pipeline(*funs, pbar=False, ignore_error=ignore_error, asiterator=False)(dp)
+                    
+            if asiterator:
+                return fun(dlk)
+            else:
+                return list(fun(dlk))
+            
+        return wrapper   
+    
+    def initialize(self, use_buffer=True, pbar=True):
+        
+        buffer = []
         
+        for dp in self.objs:
+            
+            if use_buffer:
+                buffer.extend(dp.initialize(buffer=buffer, pbar=pbar).objs)
+                
+                buffer = list(set(buffer))
+            else:
+                dp.initialize(pbar=pbar)
+            
+        return self
+    
 #%%       
 if __name__ == '__main__':
 
     pass
```

## datasurfer/datapool.py

```diff
@@ -7,186 +7,25 @@
 import datetime
 import json
 import importlib
 import pandas as pd
 import numpy as np
 import warnings
 import random
-
 import gc
 import traceback
 
 from pathlib import Path
-from tqdm import tqdm
-from collections import abc
-   
 from itertools import chain
-from functools import reduce, wraps
-
-from datasurfer.datainterface import DataInterface
-
-random.seed()
-#%% Collect files
-
-def collect_files(root, *patts, warn_if_double=True, ignore_double=False):
-    '''
-    Gather files from the data directory that match patterns
-    
-    Parameters:
-        root (str): The root directory to start searching for files.
-        *patts (str): Variable number of patterns in the format of regular expressions.
-        warn_if_double (bool, optional): If True, a warning will be raised if files with the same name exist. Defaults to True.
-        ignore_double (bool, optional): If True, files with the same name will be ignored and not returned. Defaults to False.
-        
-    Yields:
-        Path: An iterator of found file paths.
-        
-    Usage:
-        # Collect csv files in the current folder, return a list
-        collection = list(collect_files('.', r'.*\.csv', warn_if_double=True))
-    '''
-    if isinstance(root, (list, tuple, set)):
-        root = chain(*root)
-    
-    found = {}
-       
-    for r, ds, fs in os.walk(root):
-        for f in fs:
-            ismatch = any(re.match(patt, f) for patt in patts)
-            
-            if ismatch: 
-                path = (Path(r) / f)
-                
-                if warn_if_double and f in found:
-                    dirs = '\n\t'.join(found[f])
-                    warnings.warn(f'"{path}" exists already in:\n{dirs}')
-                    
-                if (f not in found) or (not ignore_double) :
-                    yield path
-                    
-                found.setdefault(f, []).append(r)
-                
-                
-                
-#%% Show_Pool_Progress_bar
-def show_pool_progress(msg, show=False, set_init=True, count=None):
-    """
-    Decorator function that adds progress bar functionality to a method in a data pool object.
-
-    Parameters:
-    - msg (str): The message to display in the progress bar.
-    - show (bool): Whether to show the progress bar or not. Default is False.
-    - set_init (bool): Whether to set the 'initialized' attribute of the data pool object to True after the method is executed. Default is True.
-    - count (int): The total count of objects to iterate over. If None, the count is determined by the length of the data pool object. Default is None.
-
-    Returns:
-    - The decorated method.
-    """
-    
-    def decorator(func):
-    
-        @wraps(func)
-        def wrapper(self, *args, **kwargs):
-                    
-            res = func(self, *args, **kwargs)
-
-            flag_pbar = (not self.silent) and (not self.initialized or show)
-            
-            if count is None:                
-                num = len(self.objs)
-            else:
-                num = count
-                
-            rng = range(num)
-            
-            if flag_pbar:
-                pbar = tqdm(rng)
-                iterator = pbar
-            else:
-                iterator = rng
-             
-            for idx in iterator:
-                
-                if flag_pbar: 
-                
-                    if count is None:
-
-                        pbar.set_description(f'{msg} "{self.objs[idx].name}"')
-                        
-                    else:
-                        
-                        pbar.set_description(f'{msg}')
-            
-                yield next(res)
-                
-            if set_init:
-                
-                self.initialized = True
-                        
-        return wrapper
-    
-    return decorator
-
-
-#%% Combine configs
-
-def combine_configs(*cfgs):
-    """
-    Combines multiple configuration dictionaries into a single dictionary.
-
-    Args:
-        *cfgs: Variable number of configuration dictionaries.
+from functools import reduce
 
-    Returns:
-        dict: A dictionary containing the combined configurations.
-
-    Example:
-        >>> cfg1 = {'a': 'apple', 'b': 'banana'}
-        >>> cfg2 = {'a': 'avocado', 'c': 'cherry'}
-        >>> combine_configs(cfg1, cfg2)
-        {'a': ['apple', 'avocado'], 'b': ['banana'], 'c': ['cherry']}
-    """
-    out = dict()
+from datasurfer.lib_objects import DataInterface
+from datasurfer.datautils import collect_files, combine_configs, show_pool_progress
     
-    for k, v in chain(*[cfg.items() for cfg in cfgs]):
-        
-        if isinstance(v, str):   
-             
-            out.setdefault(k, set()).add(v)
-            
-        else:
-            if k in out:
-                out[k] = out[k].union(set(v))
-            else:
-                out[k] = set(v)
-                
-            
-    if not out:
-        out = None
-    else:
-        for k, v in out.items():
-            out[k] = sorted(v)
-            
-    return out
-
-#%%
-def check_config_duplication(cfg):
-    
-    vals = list()
-    for val in cfg.values():
-        
-        if isinstance(val, str):
-            vals.append(val)
-        if isinstance(val, abc.Sequence) and not isinstance(val, str):
-            vals.extend(list(val))
-    
-    stat = dict()
-    [stat.setdefault(vals.count(s), set([])).add(s) for s in vals]
-    
-    return stat
+random.seed()
 
   
 #%% Data Pool
 
 class Data_Pool(object):
     """
     A class representing a data pool to process datasets.
@@ -288,17 +127,15 @@
             if all(isinstance(s, (str, Path)) for s in datobjects) and all(Path(s).is_dir() for s in datobjects):
                 
                 datobjects = reduce(lambda x, y: 
                                     Data_Pool(x, config=config, interface=interface, **kwargs)
                                     + Data_Pool(y, config=config, interface=interface,  **kwargs), datobjects)    
         elif datobjects is not None:
                 datobjects = [datobjects]
-            
-
-
+        
         if datobjects is None or len(datobjects)==0:
             datobjects = []
             
         objs = []
         
         for obj in datobjects:
             
@@ -318,20 +155,20 @@
                                
             elif isinstance(interface, type) and issubclass(interface, DataInterface):
                 objs.append(interface(obj, config=config, **kwargs))
                 
             elif isinstance(obj, (str, Path)):
                 key = Path(obj).suffix.lower()               
                 if key in map_interface:
-                    cls = getattr(importlib.import_module('datasurfer'), map_interface[key] )               
+                    module, cls = map_interface[key]
+                    cls = getattr(importlib.import_module(f'datasurfer.{module}'), cls)               
                     objs.append(cls(obj, config=config, **kwargs))
                 else:
                     raise ValueError(f'Can not find any interface for "{obj}"')
-       
-                
+      
         self.objs = sorted(set(objs), key=lambda x:x.name)
         self.apply_comments(**comments)
         
         self.initialized = False
         
 
     def __enter__(self):
@@ -422,36 +259,31 @@
                 if inval.strip()[0] in '#@%':
                     
                     patt = inval.strip()[1:]
                     
                     out = self.search_signal(patt, ignore_case=True)
                 else:
                 
-                    out = [self.get_object(name) for name in self.search_object(inval)]
+                    out = self.__class__([self.get_object(name) for name in self.search_object(inval)])
             else:
                 if inval in self.keys():
-                    out = self.get_object(inval)
+                    out = self.get_object(inval)                    
                 else:
-                    out = self.get_signal(inval)
+                    out = self.get_signal(inval.strip())
+                    
             
         elif isinstance(inval, (list, tuple, set)):
             
-            out = []
             
             if all(na in self.keys() for na in inval):
                 
-                out = [self.get_object(n) for n in inval]
+                out = self.__class__([self.get_object(n) for n in inval])
                 
             else:
-                
-                for n in inval:
-                    
-                    out.append(self.get_signal1D(n))
-                    
-                out = pd.concat(out, axis=1)
+                out = self.get_signal1Ds(*inval)
             
             
         elif hasattr(inval, '__call__'):
             
             if isinstance(inval, type) and issubclass(inval, DataInterface):
                 
                 out = self.__class__([obj for obj in self.objs if isinstance(obj, inval)])
@@ -556,33 +388,33 @@
             df = pd.concat([comments, signal, length, count, memory, itype, ftype, size, date, path], axis=1)
         else:
             df = pd.concat([itype, ftype, size, date, path], axis=1)
                     
         return df
                 
     def memory_usage(self, pbar=True):
-            """
-            Calculate the memory usage of each object in the datapool.
+        """
+        Calculate the memory usage of each object in the datapool.
 
-            Parameters:
-            - pbar (bool): Whether to show a progress bar during calculation. Default is True.
+        Parameters:
+        - pbar (bool): Whether to show a progress bar during calculation. Default is True.
 
-            Returns:
-            - s (pd.Series): Series containing the memory usage of each object.
-            """
+        Returns:
+        - s (pd.Series): Series containing the memory usage of each object.
+        """
+        
+        @show_pool_progress('Calculating', show=pbar)
+        def fun(self):
             
-            @show_pool_progress('Calculating', show=pbar)
-            def fun(self):
+            for obj in self.objs:
                 
-                for obj in self.objs:
-                    
-                    yield obj.name, obj.memory_usage().sum()
-            
-            s = pd.Series(dict(fun(self)), name='Memory Usage')
-            return s
+                yield obj.name, obj.memory_usage().sum()
+        
+        s = pd.Series(dict(fun(self)), name='Memory Usage')
+        return s
 
     def keys(self):
         """
         Returns a list of keys in the datapool.
         """
         return [obj.name for obj in self.objs]
     
@@ -753,27 +585,29 @@
         def get(self):
             for obj in self.objs:
                 yield obj.name, obj.count().sum()
                 
         return pd.Series(dict(get(self)), name='Signal Size')
     
     
-    def initialize(self, pbar=True):
+    def initialize(self, buffer=None, pbar=True):
         """
         Initializes the datapool by calling the `initialize` method on each object in the datapool.
 
         Args:
             pbar (bool, optional): Whether to show a progress bar during initialization. Defaults to True.
         """
         @show_pool_progress('Initializing', show=pbar, set_init=True)
         def get(self):
             for obj in self.objs:
-                obj.initialize()
+                bf = None if obj not in buffer else buffer[buffer.index(obj)].df
+                obj.initialize(buffer=bf)               
                 yield
-
+                
+        buffer = [] if buffer is None else buffer
         list(get(self))
 
         return self
     
     def load_signals(self, *keys, mapping=None, pbar=True):
         """
         Load signals from the data pool.
@@ -907,14 +741,29 @@
             
         """
         dats = list(self.iter_signal(
             signame, ignore_error=ignore_error, mask=mask))
 
         return pd.concat(dats, axis=1)
     
+    def get_signals(self, *signames, ignore_error=True, mask=None):
+        """
+        Retrieves the values of multiple signals from the datapool.
+
+        Args:
+            *signames: Variable-length argument list of signal names.
+            ignore_error (bool): Flag indicating whether to ignore errors when retrieving signals.
+            mask: Optional mask to apply to the retrieved signals.
+
+        Returns:
+            dict: A dictionary mapping signal names to their corresponding values.
+        """
+        return dict((signame, self.get_signal(signame, ignore_error=ignore_error, mask=mask))
+                    for signame in signames)
+    
     def get_signal1D(self, signame, ignore_error=True, mask=None, reindex=True):
         """
         Retrieve a 1-dimensional signal from the datapool.
 
         Parameters:
         - signame (str): The name of the signal to retrieve.
         - ignore_error (bool): Whether to ignore errors if the signal is not found. Default is True.
@@ -930,28 +779,24 @@
         out = pd.DataFrame(np.concatenate(dats, axis=0), columns=[signame])
 
         if reindex:
             out.index = np.arange(len(out))
 
         return out
     
-    def get_signals(self, *signames, ignore_error=True, mask=None):
-        """
-        Retrieves the values of multiple signals from the datapool.
-
-        Args:
-            *signames: Variable-length argument list of signal names.
-            ignore_error (bool): Flag indicating whether to ignore errors when retrieving signals.
-            mask: Optional mask to apply to the retrieved signals.
+    def get_signal1Ds(self, *signals, ignore_error=True, mask=None):
+        out = []
+        for sig in signals:
+            
+            out.append(self.get_signal1D(sig, ignore_error=ignore_error, mask=mask))
+            
+        out = pd.concat(out, axis=1)
+        return out
+    
 
-        Returns:
-            dict: A dictionary mapping signal names to their corresponding values.
-        """
-        return dict((signame, self.get_signal(signame, ignore_error=ignore_error, mask=mask))
-                    for signame in signames)
     
     def get_object(self, name):
         """
         Retrieve an object from the datapool by its name.
 
         Args:
             name (str): The name of the object to retrieve.
@@ -1181,41 +1026,43 @@
         else:
             assert len(mask_array) == len(self.objs), "The length of the mask array does not match the number of data objects."
             
             out = [obj for obj, msk in zip(self.objs, mask_array) if msk]
         return out
     
     @staticmethod
-    def pipeline(*funs, pbar=True, ignore_error=True, asiterator=False):
+    def pipeline(*funs, pbar=True, ignore_error=True, asiterator=True):
         """
         Applies a series of functions to each object in the data pool and yields the result.
 
         Args:
             *funs: Variable number of functions to be applied to each object.
 
         Yields:
             The modified object after applying all the functions.
         """
         if not all(hasattr(fun, '__call__') for fun in funs):
             raise ValueError('Input values must be callable.')
-    
+        if asiterator: pbar=False
+        
         def wrapper(dp):
             @show_pool_progress('Processing', show=pbar)
             def fun(dp):
                 for obj in dp.objs:                            
-                    for fun in funs: 
-                        try:
-                            fun(obj)
-                        except Exception as err:
-                            if ignore_error:
-                                errname = err.__class__.__name__
-                                tb = traceback.format_exc(limit=0, chain=False)
-                                warnings.warn(f'Exception "{errname}" is raised while processing "{obj.name}": "{tb}"')
-                            else:
-                                raise
+                    # for fun in funs: 
+                    #     try:
+                    #         fun(obj)
+                    #     except Exception as err:
+                    #         if ignore_error:
+                    #             errname = err.__class__.__name__
+                    #             tb = traceback.format_exc(limit=0, chain=False)
+                    #             warnings.warn(f'Exception "{errname}" is raised while processing "{obj.name}": "{tb}"')
+                    #         else:
+                    #             raise
+                    list(DataInterface.pipeline(*funs, ignore_error=ignore_error)(obj))
                     yield obj
                     
             if asiterator:
                 return fun(dp)
             else:
                 return list(fun(dp))
         return wrapper      
@@ -1463,15 +1310,15 @@
                 import yaml
                 with open(name, 'w') as file:
                     yaml.safe_dump(out, file)
 
         return out
     
     @staticmethod
-    def load_def(path, **kwargs):
+    def from_def(path, **kwargs):
         """
         Load data from a JSON file and create a Data_Pool object.
 
         Parameters:
         - name (str): The name of the JSON file to load.
         - **kwargs: Additional keyword arguments to pass to the Data_Pool constructor.
 
@@ -1627,14 +1474,33 @@
 
         Returns:
             Stat_Plots: An instance of the Stat_Plots class.
         """
         from datasurfer.lib_plots import Plots
         
         return Plots(self)
+    
+    @property
+    def stats(self):
+        """
+        Generate statistical summaries for the datapool objects.
+
+        Returns:
+            Stats: An instance of the Stats class.
+        """
+        from datasurfer.lib_stats import Stats
+        
+        return Stats(self)
+    
+    @property
+    def mlearn(self):
+        
+        from datasurfer.lib_mlearn import MLearn
+        
+        return MLearn(self)
 
 
 
 #%% Main Loop
 
 if __name__ == '__main__':
```

## datasurfer/lib_objects/__init__.py

```diff
@@ -1,23 +1,889 @@
-from datasurfer.datainterface import DataInterface
-from datasurfer.lib_objects.data_object import DATA_OBJECT
-from datasurfer.lib_objects.pandas_object import PANDAS_OBJECT
-from datasurfer.lib_objects.finance_object import FINANCE_OBJECT
-from datasurfer.lib_objects.asammdf_object import ASAMMDF_OBJECT
-from datasurfer.lib_objects.mdf_object import MDF_OBJECT
-from datasurfer.lib_objects.matlab_object import MATLAB_OBJECT
-from datasurfer.lib_objects.ameres_object import AMERES_OBJECT
-from datasurfer.lib_objects.amegp_object import AMEGP_OBJECT
-from datasurfer.lib_objects.amedata_object import AMEDATA_OBJECT
-from datasurfer.lib_objects.json_object import JSON_OBJECT
-
-__all__ = ['DataInterface',
-           'DATA_OBJECT', 
-           'PANDAS_OBJECT', 
-           'FINANCE_OBJECT',
-           'ASAMMDF_OBJECT', 
-           'MDF_OBJECT', 
-           'MATLAB_OBJECT',
-           'AMERES_OBJECT',
-           'AMEGP_OBJECT',
-           'AMEDATA_OBJECT',
-           'JSON_OBJECT']
+import os
+import re
+import pandas as pd
+import numpy as np
+import warnings
+import traceback
+
+from abc import ABC, abstractmethod
+from pathlib import Path
+from difflib import SequenceMatcher
+from itertools import chain, zip_longest
+from datasurfer.datautils import parse_config, translate_config, extract_channels
+
+#%% Data_Interface
+
+class DataInterface(ABC):
+    """
+    A class representing parent class of all data interfaces.
+
+    Parameters:
+    - path (str or Path): The path to the data file.
+    - config (str, Path, list, tuple, set, or dict): The configuration for the data object.
+    - name (str): The name of the data object.
+    - comment (str): Additional comment or description for the data object.
+
+    Attributes:
+    - path (Path): The absolute path to the data file.
+    - config (dict): The configuration for the data object.
+    - name (str): The name of the data object.
+    - comment (str): Additional comment or description for the data object.
+    - df (DataFrame): The data stored in a pandas DataFrame.
+
+    Methods:
+    - __enter__(): Enter method for context management.
+    - __exit__(exc_type, exc_value, exc_traceback): Exit method for context management.
+    - __repr__(): Returns a string representation of the data object.
+    - __str__(): Returns a string representation of the data object.
+    - __len__(): Returns the number of rows in the data object.
+    - __getitem__(name): Returns a column or subset of columns from the data object.
+    - __setitem__(name, value): Sets the value of a column in the data object.
+    - index(): Returns the index of the data object.
+    - meta_info(): Returns metadata information about the data object.
+    - size(): Returns the size of the data file in bytes.
+    - comment(): Returns the comment or description of the data object.
+    - comment(value): Sets the comment or description of the data object.
+    - df(): Returns the data stored in a pandas DataFrame.
+    - name(): Returns the name of the data object.
+    - name(value): Sets the name of the data object.
+    - initialize(): Initializes the data object by loading the data into a DataFrame.
+    - keys(): Returns a list of column names in the data object.
+    - describe(): Returns descriptive statistics of the data object.
+    - count(): Returns the number of non-null values in each column of the data object.
+    - get(*names): Returns a subset of columns from the data object.
+    - search(patt, ignore_case=True, raise_error=False): Searches for columns that match a pattern.
+    - memory_usage(): Returns the memory usage of the data object.
+    - clean_config(): Cleans the configuration by removing keys that do not exist in the data object.
+    - search_similar(name): Searches for column names that are similar to the given name.
+    - drop(*names, nonexist_ok=True): Drops columns from the data object.
+    - search_get(patt, ignore_case=False, raise_error=False): Returns a subset of columns that match a pattern.
+    - load(*keys, mapping=None): Loads additional columns into the data object.
+    - reload(): Reloads the data object by clearing the DataFrame cache.
+    - merge(obj0): Merges another data object into the current data object.
+    - squeeze(*keys): Removes columns from the data object, keeping only the specified columns.
+    - pipe(*funs): Applies a series of functions to the data object.
+    - rename(**kwargs): Renames columns in the data object.
+    - resample(new_index=None): Resamples the data object to a new index.
+    - to_numpy(): Returns the data as a numpy array.
+    - to_dict(): Returns the data as a dictionary.
+    - to_csv(name=None, overwrite=True): Saves the data as a CSV file.
+    - to_excel(name=None, overwrite=True): Saves the data as an Excel file.
+    - save(name, overwrite=True): Saves the data object to a file.
+    - close(clean=True): Closes the data object and cleans up resources.
+    """
+   
+   
+    def __init__(self, path, config=None, name=None, comment=None):
+        """
+        Initialize a DataObject instance.
+
+        Args:
+            path (str or Path): The path to the data object.
+            config (str, Path, list, tuple, set, dict, optional): The configuration for the data object.
+                If a string or Path is provided, it is assumed to be a path to a JSON file and will be loaded as a dictionary.
+                If a list, tuple, or set of strings is provided, it will be converted into a dictionary with each string as both the key and value.
+                If a list of dictionaries is provided, the dictionaries will be combined into a single dictionary.
+                If a dictionary is provided, it will be used as is.
+                Defaults to None.
+            name (str, optional): The name of the data object. Defaults to None.
+            comment (str, optional): A comment or description for the data object. Defaults to None.
+        """
+          
+        if path is not None:
+            path = Path(path).absolute() 
+            
+        self._name = name
+        self.path = path
+        self._config = parse_config(config)
+        self._comment = comment
+        
+    def __enter__(self):
+        
+        return self
+    
+    def __exit__(self, exc_type, exc_value, exc_traceback):
+        
+        self.close()
+        
+        if exc_type:
+            
+            return False
+        
+        return True
+    
+    def __eq__(self, other):
+        
+        return (self.__class__ == other.__class__) and (self.path == other.path) and (self.name == other.name)
+    
+    def __hash__(self):
+        return hash((self.path, self.name, self.__class__))
+    
+    def __repr__(self):
+        
+        return f'<{self.__class__.__name__}@{self.name}>'   
+
+    def __str__(self):
+        
+        return self.__repr__()
+    
+    def __len__(self):
+        
+        return len(self.df)
+        
+    def __getitem__(self, name):
+        
+        if isinstance(name, str) and '*' in name:
+
+            res = self.search(name)
+        
+        else:  
+            
+            if isinstance(name, str):
+                res = self.df[name]
+            else:
+                res = self.get(*name)
+       
+        return res
+    
+    def __setitem__(self, name, value):
+        
+        self.df[name] = value
+        
+    def __contains__(self, name):
+        
+        return name in self.df.columns  
+    
+    def __iter__(self):
+        
+        return self.df.items()
+    
+    @property
+    def config(self):
+        return self._config
+    @config.setter
+    def config(self, val):
+        
+                
+        self._config = parse_config(val)  
+    
+    def apply(self, name, value):
+        """
+        Applies the given value to the specified column name in the DataFrame.
+
+        Args:
+            name (str): The name of the column to apply the value to.
+            value: The value to apply to the column.
+
+        Returns:
+            self: The updated DataInterface object.
+
+        """
+        self.df[name] = value
+        return self
+    def set_index(self, colname, name=None, drop=False):
+        """
+        Apply the values of a specified column as the new index of the DataFrame.
+
+        Parameters:
+        colname (str): The name of the column to be used as the new index.
+        name (str, optional): The name to be assigned to the new index.
+
+        Returns:
+        self: The modified DataInterface object with the new index applied.
+        """
+        if drop: 
+            self.df.set_index(colname, inplace=True)
+        else:     
+            self.df.index = self.df[colname]
+        if name:
+            self.df.name = name
+
+        return self
+    
+    def reset_index(self, drop=True):
+        """
+        Resets the index of the DataFrame to the default integer index.
+
+        Args:
+            *args: Variable length argument list.
+            **kwargs: Arbitrary keyword arguments.
+
+        Returns:
+            self: The modified DataInterface object with the index reset.
+        """
+        self.df.reset_index(inplace=True, drop=drop)
+        return self
+
+    @property
+    def index(self):
+            """
+            Returns the index of the DataFrame as a NumPy array.
+
+            Returns:
+                numpy.ndarray: The index of the DataFrame.
+            """
+            return np.asarray(self.df.index)
+    
+    @property
+    def meta_info(self):
+        """
+        Returns a dictionary containing metadata information about the data interface object.
+
+        Returns:
+            dict: A dictionary containing the following metadata information:
+                - 'path': The path of the data interface object.
+                - 'name': The name of the data interface object.
+                - 'size': The size of the data interface object.
+                - 'shape' (optional): The shape of the data interface object if it has a DataFrame associated with it.
+                - Additional metadata information from the 'comment' attribute, if available.
+        """
+        out = dict()
+        out['path'] = str(self.path)
+        out['name'] = self.name
+        out['size'] = self.size
+
+        try:
+            out.update(self.comment)
+        except:
+            pass
+
+        if hasattr(self, '_df'):
+            out['shape'] = self.df.shape
+
+        return out
+    
+    @property
+    def size(self):
+        
+        return os.path.getsize(self.path)
+    
+    @property
+    def comment(self):
+        
+        return self._comment
+    
+    @comment.setter
+    def comment(self, value):
+        
+        self._comment = value
+    
+    @property
+    def df(self):
+        
+        if not hasattr(self, '_df'):
+            
+            self._df = self.get_df()
+            
+        return self._df 
+    
+    @property
+    def name(self):
+        
+        if self._name is None:
+            
+            assert self.path is not None, 'Expect a name for data object.'
+            return self.path.stem
+        else:
+            return self._name
+        
+    @name.setter
+    def name(self, val):
+        
+        self._name = val
+        
+    @abstractmethod
+    def get_df(self):
+        pass
+        
+    def initialize(self, buffer=None):
+        """
+        Initializes the data interface by retrieving the data frame.
+        
+        Returns:
+            self: The initialized data interface object.
+        """
+        if buffer is not None:
+            self._df = buffer
+        elif not hasattr(self, '_df'):
+            self._df = self.get_df()
+        
+        return self
+    
+    def keys(self):
+        """
+        Returns a list of column names in the DataFrame.
+
+        Returns:
+            list: A list of column names.
+        """
+        return list(self.df.columns)
+    
+    def describe(self):
+        """
+        Returns a statistical summary of the DataFrame.
+
+        Returns:
+            pandas.DataFrame: A DataFrame containing various statistical measures such as count, mean, standard deviation, minimum, maximum, and quartiles.
+        """
+        return self.df.describe()
+    
+    def count(self):
+        """
+        Returns the number of rows in the DataFrame.
+
+        Returns:
+            int: The number of rows in the DataFrame.
+        """
+        return self.df.count()
+        
+    def get(self, *names):
+        """
+        Retrieve data from the DataFrame based on the given column names.
+
+        Parameters:
+            *names: str
+                The names of the columns to retrieve from the DataFrame.
+
+        Returns:
+            pandas.DataFrame or pandas.Series
+                The requested data from the DataFrame.
+
+        """
+        if len(names) == 1:
+            signame, = names
+
+            if signame.lower() == 't' or signame.lower() == 'time' or signame.lower() == 'index':
+                return pd.DataFrame(np.asarray(self.df.index), index=self.df.index)
+
+        return self.df[list(names)]
+    
+    
+    def search(self, patt, ignore_case=True, raise_error=False):
+        """
+        Search for keys in the data structure that match a given pattern.
+
+        Parameters:
+        patt (str): The pattern to search for.
+        ignore_case (bool, optional): Whether to ignore case when matching the pattern. Defaults to True.
+        raise_error (bool, optional): Whether to raise a KeyError if no matching keys are found. Defaults to False.
+
+        Returns:
+        list: A list of keys that match the pattern.
+        """
+        
+        found = []
+        
+        if ignore_case: patt = patt.lower()
+        
+        for key in self.keys():
+            
+            if ignore_case:
+                
+                key0 = key.lower()
+            else:
+                key0 = key
+            
+            if re.match(patt, key0):
+                
+                found.append(key)
+                
+                
+        if not found and raise_error:
+            
+            raise KeyError(f'Cannot find any signal with pattern "{patt}".')
+            
+        try:
+            
+            ratios = [SequenceMatcher(a=patt, b=f).ratio() for f in found]
+            
+            _, found = zip(*sorted(zip(ratios, found))[::-1])
+            
+            
+        except ValueError:
+            pass
+                
+        return list(found)
+
+
+    
+    def memory_usage(self):
+        """
+        Returns the memory usage of the DataFrame.
+
+        Returns:
+            pandas.Series: A Series containing the memory usage of each column in the DataFrame.
+        """
+        return self.df.memory_usage(deep=True)
+    
+    def clean_config(self, replace=False):
+        """
+        Clean the configuration dictionary by removing any keys that are not present in the DataFrame.
+
+        Returns:
+            self: The current instance of the object.
+        """
+        if self.config is not None:
+            new_cfg = dict()
+            keys = self.df.keys()
+            for k, v in self.config.items():
+                if k in keys:
+                    new_cfg[k] = v
+                    
+            retval = set(self.config.keys()) - set(new_cfg.keys())
+            if replace:
+                self.config = new_cfg
+                  
+        return sorted(retval)
+    
+    def search_similar(self, name, channel=True):
+        """
+        Searches for keys in the data structure that are similar to the given name.
+        
+        Args:
+            name (str): The name to search for similarities.
+        
+        Returns:
+            list: A list of keys in the data structure that are similar to the given name, sorted in descending order of similarity.
+        """
+        keys = self.keys() if not (hasattr(self, 'channels') and channel) else self.channels
+        
+        ratios = [SequenceMatcher(a=name, b=k).ratio() for k in keys]
+        
+        _, sorted_keys = zip(*sorted(zip(ratios, keys))[::-1])
+        
+        return sorted_keys
+    
+    def search_missing_config_items(self, num=5, channel=True, by_key=False):
+        """
+        Searches for missing configuration items in the data interface.
+
+        Args:
+            num (int): The maximum number of similar items to display for selection. Default is 5.
+            channel (bool): Flag indicating whether to search for missing items in channels. Default is True.
+            by_key (bool): Flag indicating whether to search for similar items by key. Default is False.
+
+        Returns:
+            dict: The updated configuration dictionary.
+        """
+
+        config = self.config or dict()
+
+        chns = self.keys() if not (hasattr(self, 'channels') and channel) else self.channels
+
+        for key, val in config.items():
+
+            if not isinstance(val, (tuple, list, set)):
+
+                val = [val]
+                config[key] = val
+
+            if any(c in chns for c in val):
+
+                continue
+
+            else:
+                if by_key:
+                    found = [self.search_similar(key, channel=channel)]
+                else:
+                    found = [self.search_similar(v, channel=channel) for v in val]
+                topfound = list(chain(*zip_longest(*found)))[:num]
+
+                while 1:
+
+                    select = input(f'Select one of following key for "{key}" or type "n" for None: \n\t'
+                                   + '\n\t'.join(f'{idx}: {s}' for idx, s in enumerate(topfound, start=1))
+                                   + '\n')
+
+                    if select == 'n':
+                        break
+                    if select == 'x':
+                        raise KeyboardInterrupt
+                    else:
+                        try:
+                            idx = int(select) - 1
+
+                            k = topfound[idx]
+                            config[key].append(k)
+                            break
+                        except (IndexError, ValueError):
+                            print('Invalid input, try again')
+                            continue
+
+        return self.config
+        
+
+    
+    
+    def drop(self, *names, nonexist_ok=True):
+        """
+        Drop columns from the DataFrame.
+
+        Args:
+            *names: Variable length argument list of column names to be dropped.
+            nonexist_ok (bool, optional): If True, ignore columns that do not exist. If False, raise KeyError for non-existent columns. Defaults to True.
+
+        Returns:
+            self: Returns the modified DataInterface object.
+
+        Raises:
+            KeyError: If a non-existent column is specified and nonexist_ok is False.
+        """
+        keys = []
+
+        for name in names:
+            if name in self.df:
+                keys.append(name)
+            elif not nonexist_ok:
+                raise KeyError(f'"{name}" does not exist in "{self.name}".')
+
+        self.df.drop(columns=keys, inplace=True)
+
+        return self
+    
+    def search_get(self, patt, ignore_case=False, raise_error=False):
+        """
+        Returns the subset of the DataFrame that matches the given pattern.
+
+        Args:
+            patt (str): The pattern to search for.
+            ignore_case (bool, optional): Whether to ignore case when searching. Defaults to False.
+            raise_error (bool, optional): Whether to raise an error if no matches are found. Defaults to False.
+
+        Returns:
+            pandas.DataFrame: The subset of the DataFrame that matches the pattern.
+        """
+        return self.df[self.search(patt, ignore_case, raise_error)]
+
+    
+    def load(self, *keys, mapping=None):
+        """
+        Load data from the specified keys into the data object.
+        
+        Args:
+            keys: Variable number of keys to load data from.
+            mapping: Optional mapping configuration for data extraction.
+            
+        Returns:
+            The loaded data as a dictionary.
+        """
+        if not keys:       
+            keys = mapping.values()
+            
+        @translate_config(mapping)
+        @extract_channels(mapping)
+        def get(self, *keys):
+            return self.get(*keys)
+
+        df = get(self, *keys)
+        
+        for k, v in df.items():
+            self.df[k] = v
+            
+        return df
+    
+
+    def reload(self):
+        """
+        Reloads the data by deleting the existing dataframe and returning the instance.
+
+        Returns:
+            self: The instance of the DataInterface class.
+        """
+        if hasattr(self, '_df'):
+            del self._df
+        return self
+   
+    def merge(self, obj0, reset_index=False):
+        """
+        Merges the columns of another object into the current object.
+
+        Parameters:
+        - obj0: Another object to merge with.
+
+        Returns:
+        - self: The merged object.
+        """
+        keys = obj0.df.columns.difference(self.df.columns)
+        
+        if len(keys):
+            if reset_index:
+                self.reset_index()
+                obj0.reset_index()
+                
+            self._df[keys] = obj0[keys]
+                    
+        return self
+    
+    def squeeze(self, *keys):
+        """
+        Selects and returns a new DataFrame containing only the specified columns.
+
+        Parameters:
+        *keys: Variable length argument representing the column names to be selected.
+
+        Returns:
+        self: Returns the modified DataInterface object with the new DataFrame.
+
+        Raises:
+        KeyError: If any of the specified column names are not found in the DataFrame, it raises a KeyError.
+        """
+        try:
+            self._df = self.df[list(keys)]
+            
+        except KeyError:
+            new_keys = list(set(self._df.columns) & set(keys))
+            self._df = self.df[new_keys]
+            
+        return self
+    
+    @staticmethod
+    def pipeline(*funs, ignore_error=True):
+        """
+        Executes a series of functions on an object in a pipeline fashion.
+
+        Args:
+            *funs: Variable number of functions to be executed on the object.
+            ignore_error (bool, optional): Flag to indicate whether to ignore errors raised during execution. 
+                                            Defaults to True.
+
+        Raises:
+            Exception: If ignore_error is False and an error is raised during execution.
+
+        Returns:
+            None
+        """
+        if not all(hasattr(fun, '__call__') for fun in funs):
+            raise ValueError('Input values must be callable.')
+        
+        def wrapper(obj):
+            for fun in funs:
+                try:
+                    yield fun(obj)
+                except Exception as err:
+                    if ignore_error:
+                        errname = err.__class__.__name__
+                        tb = traceback.format_exc(limit=0, chain=False)
+                        warnings.warn(f'Exception "{errname}" is raised while processing "{obj.name}": "{tb}"')
+                    else:
+                        raise
+        return wrapper
+
+    
+    def rename(self, **kwargs):
+        """
+        Renames the columns of the DataFrame using the provided key-value pairs.
+
+        Args:
+            **kwargs: Key-value pairs where the key represents the current column name and the value represents the new column name.
+
+        Returns:
+            The modified DataFrame object.
+
+        Example:
+            df.rename(column1='new_column1', column2='new_column2')
+
+        """
+        keymap = kwargs
+
+        @translate_config(keymap)
+        def get(self):
+            return self._df
+
+        get(self)
+
+        return self
+    
+    def resample(self, new_index=None):
+        """
+        Return a new DataFrame with all columns values interpolated
+        to the new_index values.
+
+        Parameters:
+        - new_index (array-like, optional): The new index values to interpolate the columns to.
+                                            If not provided, the original DataFrame is returned.
+
+        Returns:
+        - new_obj (DATA_OBJECT): A new instance of the DATA_OBJECT class with the resampled DataFrame.
+        """
+        from datasurfer import DATA_OBJECT
+        
+        if new_index is not None:
+            new_index = np.asarray(new_index)
+
+            if new_index.size == 1:
+                idx = self.df.index
+                new_index = np.arange(min(idx), max(idx)+new_index, new_index)
+
+            df_out = pd.DataFrame(index=new_index)
+            df_out.index.name = self.df.index.name
+
+            for colname, col in self.df.items():
+                try:
+                    col = col.astype(float)
+                    df_out[colname] = np.interp(new_index, self.df.index, col)
+                except (TypeError, ValueError):
+                    warnings.warn(f'"{colname}" can not be resampled.')
+        else:
+            df_out = self.df
+
+        new_obj = DATA_OBJECT(path=str(self.path), config=self.config,
+                              name=self.name,
+                              comment=self.comment, df=df_out)
+
+        return new_obj
+    
+    def fill_missing_keys(self, config=None):
+        """
+        Fills in missing keys in the configuration dictionary.
+
+        Args:
+            config (dict, optional): The configuration dictionary to fill in missing keys for.
+                If not provided, the method will use the default configuration.
+
+        Returns:
+            dict: The updated configuration dictionary with missing keys filled in.
+        """
+        from datasurfer.datapool import combine_configs
+        config = config or self.config
+                
+        config = combine_configs(parse_config(config))
+
+        missing_keys = [k for k in config if k not in self.df.columns]
+
+        for mk in missing_keys:        
+            sigs = config[mk]
+            for sig in sigs:
+                for k, vs in config.items():
+                    
+                    if sig in vs and k in self.df.columns:
+                        self.df[mk] = self.df[k]
+            
+        return self
+    
+    def to_numpy(self):
+        """
+        Converts the data to a NumPy array.
+
+        Returns:
+            numpy.ndarray: The data as a NumPy array.
+        """
+        return self.resample()
+        
+    def to_dict(self):
+        """
+        Converts the data interface object to a dictionary.
+
+        Returns:
+            dict: A dictionary representation of the data interface object.
+        """
+        out = dict()
+        
+        out['path'] = str(self.path)
+        out['config'] = self.config
+        out['comment'] = self.comment  
+        out['name'] = self.name
+        out['df'] = self.df.to_numpy()
+        out['index'] = self.df.index
+        out['columns'] = self.df.columns
+        
+        return out
+    
+    def to_csv(self, name=None, overwrite=True):
+        """
+        Export the DataFrame to a CSV file.
+
+        Args:
+            name (str, optional): The name of the CSV file. If not provided, the name will be generated based on the object's name. Defaults to None.
+            overwrite (bool, optional): Whether to overwrite an existing file with the same name. Defaults to True.
+
+        Returns:
+            self: The current instance of the DataInterface object.
+        """
+        if name is None:
+            name = self.name + '.csv'
+            
+        if overwrite or not Path(name).is_file():
+            self.df.to_csv(name)
+        
+        return self
+
+    def to_excel(self, name=None, overwrite=True):
+        """
+        Export the DataFrame to an Excel file.
+
+        Args:
+            name (str, optional): The name of the Excel file. If not provided, the name will be generated based on the object's name. Defaults to None.
+            overwrite (bool, optional): Whether to overwrite the file if it already exists. Defaults to True.
+
+        Returns:
+            self: The current instance of the DataInterface object.
+        """
+        if name is None:
+            name = self.name + '.xlsx'
+        
+        if overwrite or not Path(name).is_file():
+            self.df.to_excel(name)
+        
+        return self
+    
+    def save(self, name, overwrite=True):
+        
+        if overwrite or not Path(name).is_file():
+            
+            from datasurfer import DATA_OBJECT
+            
+            dobj = DATA_OBJECT(path=self.path, config=self.config, name=self.name, 
+                           comment=self.comment, df=self.df)
+            dobj.save(name)
+        
+        return self
+    
+    def clean(self):
+        """
+        Cleans the data by deleting the internal DataFrame object.
+        
+        Returns:
+            self: The DataInterface object after cleaning.
+        """
+        if hasattr(self, '_df'):
+            del self._df
+        return self
+                
+    def close(self, clean=True):
+        """
+        Closes the file handler and performs optional cleanup.
+
+        Args:
+            clean (bool, optional): Flag indicating whether to perform cleanup. Defaults to True.
+        """
+        if hasattr(self, '_fhandler') and hasattr(self._fhandler, 'close'):
+            self._fhandler.close()
+            del self._fhandler
+
+        if clean:
+            self.clean()
+   
+    @property
+    def plot(self):
+        """
+        Generate a statistical plot using the Stat_Plots class.
+
+        Returns:
+            Stat_Plots: An instance of the Stat_Plots class.
+        """
+        from datasurfer.lib_plots import Plots
+        
+        return Plots(self)
+    
+    @property
+    def stats(self):
+        """
+        Generate statistical summaries for the datapool objects.
+
+        Returns:
+            Stats: An instance of the Stats class.
+        """
+        from datasurfer.lib_stats import Stats
+        
+        return Stats(self)
+# %%
```

## datasurfer/lib_objects/amedata_object.py

```diff
@@ -1,15 +1,13 @@
-import pandas as pd
-import numpy as np
-from ..datainterface import DataInterface
-
-
 import re
 import numpy as np
 import pandas as pd
+from datasurfer.lib_objects import DataInterface
+from functools import reduce
+#%% AMEDATA_OBJECT
 
 class AMEDATA_OBJECT(DataInterface):
     """ 
     A class reads AMESim tables.
 
     Attributes:
         fhandler (list): The lines of the file represented by the object.
@@ -130,8 +128,51 @@
     def get_df(self):
         """
         Method that returns the data as a pandas DataFrame.
 
         Returns:
             DataFrame: The data as a DataFrame.
         """
-        return pd.DataFrame(self.data1D, columns=['data'])
+        return pd.DataFrame(self.data1D, columns=['data'])
+    
+    @staticmethod
+    def dump(*inputs, titles=None, units=None):
+        
+        axispoints = [np.unique(arr) for arr in inputs[:-1]]
+        data = np.asarray(inputs[-1]).ravel()
+        
+        ncol= len(axispoints[0])
+        ndim = len(axispoints)
+        nrow = int(len(data)/ncol)
+        titles = titles or []
+        units = units or []
+        
+        assert ncol*nrow == len(data), 'Data length not match axispoints'
+               
+        lines = [f'# Table format: {ndim}D']
+             
+        if units:
+            lines.append(f'# table_unit = {units[0]}')
+            for idx, unit in enumerate(units[1:], 1):
+                lines.append(f'# axis{idx}_unit = {unit}')   
+                    
+        for idx, title in enumerate(titles, 1):
+            
+            lines.append(f'# axis{idx}_title = {title}')
+       
+        if ndim == 1:
+            for x, y in zip(axispoints[0], data):
+                lines.append(f'{x:>30.14e} {y:>30.14e}')                
+        else:
+            for arr in axispoints:
+                lines.append(f'{len(arr)}')
+            for arr in axispoints:
+                lines.append(''.join([f'{x:>30.14e}' for x in arr]))
+            
+            for row in range(nrow):               
+                lines.append(''.join([f'{x:>30.14e}' for x in data[row*ncol:(row+1)*ncol]]))         
+        return lines
+    
+if __name__ == '__main__':
+    pass    
+
+# %%
```

## datasurfer/lib_objects/amegp_object.py

```diff
@@ -1,13 +1,13 @@
 #%% Import Libraries
 
 import re
 import pandas as pd
 from pathlib import Path
-from ..datainterface import DataInterface
+from datasurfer.lib_objects import DataInterface
 
 
 #%% AMEGP_OJBECT
 class AMEGP_OBJECT(DataInterface):
     """
     Represents an object for handling AMEGP data.
```

## datasurfer/lib_objects/ameres_object.py

```diff
@@ -3,15 +3,16 @@
 
 import re
 import pandas as pd
 import numpy as np
 
 from pathlib import Path
 
-from ..datainterface import DataInterface , translate_config, extract_channels
+from datasurfer.lib_objects import DataInterface 
+from datasurfer.datautils import translate_config, extract_channels
 
 
 
 #%% AMERES_OJBECT
 class AMERES_OBJECT(DataInterface):
     """
     Represents an object for handling AMERES data.
@@ -188,37 +189,37 @@
                 res = pd.DataFrame(self.t, index=self.t, columns= ['time'])
         else:
             res = self.get_channels(*names)
                                                
         return res
 
     def get_results(self, rows=None):
-            """
-            Retrieves the results from a file and returns them as a NumPy array.
+        """
+        Retrieves the results from a file and returns them as a NumPy array.
 
-            Parameters:
-            - rows (list or None): Optional list of row indices to retrieve. If None, all rows are retrieved.
+        Parameters:
+        - rows (list or None): Optional list of row indices to retrieve. If None, all rows are retrieved.
 
-            Returns:
-            - array (ndarray): NumPy array containing the retrieved results.
-            """
-            
-            with open(self.path, "rb") as fobj:
-            
-                narray, = np.fromfile(fobj, dtype=np.dtype('i'), count=1)
-                nvar, = abs(np.fromfile(fobj, dtype=np.dtype('i'), count=1))
-                _ = np.hstack([[0], np.fromfile(fobj, dtype=np.dtype('i'), count=nvar)+1])                        
-                nvar = nvar + 1
-                array = np.fromfile(fobj, dtype=np.dtype('d'), count=narray*nvar)
-                array = array.reshape(narray, nvar).T
-                    
-            array = (array if rows is None 
-                     else array[np.concatenate([[0], np.asarray(rows, dtype=int).ravel()])])
-            
-            return array
+        Returns:
+        - array (ndarray): NumPy array containing the retrieved results.
+        """
+        
+        with open(self.path, "rb") as fobj:
+        
+            narray, = np.fromfile(fobj, dtype=np.dtype('i'), count=1)
+            nvar, = abs(np.fromfile(fobj, dtype=np.dtype('i'), count=1))
+            _ = np.hstack([[0], np.fromfile(fobj, dtype=np.dtype('i'), count=nvar)+1])                        
+            nvar = nvar + 1
+            array = np.fromfile(fobj, dtype=np.dtype('d'), count=narray*nvar)
+            array = array.reshape(narray, nvar).T
+                
+        array = (array if rows is None 
+                    else array[np.concatenate([[0], np.asarray(rows, dtype=int).ravel()])])
+        
+        return array
     
     def keys(self):
         
         if not len(self.df):
             
             res = self.channels
```

## datasurfer/lib_objects/asammdf_object.py

```diff
@@ -1,41 +1,34 @@
 #%% Import Libraries
-
-
 import re
 import pandas as pd
 import numpy as np
 import warnings
-
+import asammdf
+from asammdf import set_global_option    
+set_global_option("raise_on_multiple_occurrences", False) 
 from xml.etree.ElementTree import fromstring
-
-try:
-    import asammdf    
-    from asammdf import set_global_option
-    
-    set_global_option("raise_on_multiple_occurrences", False)   
-except:    
-    warnings.warn('Can not import "asammdf"')
-
-from ..datainterface import DataInterface, translate_config, extract_channels
+from datasurfer.lib_objects import DataInterface
+from datasurfer.datautils import translate_config, extract_channels
 
 #%% ASAMMDF_OBJECT
 class ASAMMDF_OBJECT(DataInterface):
     """
     Represents an ASAM MDF object.
 
     Args:
         path (str): The path to the MDF file.
         config (dict, optional): The configuration dictionary. Defaults to None.
         sampling (float, optional): The sampling rate. Defaults to 0.1.
         name (str, optional): The name of the object. Defaults to None.
         comment (str, optional): The comment for the object. Defaults to None.
         autoclose (bool, optional): Whether to automatically close the object. Defaults to False.
     """
-
+  
+    
     def __init__(self, path, config=None, sampling=0.1, name=None, 
                  comment=None, autoclose=False):
         """
         Initializes a new instance of the ASAMMDF_OBJECT class.
 
         Args:
             path (str): The path to the MDF file.
```

## datasurfer/lib_objects/data_object.py

```diff
@@ -1,10 +1,10 @@
 import pandas as pd
 import numpy as np
-from ..datainterface import DataInterface
+from datasurfer.lib_objects import DataInterface
 
 
 #%% DATA_OBJECT
 class DATA_OBJECT(DataInterface):
     """
     Represents a data object.
```

## datasurfer/lib_objects/finance_object.py

```diff
@@ -1,11 +1,12 @@
 import pandas as pd
-from ..datainterface import translate_config
-from .pandas_object import PANDAS_OBJECT
+from datasurfer.datautils import translate_config
+from datasurfer.lib_objects import PANDAS_OBJECT
 
+#%%
 
 class FINANCE_OBJECT(PANDAS_OBJECT):
     """
     A class representing a finance object.
     
     Attributes:
         path (str): The path to the finance object.
```

## datasurfer/lib_objects/json_object.py

```diff
@@ -1,23 +1,42 @@
 
 import json
 import pandas as pd
-from datasurfer.datainterface import DataInterface, translate_config
-
+from datasurfer.lib_objects import DataInterface
+from datasurfer.datautils import translate_config
 
+#%%
 class JSON_OBJECT(DataInterface):
-   
-    def __init__(self, path=None, config=None, name=None, comment=None):
+    """
+    Represents a JSON object.
+
+    Args:
+        path (str): The path to the JSON file.
+        config (dict): Configuration options for the JSON object.
+        name (str): The name of the JSON object.
+        comment (str): Additional comments about the JSON object.
+
+    Attributes:
+        path (str): The path to the JSON file.
+        config (dict): Configuration options for the JSON object.
+        name (str): The name of the JSON object.
+        comment (str): Additional comments about the JSON object.
+    """
 
+    def __init__(self, path=None, config=None, name=None, comment=None):
         super().__init__(path, config=config, name=name, comment=comment)
         
     @translate_config()
     def get_df(self):
+        """
+        Returns a pandas DataFrame representation of the JSON object.
 
+        Returns:
+            pandas.DataFrame: The DataFrame representation of the JSON object.
+        """
         dat = json.load(open(self.path, 'r'))
-
         df = pd.DataFrame(dat).transpose()
-        return df       
+        return df
         
 if __name__ == '__main__':
     
     pass
```

## datasurfer/lib_objects/matlab_object.py

```diff
@@ -1,19 +1,11 @@
 #%% Import Libraries
-
-
 import pandas as pd
 import numpy as np
-
-import scipy.io
-
-from datasurfer.datainterface import DataInterface
-
-
-
+from datasurfer.lib_objects import DataInterface
 
 #%% MATLAB_OJBECT
 
 class MATLAB_OBJECT(DataInterface):
     """
     Represents a MATLAB object that can be loaded from a file.
 
@@ -49,14 +41,15 @@
         Returns:
             numpy.ndarray: The loaded data.
 
         Raises:
             ValueError: If the data area cannot be found in the MATLAB file.
 
         """
+        import scipy.io
         if not hasattr(self, '_fhandler'):
             mat = scipy.io.loadmat(self.path)
             if self.matkey:
                 self._fhandler = mat[self.matkey]
             else:
                 for v in mat.values():
                     if isinstance(v, np.ndarray) and v.size > 0:
@@ -88,23 +81,23 @@
     def get_df(self):
         """
         Returns a pandas DataFrame containing the loaded data.
 
         Returns:
             pandas.DataFrame: The DataFrame containing the loaded data.
 
-        """
-        
+        """        
         dat = dict((k, self.fhandler[k].ravel()[0].ravel()) for k in self.fhandler.dtype.fields.keys())
         df = pd.DataFrame()
 
         for key, value in dat.items():
             try:
                 df[key] = value
             except ValueError:
                 if len(value) == 1:
                     df[key] = value[0]
                 else:
                     raise ValueError(f'Can not convert "{key}" to DataFrame')
 
         return df
+    
 # %%
```

## datasurfer/lib_objects/mdf_object.py

```diff
@@ -1,25 +1,16 @@
 #%% Import Libraries
 
-import os
-#import h5py
-import re
-
 
 import pandas as pd
 import numpy as np
 import warnings
-
-
-try:
-    import mdfreader
-except:    
-    warnings.warn('Can not import "mdfreader"')
-
-from ..datainterface import DataInterface, translate_config, extract_channels
+import mdfreader
+from datasurfer.lib_objects import DataInterface
+from datasurfer.datautils import translate_config, extract_channels
 
 #%% MDF_OBJECT
 class MDF_OBJECT(DataInterface):
     """
     Represents an MDF object that provides access to MDF file data.
 
     Args:
@@ -46,14 +37,15 @@
     def fhandler(self):
         """
         Lazily initializes and returns the MDF file handler.
 
         Returns:
             Mdf: The MDF file handler.
         """
+
         if not hasattr(self, '_fhandler'):
             self._fhandler = mdfreader.Mdf(self.path, no_data_loading=True)
         return self._fhandler
 
     @property
     def info(self):
         """
```

## datasurfer/lib_objects/pandas_object.py

```diff
@@ -1,11 +1,12 @@
 
 import numpy as np
 import pandas as pd
-from ..datainterface import DataInterface, translate_config
+from datasurfer.lib_objects import DataInterface
+from datasurfer.datautils import translate_config
 
 
 #%%
 class PANDAS_OBJECT(DataInterface):
     """
     A class representing a Pandas object.
```

## datasurfer/lib_plots/__init__.py

```diff
@@ -1,15 +1,15 @@
 import numpy as np
 import matplotlib.pyplot as plt
-import matplotlib.ticker as ticker
 import pandas as pd
 from collections import abc
 from functools import wraps
 from datasurfer.lib_plots.plot_collection import plot_histogram, plot_dendrogram, plot_parallel_coordinate
-from datasurfer.lib_plots.plot_utils import parallel_coordis
+from datasurfer.lib_plots.plot_utils import parallel_coordis, get_histo_bins
+from datasurfer.datautils import parse_data
 
 figparams = {'figsize': (8, 6), 
              'dpi': 120,}
 
 def set_ax(ax):
     
     ax.minorticks_on()
@@ -35,55 +35,15 @@
         ax = func(self, *keys, ax=ax, **kwargs)
         if setax:
             axisfunc(ax)
         
         return ax
     return wrapper
     
-def parse_data(func):   
-    @wraps(func)
-    def wrapper(self, *keys, **kwargs):
-        
-        def get(keys):
-            out = []
-            lbls = []
-            for key in keys:
-                if isinstance(key, str):
-                    out.append(self.dp[[key]].dropna().to_numpy().ravel())
-                    lbls.append(key)
-                elif isinstance(key, pd.Series):
-                    out.append(key.dropna().to_numpy())
-                    lbls.append(key.name)
-                elif isinstance(key, pd.DataFrame):
-                    out.extend(key.dropna().to_numpy().T)
-                    lbls.extend(key.columns)
-                elif isinstance(key, np.ndarray):
-                    out.append(key)
-                    lbls.append(None)
-                elif isinstance(key, abc.Sequence):
-                    o, ls = get(key)
-                    out.append(o)
-                    lbls.extend(ls)
-                else:
-                    raise ValueError('keys must be strings or numpy arrays')
-                
-            return out, lbls
-        
-        if all(isinstance(key, str) for key in keys):
-            out = self.dp[keys].dropna().to_numpy().T    
-            lbls = keys
-        else:        
-            out, lbls = get(keys)
-        
-        if ('labels' not in kwargs) and all(lbl is not None for lbl in lbls) :
-            kwargs['labels'] = lbls     
 
-        return func(self, *out, **kwargs)
-    
-    return wrapper
     
 
 class Plots(object):
     """
     A class for generating statistical plots.
     
     Parameters:
@@ -94,14 +54,29 @@
         """
         Initialize the Stat_Plots object.
         
         Parameters:
         - dp: A pandas DataFrame containing the data.
         """
         self.dp = dp
+        
+    def __call__(self, key, **kwargs):
+        """
+        Call the `line` method with the given key and keyword arguments.
+
+        Parameters:
+        - key: The key to identify the line.
+        - **kwargs: Additional keyword arguments to pass to the `line` method.
+
+        Returns:
+        - The result of the `line` method.
+
+        """
+        return self.line(key, **kwargs)
+    
     
     def set_figparam(self, **kwargs):
         """
         Set the figure parameters for the plots.
         
         Parameters:
         - **kwargs: The keyword arguments to be passed to the matplotlib figure function.
@@ -147,16 +122,16 @@
 
         
         if ax is None:           
             _, ax = plt.subplots()
             
             
         if bins is None:
-            bins = np.linspace(np.min(data), np.max(data), 10)
-        
+            # bins = np.linspace(np.min(data), np.max(data), 10)
+            bins = get_histo_bins(data)
         elif isinstance(bins, (abc.Sequence, np.ndarray)):
             bins = np.asarray(bins)
         
         elif isinstance(bins, int):
             bins = np.linspace(np.min(data), np.max(data), bins) 
               
         else:
@@ -270,18 +245,107 @@
         plot_parallel_coordinate(host=ax, df=df.dropna(), **default)
         
         return ax
     
     @define_ax
     @parse_data
     def parallel_coordis(self, *keys, **kwargs):
+        """
+        Generate a parallel coordinates plot using the given keys as dimensions.
+
+        Parameters:
+        - keys: The keys to be used as dimensions for the parallel coordinates plot.
+        - kwargs: Additional keyword arguments to be passed to the `parallel_coordis` function.
+
+        Returns:
+        - self: The current instance of the class.
+
+        Example usage:
+        ```
+        labels = ['A', 'B', 'C']
+        keys = [1, 2, 3]
+        parallel_coordis(labels, *keys, color='blue')
+        ```
+        """
         labels = kwargs.pop('labels')
         df = pd.DataFrame(dict(zip(labels, keys)))               
         parallel_coordis(df.values.T, **kwargs)
         
         return self
     
+    @define_ax
+    def heatmap(self, *keys, ax=None, **kwargs):
+        """
+        Generate a heatmap plot based on the correlation between the specified keys.
+
+        Parameters:
+            *keys: Variable length argument list of keys to calculate correlation.
+            ax: Optional matplotlib Axes object to plot the heatmap on.
+            **kwargs: Additional keyword arguments to customize the plot.
+
+        Returns:
+            ax: The matplotlib Axes object containing the heatmap plot.
+        """
+        import seaborn as sns
+
+        cmap = kwargs.pop('cmap', sns.diverging_palette(230, 20, as_cmap=True))
+
+        corr = self.dp.stats.corr(*keys)
+
+        default = dict(annot=True, cmap=cmap, cbar=False, vmin=-1, vmax=1)
+
+        default.update(kwargs)
+
+        sns.heatmap(corr, ax=ax, **default)
+        ax.set_aspect('equal')
+
+        return ax
+    
+    @define_ax
+    def wordcloud(self, text=None, ax=None, remove=('\n', '\t'), **kwargs):
+        """
+        Generate a word cloud visualization.
+
+        Parameters:
+            text (str): The text to generate the word cloud from. If not provided, it will use the comments from the data provider.
+            ax (matplotlib.axes.Axes, optional): The matplotlib axes to plot the word cloud on. If not provided, a new figure will be created.
+            **kwargs: Additional keyword arguments to customize the word cloud.
+
+        Returns:
+            matplotlib.axes.Axes: The matplotlib axes containing the word cloud plot.
+        """
+        from datasurfer.lib_plots.plot_collection import plot_wordcloud
+
+        if text is None:
+            text = ' '.join([txt for txt in self.dp.comments().values.tolist() if isinstance(txt, str)])
+
+        for r in remove:
+            text = text.replace(r, ' ')
+            
+        ax = plot_wordcloud(ax=ax, text=text, **kwargs)
+
+        return ax
+    
+    @define_ax
+    @parse_data
+    def kde(self, *keys, ax=None, **kwargs):
+
+        from datasurfer.lib_stats.distrib_methods import get_kde
+        lbls = kwargs.pop('labels', [None]*len(keys))
+        num = kwargs.pop('count', 100)
+        pltkws = kwargs.pop('plot_kws', {})
+        
+        for key, lbl in zip(keys, lbls):
+            
+            kde = get_kde(key, **kwargs)
+            x = np.linspace(np.min(key), np.max(key), num)
+            ax.plot(x, kde(x), label=lbl, **pltkws)
+
+        
+        return ax
+        
+    
     
        
 if __name__ == '__main__':
     
     pass
```

## datasurfer/lib_plots/plot_collection.py

```diff
@@ -6,15 +6,15 @@
 """
 import numpy as np
 import matplotlib.pyplot as plt
 import matplotlib.ticker as tck
 import warnings
 from matplotlib.path import Path
 import matplotlib.patches as patches
-from scipy.cluster.hierarchy import dendrogram, linkage
+
 from datasurfer.lib_plots.plot_utils import trigrid
 
 #%%
 
 def plot_violin(ax, df, ldata=None, rdata=None, **kwargs):
     def calc_violin_data(w, y0):
         from scipy import interpolate
@@ -92,14 +92,15 @@
     - method (str, optional): The linkage method to be used. Defaults to 'centroid'.
 
     Returns:
     - Z (numpy.ndarray): The linkage matrix.
     - dn (dict): The dendrogram dictionary.
 
     """
+    from scipy.cluster.hierarchy import dendrogram, linkage
     Z = linkage(df.values.T, method=method)
     dn = dendrogram(Z, ax=ax)
     ax.spines['top'].set_visible(False)
     ax.spines['right'].set_visible(False) 
     ax.spines['left'].set_visible(False) 
     ax.axes.get_yaxis().set_visible(False)
     
@@ -781,15 +782,14 @@
     
     if rebuildx:
         x = np.arange(0, len(bins))
         
     if width is None:        
         width = min(np.diff(x)) * 0.5 / n   
 
-
     offsets = np.linspace(-(n-1)*width/2, (n-1)*width/2, n)
     bottom  = np.zeros([n, len(bins)-1], dtype=float)    
 
     xlocs = (x[:-1] + x[1:]) / 2 + offsets.reshape(-1, 1) 
     
     count = 0
     used_labels = []
@@ -801,19 +801,18 @@
                 
                 arr2d = arr2d.reshape(1, -1)
                 
         except ValueError:
             
             arr2d = [np.asarray(d) for d in dat]
         
-        for idx1, arr1d in enumerate(arr2d):
+        for arr1d in arr2d:
             
             if arr1d.size == len(bins) - 1:            
                 y = arr1d
-                #x = bins
                 
             else:               
                 y, _ = np.histogram(arr1d, bins)
                 
             if yfun:
                 y = yfun(y)
                 
@@ -923,16 +922,327 @@
     ax.minorticks_on()
     
     ax.spines['top'].set_visible(False)
     ax.spines['right'].set_visible(False)
     
     return bs
 
+
+#%%---------------------------------------------------------------------------#
+def scale_radar(array, n=6, kind='max'):
+    
+    from matplotlib.ticker import MaxNLocator, LinearLocator
+    
+    
+    if array.ndim == 1:
+        
+        array = np.atleast_2d(array).T
+        
+    if kind == 'linear':
+        locator = LinearLocator(n)
+    else:
+        locator = MaxNLocator(n)
+        
+    tvalues = ([locator.tick_values(*aminmax) for aminmax
+                        in zip(array.min(axis=0), array.max(axis=0))])
+        
+    aminmax = np.vstack((arr.min(), arr.max()) for arr in tvalues)
+    amins = aminmax[:, 0]
+    aptps = aminmax.ptp(axis=1)
+
+    
+    ticks = np.vstack(tvalue[:n] for tvalue in tvalues)
+    
+    arrays = (array - amins) / aptps
+    
+    return arrays, ticks
+
+#%%---------------------------------------------------------------------------#
+def radar(titles, **title_opts):
+
+    from matplotlib.path import Path
+    from matplotlib.spines import Spine
+    from matplotlib.projections.polar import PolarAxes
+    from matplotlib.projections import register_projection
+    from matplotlib.collections import LineCollection
+
+
+    def unit_poly_verts(theta):
+        """Return vertices of polygon for subplot axes.
     
+        This polygon is circumscribed by a unit circle centered at (0.5, 0.5)
+        """
+        x0, y0, r = [0.5] * 3
+        verts = [(r*np.cos(t) + x0, r*np.sin(t) + y0) for t in theta]
+        return verts
+
+    def draw_poly_patch(self):
+        
+        verts = unit_poly_verts(THETA)
+        
+        return plt.Polygon(verts, closed=True, edgecolor='k')
+    
+    TITLES = list(titles)   
+    THETA = np.linspace(0, 2*np.pi, len(TITLES), endpoint=False) + np.pi/2 
+    FRAME = title_opts.pop('frame', 'polygon')
+    
+    class RadarAxes(PolarAxes):
+    
+        name = 'radar'
+        
+        def __init__(self, *args, **kwargs):
+            
+            self.dim = len(TITLES)
+            
+            self.theta = np.linspace(0, 2*np.pi, self.dim+1) + np.pi/2
+            
+            super().__init__(*args, **kwargs)
+                        
+            super().grid(False)
+
+            self.rgrid() 
+            
+            title_opts_ = dict(fontsize=12, weight="bold", color="black")
+            
+            title_opts_.update(title_opts)
+            self.set_thetagrids(np.degrees(THETA).astype(np.float32), 
+                                labels=TITLES, **title_opts_) 
+
+            self.set_xlim(np.pi/2, np.pi*2 + np.pi/2)
+            self.set_ylim(0, 1)
 
 
+            self.set_yticklabels([])
+            for tick, d in zip(self.xaxis.get_ticklabels(), THETA):
 
+                if np.pi / 2 < d < np.pi * 1.5:
+                    tick.set_ha('right')
+                elif np.pi * 1.5 < d:
+                    tick.set_ha('left')
+
+        def _gen_axes_patch(self):
+            
+            return draw_poly_patch(self)
+        
+    
+        def _gen_axes_spines(self):
+            if FRAME == 'circle':
+                return super()._gen_axes_spines()
+            else:
+                # The following is a hack to get the spines (i.e. the axes frame)
+                # to draw correctly for a polygon frame.
+        
+                # spine_type must be 'left', 'right', 'top', 'bottom', or `circle`.
+                spine_type = 'circle'
+                verts = unit_poly_verts(THETA)
+                # close off polygon by repeating first vertex
+                verts.append(verts[0])
+                path = Path(verts)
+        
+                spine = Spine(self, spine_type, path)
+                spine.set_transform(self.transAxes)
+                
+                return {'polar': spine}
+            
+        def rgrid(self, b=True, count=5, **kwargs):
+            
+            if hasattr(self, '_rgrids'):
+                for col in self._rgrids:
+                    
+                    col.remove()
+            
+            if b:
+            
+                defaults = dict(color='grey', lw=0.5)       
+                defaults.update(kwargs)
+                
+                dy = 1 / count
+                            
+                ys = np.ones_like(self.theta) * np.arange(dy, 1+dy, dy).reshape(-1, 1)
+                
+                xs = np.tile(self.theta, (count, 1))
+                
+                xys = np.dstack([xs, ys])
+        
+                line_segments = LineCollection(xys, **defaults)
+                
+                line_colls1 = self.add_collection(line_segments)
+                
+                xs = np.tile(THETA.reshape(-1, 1), 2)
+                ys = np.tile([0, 1], (self.dim, 1))
+                
+                xys = np.dstack([xs, ys])
+                line_segments = LineCollection(xys, **defaults)
+                
+                line_colls2 = self.add_collection(line_segments)
+                                
+                self._rgrids = (line_colls1, line_colls2)
+
+        def set_zebra(self, **kwargs):
+            
+            assert hasattr(self, '_rgrids')
+            
+            defaults = dict(color='grey', alpha=0.2)
+            
+            defaults.update(kwargs)
+            arr = np.dstack([p.vertices for p in self._rgrids[0].get_paths()])
+            self._rgrids[0].set_visible(defaults.pop('edge', False))
+            xs, ys = arr[:, 0, :], arr[:, 1, :]
+            
+            xs = xs.T
+            ys = ys.T
+            
+
+            return [self.fill_between(xs[2*i], ys[2*i], ys[2*i+1], 
+                        zorder=0, **defaults) for i in range(len(xs) // 2)]
+            
+        def get_theta(self, title):
+            
+            return THETA[TITLES.index(title)]
+        
+        def scale(self, array, *arg,  **kwargs):
+            
+            assert hasattr(self, '_rgrids')
+
+            origin = kwargs.pop('include_origin', False)
+            apply = kwargs.pop('apply_tick', False)
+            kind = kwargs.pop('kind', 'max')
+            
+            arr = np.dstack(p.vertices for p in self._rgrids[0].get_paths())
+
+            _, _, n = arr.shape
+            
+
+            arrays, ticks = scale_radar(array, n=n+1, kind=kind)
+            
+            if not origin:
+                
+                ticks = ticks[:, 1:]
+
+            
+            if apply:
+                
+                for title, labels in zip(TITLES, ticks):
+                    
+                    
+                    self.set_rlabel(title, labels, include_origin=origin, **kwargs)
+            
+            return arrays, ticks
+            
+        def set_rlabel(self, title, labels, **kwargs):
+            
+            def get_txt():
+                
+                for xy, lbl in zip(xys, labels):
+                    
+                    yield self.text(*xy, fmt(lbl), **kwargs)
+                    
+            assert hasattr(self, '_rgrids')
+            assert hasattr(labels, '__iter__')
+            
+            fmt = kwargs.pop('fmt', str)
+            origin = kwargs.pop('include_origin', False)
+            
+            array = np.dstack([p.vertices for p in self._rgrids[0].get_paths()])
+            
+
+            idx = TITLES.index(title)
+            xys = array[idx].T
+
+            if origin:
+
+                xys = np.concatenate([[[0, 0]], xys], axis=0)            
+            
+            return list(get_txt())
+        
+        def plot(self, array, *args, **kwargs):
+            
+            clustered = kwargs.pop('clustered', False)            
+            yarray = np.atleast_2d(np.asarray(array))
+            
+            assert np.all((yarray >= 0) & (yarray <= 1))
+            num, dim = yarray.shape
+            
+            if dim != self.dim and num == self.dim:
+                
+                yarray = yarray.T
+            
+                num, dim = dim, num
+            
+            assert dim == self.dim
+            
+            yarray = np.concatenate([yarray, yarray[:, :1]], axis=1)
+
+            
+            if clustered:
+                
+                xys = np.dstack([np.tile(self.theta, (num, 1)), yarray])
+                line_segments = LineCollection(xys, *args, **kwargs)
+                
+                lines = self.add_collection(line_segments)
+            
+            else:
+                lines = super().plot(self.theta, yarray.T, *args, **kwargs)
+            
+            return lines
+        
+        def fill_lines(self, lines, **kwargs):
+            
+            colors = kwargs.pop('colors', [None]*len(lines))
+            
+            assert len(colors) == len(colors)
+            
+            for line, c in zip(lines, colors):
+                
+                color = line.get_color() if c is None else c
+                super().fill(*line.get_xydata().T, color=color, **kwargs)
+                
+            return self
+        
+    register_projection(RadarAxes)
+    
+    return RadarAxes.name 
+
+def plot_wordcloud(ax, text, **kwargs):
+    """
+    Plot a word cloud on the given axes.
+
+    Parameters:
+    - ax (matplotlib.axes.Axes): The axes on which to plot the word cloud.
+    - text (str): The text to generate the word cloud from.
+    - **kwargs: Additional keyword arguments to customize the word cloud appearance.
+
+    Returns:
+    - matplotlib.axes.Axes: The axes with the word cloud plotted.
+    """
+    from wordcloud import WordCloud
+    
+    default = dict(background_color='black', colormap='viridis', height=600, width=1000,
+                   contour_width=2, contour_color='steelblue')
+    
+    default.update(kwargs)
+    wc = WordCloud(**default)
+    wc.generate(text)
+    
+    ax.imshow(wc, interpolation='bilinear')
+    ax.axis('off')
+    
+    return ax
+    
+    
 #%% main
 
 if __name__ == '__main__':
     
-    pass
+    fig = plt.figure(dpi=120)
+    
+    ax = fig.add_subplot(111, projection=radar(['A', 'B', 'C', 'DYE', 'E', 'FICE']))
+    ax.rgrid(lw=1, linestyle='--',  alpha=0.4)
+    
+    lines = ax.plot([0.5, 0.4, 0.6, 0.8, 0.6, 0.3], color='black')
+    lines = ax.plot([0.5, 0.4, 0.8, 0.2, 0.6, 0.3], color='red')
+    ax.fill_lines(lines, alpha=0.3, label='boo')
+    ax.set_zebra( lw= 1)
+    ax.set_rlabel('E', list('abcde'), color='b')
+    ax.set_rlabel('A', list('cdefg'), color='b')
+    ax.legend()
+    plt.show()
```

## datasurfer/lib_plots/plot_utils.py

```diff
@@ -4,14 +4,48 @@
 import warnings
 import matplotlib
 import matplotlib.pyplot as plt
 import matplotlib.tri as tri 
 
 from matplotlib.collections import LineCollection
 
+
+#%%
+def get_histo_bins(arr, num=10, decimals=0, base=None, maxi=None):
+    
+    arr = np.asarray(arr).ravel()
+    
+    ptp = arr.ptp()
+    
+    assert ptp > 0, 'Data has no range'
+    
+    while 1:    
+        
+        if np.round(ptp, decimals-1) > 0:
+            
+            break       
+        decimals += 1
+
+    base = base if base is not None else np.floor(arr.min())
+    
+    ptp = np.round(arr.max()-base, decimals-1)
+    ptp = ptp * (num + 1) / num
+    
+    maxi = maxi if maxi is not None else base+ptp
+
+    bins = np.round(np.linspace(base, maxi, num+1), decimals)
+    
+    if decimals == 0:
+        
+        bins = bins.astype(int)
+        
+    return bins
+
+
+
 #%%---------------------------------------------------------------------------#  
 def find_enclosed_data(x, y, linedata, boundary=True):
     
     from scipy.spatial import Delaunay
     
     if not np.allclose(linedata[0], linedata[-1]):
         
@@ -1235,43 +1269,56 @@
                    
         axis.set_minor_locator(locator_minor)
     
     return axis
     
     
 #%%---------------------------------------------------------------------------#
-if __name__ == '__main__':
+# if __name__ == '__main__':
     
 
-    titles = [s.strip().capitalize() for s 
-              in open('pareto_overview.csv', 'r').readline().strip('#').split(';')]
+#     titles = [s.strip().capitalize() for s 
+#               in open('pareto_overview.csv', 'r').readline().strip('#').split(';')]
     
-    array = np.loadtxt('pareto_overview.csv', delimiter=';')
+#     array = np.loadtxt('pareto_overview.csv', delimiter=';')
 
-    array = array[(array[:, 7] < 55) & (array[:, 8] < 2)]
-    array = (array - array.min(axis=0)) / (array.max(axis=0) - array.min(axis=0))    
+#     array = array[(array[:, 7] < 55) & (array[:, 8] < 2)]
+#     array = (array - array.min(axis=0)) / (array.max(axis=0) - array.min(axis=0))    
     
     
-    fig = plt.figure(tight_layout=True, figsize=(14, 7), dpi=120)
+#     fig = plt.figure(tight_layout=True, figsize=(14, 7), dpi=120)
     
-    ax0 = fig.add_subplot(111, projection=register_radar(titles=titles[-6:]))
-#    
-    ax0.plot(array[:3, -6:], labels=['Design {}'.format(i) for i in range(len(array))])
+#     ax0 = fig.add_subplot(111, projection=register_radar(titles=titles[-6:]))
+   
+#     ax0.plot(array[:3, -6:], labels=['Design {}'.format(i) for i in range(len(array))])
 
 #    ax.plot()
-    ax0.plot(array[6, -6:], label='sum')
+#     ax0.plot(array[6, -6:], label='sum')
 
-    ax0.grid(True, color='b', ls=':')
+#     ax0.grid(True, color='b', ls=':')
     
-    ax0.set_ylim(0, 1)
-    ax0.set_xlim(0, 2*np.pi)
+#     ax0.set_ylim(0, 1)
+#     ax0.set_xlim(0, 2*np.pi)
 
-    ax0.legend(loc=1).draggable(True)
+#     ax0.legend(loc=1).draggable(True)
     
 #    ax = fig.add_subplot(212)
-#    
+   
 #    parallel_coordinates(array[:20], titles=titles, color='blue', alpha=1, linewidth=0.2)
 #    print('\n'.join(sorted(dir(ax.xaxis.get_ticklabels()[0]))))
-    plt.show()
+#     plt.show()
+    
+    
+if __name__ == '__main__':
+
+    fig = plt.figure(dpi=120)
     
+    ax = fig.add_subplot(111, projection=reg_radar(['A', 'B', 'C', 'D', 'E']))
+#    ax.rgrid(lw=1, linestyle='--',  alpha=0.4)
     
+    lines = ax.plot([0.5, 0.4, 0.6, 0.8, 1])
     
+    ax.fill_lines(lines, alpha=0.5, label='boo')
+    ax.set_zebra( lw= 1)
+    #ax.set_rlabel('E', list('abcde'), color='b')
+    ax.legend()
+    plt.show()
```

## Comparing `datasurfer-1.0.7.dist-info/LICENSE` & `datasurfer-1.0.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `datasurfer-1.0.7.dist-info/METADATA` & `datasurfer-1.0.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datasurfer
-Version: 1.0.7
+Version: 1.0.9
 Summary: A Python package for data processing
 Home-page: https://github.com/yuw1si/datasurfer
 Author: Wei Yu
 Author-email: wei.yu2@de.bosch.com
 License: MIT license
 Keywords: datasurfer
 Classifier: Development Status :: 2 - Pre-Alpha
```

## Comparing `datasurfer-1.0.7.dist-info/RECORD` & `datasurfer-1.0.9.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -13,31 +13,38 @@
 datastructure/lib_objects/data_object.py,sha256=mb5Noi9M-_RUARhzHhqsm9u40eX4GavOp53JguFHPvk,3021
 datastructure/lib_objects/finance_object.py,sha256=0DkfNPoBgaX4_H3E3Fru8gKqKcLsjwlD2-GKnsvTfvw,2211
 datastructure/lib_objects/matlab_object.py,sha256=YgdumVb8s3POb_frQGaXdCQO_gftKQ3EWp6ia-JJ3EU,2862
 datastructure/lib_objects/mdf_object.py,sha256=ENAv1y-1gOrFICz63a5KQ7sBTbP9t-Gdn4lELWKqmAE,6114
 datastructure/lib_objects/pandas_object.py,sha256=YoBAjfqa9eJoemr-fdFNrjsdw7igwOQuQmkXkbfqTFY,1624
 datasurfer/Plot_Collections.py,sha256=Y5Q4RgWdtbVkGZDe8n5zWN6VK5Morny1xUPJPpLIcIA,40406
 datasurfer/Plot_IDIADA.py,sha256=kJI_5tHpNnXwOg0h6FetAcatnR1I4rLMwIIVtCLsBTY,27117
-datasurfer/__init__.py,sha256=D8RWr0YLsbHVltfGDDl9NL3d6oABNLkZB-kRa_2flJM,324
+datasurfer/__init__.py,sha256=88Exe2wfVCMo_R5vfyKcpv-XB9YtB6eVkHkoo51L53Q,306
 datasurfer/datainterface.py,sha256=Qb3MAB5en6gs6L_rZbWwqDEgczCuciAQGQfp3ncDOnA,33171
-datasurfer/datalake.py,sha256=Ufe3Vn4iVz5ay70ZSgHrG1bc2rkCIDWWHOc0pC8Er5Y,14647
-datasurfer/datapool.py,sha256=5m1PqihEjRMaTuG--af3WOxQA7X2GYeuJlVx28eDEgs,55620
-datasurfer/lib_objects/__init__.py,sha256=i48EdJOty1S8ILqJq5U5aQ3RSByZfrf10FnIaItVO0w,1000
-datasurfer/lib_objects/amedata_object.py,sha256=a8ZHa3aVz5gEE56MeuyyL0rjBTMwjtqfU9Mm_CzsMjo,4019
-datasurfer/lib_objects/amegp_object.py,sha256=aNRkiLZJt05ZrLTTMGaaZnuO23DQ3b9YL2jISu9W7lk,4030
-datasurfer/lib_objects/ameres_object.py,sha256=pydlLmuqurAsqNUNW9UQ-LcnaAEZn7Y6C_aFUF8Tilc,7764
-datasurfer/lib_objects/asammdf_object.py,sha256=RX1eXYIIFtDuXKZ0rXSy0RYR4Ccd767Av_-iUoJk_c4,8558
-datasurfer/lib_objects/data_object.py,sha256=mb5Noi9M-_RUARhzHhqsm9u40eX4GavOp53JguFHPvk,3021
-datasurfer/lib_objects/finance_object.py,sha256=0DkfNPoBgaX4_H3E3Fru8gKqKcLsjwlD2-GKnsvTfvw,2211
-datasurfer/lib_objects/json_object.py,sha256=WfGqgqeWOcOoqBQc63dAjTi8qIRbjuLyhZk_5Sm_YCI,532
-datasurfer/lib_objects/matlab_object.py,sha256=KfSnWydwgXktDJ3Q9Wlm_Wy8FVY9nTXQAkybTPsgpR8,3192
-datasurfer/lib_objects/mdf_object.py,sha256=ENAv1y-1gOrFICz63a5KQ7sBTbP9t-Gdn4lELWKqmAE,6114
-datasurfer/lib_objects/pandas_object.py,sha256=-y0apLi4ygARa6dod9Rcl2pAqy1qMIno3bb5h5Ttx0Q,1812
-datasurfer/lib_plots/__init__.py,sha256=QiNtJoxnz3VVayEg0E6qkeaL7PU9IaAy6eQO_cyzuD8,9500
-datasurfer/lib_plots/plot_collection.py,sha256=_16hhnGEwL7x3cbRZU-hN1o8M0TnAinwsFaCiGUJktY,30030
-datasurfer/lib_plots/plot_utils.py,sha256=Y5Q4RgWdtbVkGZDe8n5zWN6VK5Morny1xUPJPpLIcIA,40406
-datasurfer-1.0.7.dist-info/AUTHORS.rst,sha256=J_0aYn4esxraQ7_QjD_zsE4jKxW_jT-eyHsFfkckGB4,164
-datasurfer-1.0.7.dist-info/LICENSE,sha256=kv5c-cVFsGGcBOhI8EXem_QLS5QWzY4cys8HKdKVmkY,1530
-datasurfer-1.0.7.dist-info/METADATA,sha256=QcjsOHrX_ZQAccGg5qnCz-RCmffZ1VQSlXSsn5P8AMA,2636
-datasurfer-1.0.7.dist-info/WHEEL,sha256=iYlv5fX357PQyRT2o6tw1bN-YcKFFHKqB_LwHO5wP-g,110
-datasurfer-1.0.7.dist-info/top_level.txt,sha256=Q4FtCvJzAL3PPM_4zc_L-GkozI100aBxRK35CQQ44WQ,11
-datasurfer-1.0.7.dist-info/RECORD,,
+datasurfer/datalake.py,sha256=Dr_PBo_Ae11WHiQ5vjAituhLHRo0cuaQ9tSliWIBm6M,16163
+datasurfer/datapool.py,sha256=RhCnEhh6IEnI-54NWymBJsNOe8-MVjjodZ4qvcVh_A8,51607
+datasurfer/datautils.py,sha256=CmmkY9hs4teHjb7NbQ1ud_9cfJXmrFFg8kSy8XWgQYk,15464
+datasurfer/lib_mlearn/__init__.py,sha256=bGvhxdTI4AboMBxm9gxTJ8FOtpwHjfuLUggh-4-DAHY,2453
+datasurfer/lib_mlearn/preproc.py,sha256=RxJGqReWU1yfjuDQrsRIMrimxUPkGmnBrEl5nRcLorY,227
+datasurfer/lib_objects/__init__.py,sha256=Gg7SBIWOqloVOZCNRtyJZlsW1_qZXVaOSzByt6VVv2g,29556
+datasurfer/lib_objects/amedata_object.py,sha256=viQXMre_ZMGwMDYpxavjoTjaaRKcSkI2wSYHbI9EKac,5503
+datasurfer/lib_objects/amegp_object.py,sha256=nK7Khwj8It9afWI28H1PIq_5UTkcfgUEQZifojoZgnU,4037
+datasurfer/lib_objects/ameres_object.py,sha256=mVxEJVh0UVli6GQmKNGSFKvwlGNQZCzDxnDjlHOl7ew,7723
+datasurfer/lib_objects/asammdf_object.py,sha256=nGzkJMv-Lb-pI9qxLarNUCARy-oG2nMyeEkaweJTpB8,8510
+datasurfer/lib_objects/data_object.py,sha256=VCbmeZdb-rh6dhTjPNiSYoFeuXX4udc-bLB3CDh6iKg,3028
+datasurfer/lib_objects/finance_object.py,sha256=7i3WD-MDoVAc2OllROMboza3cVQlqi3IOgeRInGiyTc,2229
+datasurfer/lib_objects/json_object.py,sha256=EShRMAjmX3cgKjofBJkccM7hUA0K1iu7BCfc0uUQtis,1292
+datasurfer/lib_objects/matlab_object.py,sha256=BwJ8_krbYwYDefD8uWSgATpMb3QDlzkS-C1J1jsrxy4,3188
+datasurfer/lib_objects/mdf_object.py,sha256=5xpIlcT6xDPMOec7b_Gopqq450NipbtO8OTNmC4n2GE,6040
+datasurfer/lib_objects/pandas_object.py,sha256=5HoBq3brkv7hdRpzmrpiwJ9jXszaB_ntRhgmTy70lNg,1852
+datasurfer/lib_objects/xkf_object.py,sha256=5eJQ8ItZLfEvgO26mkJClr9_i3FWH9kuUfmEUlwiGHQ,2426
+datasurfer/lib_plots/__init__.py,sha256=cs33mmfak2guDPuwpq6BjRJuDPqNsZKYst1opMO_T-Q,11502
+datasurfer/lib_plots/plot_collection.py,sha256=OBdsPA7Ptte6q8Z8DoZ8Ie_DgWvCjOlGv40-SZNnBgw,40380
+datasurfer/lib_plots/plot_utils.py,sha256=6KhKpXX2gvp4k1VaLu9U-cktZ_Azi7b8IBItYVLPNCA,41548
+datasurfer/lib_stats/__init__.py,sha256=9EChxWc8KVpEHumvUmmocbSSxp74c9WPHA5sJ9uJJZc,4982
+datasurfer/lib_stats/distrib_methods.py,sha256=VO3n_CnhxfnEPFOyonLX3XcAkbyhX-jnUwUXSWhJIRg,667
+datasurfer/lib_stats/interp_methods.py,sha256=uQOE5vixLn8Ifmqq7dDiyt-zLTIHJeM54rtNteX5GCw,1955
+datasurfer-1.0.9.dist-info/AUTHORS.rst,sha256=J_0aYn4esxraQ7_QjD_zsE4jKxW_jT-eyHsFfkckGB4,164
+datasurfer-1.0.9.dist-info/LICENSE,sha256=kv5c-cVFsGGcBOhI8EXem_QLS5QWzY4cys8HKdKVmkY,1530
+datasurfer-1.0.9.dist-info/METADATA,sha256=Llc8cxKQ_gHAiqDH22Xp8bVaonD5zfM8wvLWPQUWqeA,2636
+datasurfer-1.0.9.dist-info/WHEEL,sha256=iYlv5fX357PQyRT2o6tw1bN-YcKFFHKqB_LwHO5wP-g,110
+datasurfer-1.0.9.dist-info/top_level.txt,sha256=Q4FtCvJzAL3PPM_4zc_L-GkozI100aBxRK35CQQ44WQ,11
+datasurfer-1.0.9.dist-info/RECORD,,
```

